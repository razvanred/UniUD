# Programmazione logica

Dobbiamo definire una semantica, ovvero quali passaggi servono per ottenere le risposte calcolate.

Può essere una descrizione molto logica (la semantica $S(P)$) oppure proprio la computazione da eseguire per trovare la risposta (il behaviour del programma $B[P]$).

Dipende molto da che proprietà vogliamo scoprire.

Possiamo definire $B[P]=\{(G,\theta)|G\rightarrow^\theta_P\Box\}$

Una semantica è *corretta* rispetto ad un behaviour se $S[P_1]=S[P_2]\rightarrow B[P_1]=B[P_2]$.
Fissato un comportamento che vogliamo avere, cerchiamo la semantica che concorda col comportamento.

L'implicazione è vera anche se programmi con semantica diversa hanno comportamento diverso, questo significa che la semantica ha più informazioni di quelle che servono.\
Se diventa una doppia implicazione di dice *fully abstract*.

Ovviamente possiamo prendere come semantica il behaviour stesso è sempre fully abstract, ma non serve a niente.

Una interpretazione $I\in\mathbb I$ è una funzione che va dall'insieme di tutti i predicati $MGC$ all'insieme delle sostituzioni (assegnamenti di valori alle variabili all'interno del predicato).
Otterremo una semantica un po' più generale della semantica delle conseguenze immediate

Una semantica che corretta è $T\^{CA}_P:\mathbb I\mapsto\mathbb I$, l'operatore: $T\^{CA}_P(I)=\lambda p(\overrightarrow{X}).\{$ Le sostituzioni $\theta$ tali che: Prendiamo una $p(\overrightarrow{T}):-p_1(t_1),...,p_n(t_n)$ dove $\overrightarrow{T}$ è una rinomina delle variabili con nessun nome in comune con $\overrightarrow{X}$; e prendiamo tutte le sostituzioni $\theta_i in I(p_i(\overrightarrow{X_i}))$; a questo punto prendiamo il mgu $\theta$ (most general unifier) di tutte queste $\}$.

Quindi dato un programma:

```
ap([],X,X).
ap( [X|XS], Y, [X|ZS] ) :- ap(XS, Y, ZS).
```

Partiamo con $T\^{CA}_P\uparrow0=\empty$ e costruisce prima le sostituzioni di $ap(X,Y,Z)$ in cui $X$ è la lista vuota. Poi quelle in cui è una lista singoletta, poi quando è di 2 elementi, etc.

$T\^{CA}_P$ è monotono.

Una differenza tra questo è la concatenazione ad esempio di haskell, o anche quella di ASP, è che funziona per qualsiasi variabile.
Il minimo punto fisso è l'insieme con le sostituzioni di qualsiasi lista possibile.

Perché questa semantica è sufficiente per qualunque goal? Perché per qualunque goal iniziale $G$, se esiste una mgu per $G$ costruito in questo modo, applicando questa sostituzione possiamo svuotare il goal. Quindi $G\rightarrow^\theta_P\Box$, che è il behaviour descritto prima.

Se due programmi hanno le stesse risposte calcolate hanno lo stesso grounding delle risposte, ma non necessariamente viceversa. La semantica delle conseguenze immediate che abbiamo visto ad AR calcola il grounding delle risposte calcolate.

Indichiamo con $F^{CA}_P=lfp(T^{CA}_P)$

## Versione classica ("semantica")

Invece che usare $T_P\^{CA}$ usiamo $T_P^S$, che trova le conseguenze immediate, scegliendo uan regola, poi trovando l'mgu con quello ce sappiamo finora, e inserisce nel mucchio la testa istanziata, non la sostituzione.

Per passare dall'altra versione a questa è sufficiente, per ogni interpretazione che vogliamo codificare $c(I)$, applicarla ad ogni predicato del MGC. Questa $c$ è invertibile e diventa la funzione $d$.
Sono un'isomorfismo.

$T_P\^{CA}=d\circ T_P^S\circ c$

Indichiamo con $F^{CA}_P=lfp(T^{CA}_P)$

## Teorema

$G\rightarrow^\theta\Box$ (ha una risposta calcolata), sse $\exist B_1,...,B_n\in F^S_P$ o rinomine, tali che $\theta=mgu(G,(B_1,...))$

Quindi come corollario, $F^S_P$ è fully abstract. Per l'isomorfismo di prima, anche $F^{CA}_P$.

Grazie a questo teorema, se lavoriamo su queste astrazioni, tutta l'imprecisione viene introdotta a noi e non l'astrazione.

## Indicizzazione di pos

$[MGC\mapsto\wp(subst)]\stackrel{\rightarrow}{\leftarrow}[MGC\mapsto POS]$

Indichiamo con $\Omega(V,con)$ le formule con atomi $V$ e connettori $con$.

Un assegnamento è una funzione $r:V\mapsto\mathbb B$ (booleani). Diciamo che $R\vDash f\in\Omega(V,\Sigma)$ se sostituendo le variabili con $r(var)$ la formula è vera.

$f_1\Rightarrow f_2$ se ogni $r$ che rende vero $f_1$ rende vera $f_2$. *Non* è un ordine su $\Omega$. Ma se usiamo $\Leftrightarrow$ come classe di equivalenza, lo è. Non solo, otteniamo $(\Omega/_\Leftrightarrow,\Rightarrow,\lor,\land,[true]_\Leftrightarrow,[false]_\Leftrightarrow)$ è un lattice completo. Usiamo $\leq$ per dire $\Leftrightarrow$.

$POS_V=\{f\in\Omega{V,\{\lnot,\land,\lor,\rightarrow,\leftarrow,\leftrightarrow\}}|\lambda x.true\vDash f\}/_\Leftrightarrow$. Non tutte le formule si possono scrivere, ma solo quelle che sono vere quando $x$ è $true$.

Guarda caso, viene fuori che $POS_V=\Omega(V,\{\lor,\land,\leftrightarrow\})/_\Leftrightarrow$.

Se proviamo a fare $POS_{\{x,y\}}$ viene fuori un cubo, con come bottom $x\land y$. Possiamo provarlo.

Indichiamo: $Pos_V=POS_V\cup\{false\}$, che serve per togliere una catena discendente infinita in cui si aggiungono sempre $\land v$.

Vogliamo una funzione $\Gamma:T(\Sigma,V)\mapsto Pos_V$, e lo definiamo come:

$$
\Gamma(t)=\begin{cases}
v_1\land...\land v_n & vars(t)=\{v1,...,v_n\}\\
true & vars(t)=\emptyset
\end{cases}\\
\Gamma(x/t)=x\leftrightarrow\Gamma(t)\\
\Gamma(\theta)=\bigwedge_{x/t\in\theta}\Gamma(x/t)\\
\Gamma(\Theta)=\bigvee_{\theta\in\Theta}\Gamma(\theta)
$$

Quindi vediamo che visto che $POS$ si costruiva con $\lor,\land,\leftrightarrow$, anche le formule ottenute da $\Gamma$ sono automaticamente anche formule di POS.

Esempi:
* $\Gamma(\{\{X/g(a),Y=b\}\})=X\leftrightarrow true\land Y\leftrightarrow true=X\land Y$
* $\Gamma(\{\{X/g(S),Y/b\}\})=X\leftrightarrow S\land Y$
* $\Gamma(\{\{X/[a,L],Y/[b,L,H]\}\})=X\leftarrow L\&y\leftrightarrow L\land H$

Una cosa che possiamo voler fare è restringere uan formula ad un determinato gruppo di variabili: 
$$
f|_X=\begin{cases}
f&vars(f)\subseteq X\\
f[Y/true]\lor f[Y/false]&\exists Y\in vars(f)\setminus X
\end{cases}
$$

Esempi:
* $X\land Y|_{X,Y}=X\land Y$
* $X\leftrightarrow S\land Y|_{X,Y}=Y$
* $X\leftarrow L\&y\leftrightarrow L\land H|_{X,Y}=Y\rightarrow X$

Definiamo l'astrazione $\alpha^{POS}(I)=\lambda p(\overrightarrow{x}).\Gamma(I(p(\overrightarrow{x})))|_{\overrightarrow{x}}$.

Definiamo la concretizzazione $\gamma^{POS}$ come aggiunta di $\alpha^{POS}$.

$G_P(I)=(\alpha^{POS}\circ T^{CA}_P\circ \gamma^{POS})(I)=\lambda p(\overrightarrow{x}).\Gamma(\{\{\overrightarrow{x}/\overrightarrow{t}\theta\}|p(\overrightarrow{x}):-p_1(t_1),...\in P,\theta_i\in\gamma^{POS}(I)(p_1(\overrightarrow{x}_i)),\theta=mgu(...)\})|_{\overrightarrow{x}}$.

Vediamo il lattice (sempre con il $P$ di prima):
* $G_P\uparrow0=\{ap(x,y,z)\mapsto false\}$
* $G_P\uparrow1=\{ap(x,y,z)\mapsto (x\& y\leftrightarrow z)\lor false\}$
* $G_P\uparrow2=\{ap(x,y,z)\mapsto (z\leftrightarrow x\land y)\}$
  * $x\leftrightarrow\overline x\land Xs\&y\leftrightarrow Ys\& Zs\leftrightarrow Z_1\&Xs\leftrightarrow X_1\&Ys\leftrightarrow Zs\&(X_1\$Y_1\leftrightarrow Z_1)|_{x,y,z}=z\leftrightarrow x\land y$
  * $(z\leftrightarrow x\land y)\lor(x\& y\leftrightarrow z)=z\leftrightarrow x\land y$
* $G_P\uparrow3=\{ap(x,y,z)\mapsto (z\leftrightarrow x\land y)\}=G_P\uparrow2$

Quindi, questo punto fisso ci dice che se $Z$ è ground, allora rendiamo ground anche $X$ ed $Y$. Viceversa, se sono ground $X$ ed $Y$, rende ground $Z$. Ma non abbiamo alre informazioni.





Se prendiamo $F^{CA}_P=\{ap(x,y,z)\mapsto\{\{x/[],y/\alpha,z/\alpha\},\{x/[\alpha_1],y/\beta,z/[\alpha_1|\beta]\},...,\{x/[\alpha_1,...,\alpha_n],y/\beta,z/[\alpha_1,...,\alpha_n|\beta]\}\}\}$ e lo astraiamo otteniamo lo stesso risultato, ma è un caso perché non sempre funziona.

Facciamo un esempio con un programma che non funziona.

```
p(f(X)) :- q(X).
q(a).
r(X) :- p(g(X)).
s(X,Y) :- r(X).
s(X,a).
```

Qua calcolando sia $F^{CA}$ che $G$ vediamo che la prima è più restrittiva della seconda. Significa (come è giusto) che quando $G$ dice che una query è ground è vero.
Ma ci sono alcune query ground che sono ground e lo vediamo con $F$, ma $G$ non cattura.

## POS in haskell

La groundnes adesso è il fatto di essere completamente valutato. La usiamo per vedere se una funzione consuma tutto l'input.

Vediamo la funzione come un predicato $f(x)\triangleright\rho$. La semantica astratta va da questi predicati, a POS.

$$
G[P]_I(f(x)\triangleright\rho)=\bigvee_{f(x)\rightarrow r\in P}\left(\Gamma(\{x/t\}~\&~\xi[r\triangleright\rho]_I)\right)|_{x,\rho}
$$

Definiamo $\xi$:
* $\xi[x\triangleright\rho]_I=x\leftrightarrow\rho$ variabile
* $\xi[c(\overrightarrow{t})\triangleright\rho]_I=\rho\leftrightarrow\bigwedge_i e_i~\&~\bigwedge_i\xi[t_i]_I$ costruttore
* $\xi[f(\overrightarrow{t})\triangleright\rho]_I=I(f(\overrightarrow{e})\triangleright\rho)~\&~\bigwedge_i\Phi_i$

Definiamo $\Phi_i$:
$$
\Phi_i=\begin{cases}
\xi[t_i\triangleright\rho]_I & I(f(\overrightarrow{e})\triangleright\rho)\leq\rho\rightarrow e_i\\
true & otherwise
\end{cases}
$$

Esempio:
```
find :: int -> [int] -> [int]
find k (k':v) | k==k' = v
find k (_:xs) = find k xs
```

In questo caso avviamo che `find k xs`$\triangleright\rho~\mapsto~xs\rightarrow k\land\rho$. Se valuto `xs` devo sicuramente aver valutato tutto. Ma potrei aver valutato testa e risultato, senza aver valutato `xs`, nel caso in cui trovo subito la risposta.