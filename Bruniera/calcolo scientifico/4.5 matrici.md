# Matrici

## Norma

$$
||\circ||:\mathbb{R}\mapsto\mathbb{R}_+\cup\{0\}
$$

Una norma è una funzione che rispetta queste tre proprietà, e rappresenta una misura della "lunghezza" di un vettore:

$$
\begin{aligned}
    &1.~ ||x||=0\iff x=0\\
    &2.~ ||\alpha\cdot x||=|\alpha|\cdot||x||\\
    &3.~ ||x+y||\leq||x||+||y||\\
\end{aligned}
$$

Una classe di norme sono quelle che derivano dal teorema di Pitagora:

$$
||x||\_p=\sqrt[p]{\sum^d_{i=1}x_i^p}
$$

$||x||_2$ è la norma classica con $p=2$.

Tutte le norme danno una misura comparabile (per i vettori) e vengono considerate equivalenti.

La norma con $p\rightarrow\infty$ è:

$$
||x||\_\infty=\max_{i=1,...,d}|x_i|
$$

## Norma matriciale

La norma matriciale soddisfa delle proprietà analoghe alla norma vettoriale, più una proprietà sul prodotto:

$$
4.~ ||A\cdot B||\leq||A||\cdot||B||
$$

Come le norme vettoriali, le norme matriciali vengono considerate equivalenti.

### Norma matriciale indotta

Data una norma vettoriale $||\circ||$, si definisce la *norma matriciale indotta* da $||\circ||$:

$$
||A||=\max\_{||x||=1}||A\cdot x||
$$

Si moltiplicano tutti i vettori con $||x||=1$ alla matrice $A$, la norma della matrice è il massimo delle norme dei vettori risultanti.

Si può dimostrare che la norma indotta da $||x||_\infty$ è il massimo della sommatoria di ogni colonna:

$$
||A||\_\infty=\max\limits\_{i=1,...,d}\sum\limits^d_{j=1}|a_{i,j}|
$$

Si può dimostrare che la norma indotta da $||x||_1$ è il massimo della sommatoria di ogni riga:

$$
||A||\_1=\max\limits\_{j=1,...,n}\sum\limits^n_{i=1}|a_{i,j}|
$$

Si può dimostrare che la norma indotta da $||x||_2$ è:

$$
\begin{aligned}
    & ||A||_2=\sqrt{S(A^T\cdot A)} & [A^T\text{ è la trasposta di }A]\\
    & & [S(M)\text{ è il massimo degli autovalori di }M]
\end{aligned}
$$

## Matrici non singolari

Dire che tutte le colonne di una matrice $A$ sono linearmente indipendenti equivale a:

* $A$ non è singolare
* $\det(A)\neq0$
* $\mathrm{rank}(A)$ massimo
* Esiste l'inversa $A^{-1}$

## Condizionamento Prodotto tra matrice e vettore

Data la matrice $A$ invertibile, si vuole studiare il condizionamento del prodotto $A\cdot x=y$.\
Usando l'inversa di $A$ si può ottenere:

$$
\begin{aligned}
& y= A\cdot x\Rightarrow x=A^{-1}\cdot y & [A\text{ è invertibile}]\\
& \Rightarrow||x||=||A^{-1}~ y|| &\\
& ||x||\leq||A^{-1}||~ ||y|| & [\text{regola 4 delle norme}]
\end{aligned}
$$

Introduciamo le perturbazioni $\delta y=\tilde y-y$ e $\delta x=\tilde x-x$ delle variabili $x$ e $y$:

$$
||\delta y||=||A\cdot\delta x||\leq||A||~ ||\delta x||
$$

Gli errori sono rispettivamente $\epsilon_y=\frac{||\delta y||}{||y||}$ e $\epsilon_x=\frac{||\delta x||}{||x||}$, quindi:

$$
\begin{aligned}
& \frac{||\delta y||}{||y||}\leq\frac{||A||~ ||\delta x||}{||y||} &[\text{dividere entrambi per }y]\\
& =\frac{||A||~ ||\delta x||~ ||x||}{||y||~ ||x||} & [\text{moltiplicare e dividere per }||x||]\\
& =||A||\frac{||x||}{||y||}\epsilon_x & \left[\epsilon_x=\frac{||\delta x||}{||x||}\right]\\
& \leq||A||\frac{||A^{-1}||~ ||y||}{||y||}\epsilon_x & [||x||\leq||A^{-1}||~ ||y||]\\
& =||A||~ ||A^{-1}||\epsilon_x=\mathrm{cond}(A)\epsilon_x &\\
\end{aligned}
$$

Se $\mathrm{cond}(A)\gg1$, il problema è mal condizionato.
