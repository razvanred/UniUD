# Zeri di una funzione

Vogliamo cercare gli zeri di equazioni non lineari:

Data una funzione $f:\mathbb{R}\rightarrow\mathbb{R}$ trovate $\alpha$ tale che $f(\alpha)=0$. $\alpha$ è detta radice (o zero) della funzione.

Non sempre c'è uno zero, e non sempre è unico, a volte sono infiniti ( $cos(x)$ ).

Una radice si dice multipla quando, con una certa molteplicità $\mu$, oltre ad essere radice della funzione è radice delle sue derivate fino a $\mu-1$:

$$
\displaylines{
\text{per }\mu>1 \text{ si ha }\\
f'(x)=f''(x)=...=f^{(\mu-1)}(x)=0 \text{ e } f^{(\mu)}(x)\neq0}
$$

## Localizzazione di una radice

Stabilito che la funzione ha una radice, dobbiamo trovare un intervallo sufficientemente piccolo che la racchiuda.

**Teorema**: Sia $f$ una funzione continua in $[a,b]$ e tale che $f(a)f(b)< 0$ (ovvero che nell'intervallo la funzione cambi segno), allora $f$ ha almeno uno zero nell'intervallo. Quest'intervallo è detto **intervallo di localizzazione** (bracket). Come si può ridurre?

## Algoritmo della bisezione

Dato un intervallo che contiene uno zero (i cui estremi hanno segno opposto) si controlla il segno della funzione nel punto medio, quindi si sceglie come prossimo intervallo la parte dove l'estremo ha segno opposto del punto medio. Si prosegue fino a che l'intervallo non è più piccolo di una soglia prefissata.

Converge sempre per funzioni continue.

Il numero delle iterazioni necessarie è $k=\left\lceil\log_2\left(\frac{|b-a|}{\mathrm{tol}}\right)\right\rceil$. Non dipende dalla funzione ma dall'intervallo. È piuttosto lento perché ad ogni iterazione si avvicina di un bit al valore del risultato partendo dal range. Con $|b-a|=1$ serve circa un iterazione per ogni cifra binaria di precisione del risultato (~4 per ogni cifra decimale), ad esempio con $\mathrm{tol}=10^{-7}$ si ha $k=24$.

### Errore

Vogliamo vedere l'errore dell'approssimazione di $\alpha$ ottenuta scegliendo $\tilde\alpha=m=\frac{(a+b)}2$ dopo l'ultima operazione.

$$
|\alpha-\tilde\alpha|\leq\frac{|b-a|}2\leq\frac{\mathrm{tol}}2
$$

Sostituiamo:

```matlab
while (|b-a| > tol)
```

con:

```matlab
while ({|b-a|}/{min(|a|,|b|)} > tol)
```

per fermarci quando siamo soddisfatti dell'errore relativo invece che l'errore assoluto (l'altro può diventare più lento, a volte).
La tolleranza non è più "assoluta" ma "relativa".

Si ottiene il seguente errore relativo:

$$
\frac{|\alpha-\tilde\alpha|}{|\alpha|}\leq\frac{|b-a|}{2\min\{|a|,|b|\}}\leq\frac{\mathrm{tol}}2
$$

Può non funzionare per $\min\{|a|,|b|\}$ molto vicino a zero, perché `{|b-a|}/{min(|a|,|b|)}` diventa molto grande e può chiedere più passaggi (e con zero non funziona).

## Metodo dell'iterazione funzionale

Trasformiamo il problema di trovare gli zeri di una funzione $f(x)$ nel trovare i punti fissi di una funzione $g(x)$.
Quindi invece dell'equazione $f(x)=0$ risolviamo $g(x)=x$. Per un'opportuna $g$ i problemi sono equivalenti.

Al problema associamo lo schema iterativo:

$$
\displaylines{
x_0 \text{ dato}\\
x_{k+1} = g(x_k)}
$$

In generale alla stessa $f$ sono associati più problemi di punto fisso equivalenti, ma con proprietà iterative ben diverse (non convergono, o lo fanno lentamente).

* Esempio $f(x)=x^2-x-2$
  * $g(x)=x^2-2$ con $x_0=2.01$
    * L'iterazione non converge, si dice che il punto fisso è *repulsivo*
    * Non va bene
  * $g(x)=\sqrt{x+2}$ con $x_0=1$
    * L'iterazione converge, si dice che il punto fisso è *attrattivo*
    * In 10 passaggi raggiunge un buon errore relativo
    * Il rapporto fra gli errori in passi successivi è $\frac{|\varepsilon_{k+1}|}{|\varepsilon_k|}$ tende a $\frac14$. Ciò ci fornisce una misura della velocità di convergenza
    * È monotona (cresce/decresce sempre, o resta uguale), le stime restano sempre dallo stesso lato di $\alpha$
  * $g(x)=1+\frac2x$ con $x_0=1$
    * L'iterazione converge
    * Stavolta il rapporto tra gli errori è $\frac12$, è paragonabile a quello della bisezione
    * Non è monotona
  * $g(x)=\frac{x^2+2}{2x-1}$ con $x_0=1$
    * Converge, è monotona
    * È molto veloce. Con 6 iterazioni supera la precisione di macchina
    * L'errore non migliora linearmente come gli altri, ma è quadratico. Per questo è veloce

---

### Teorema del valore medio

Data una funzione $f$, e due valori $a$ e $b$, esiste un punto $\xi_{a,b}\in~ ]a,b[$ dove la tangente di $f(\xi_{a,b})$ ha la   stessa pendenza del segmento da $f(a)$ ad $f(b)$.

$$
f'(\xi_{a,b})=\frac{f(a)-f(b)}{a-b}~ \Rightarrow ~ f(a)-f(b)=f'(\xi_{a,b})({a-b})\\
$$

---

### Teorema della convergenza locale

Sia $g$ derivabile in un intervallo, e sia $\alpha$ un suo punto fisso nell'intervallo.\
$x_k$ converge se esiste un intorno circolare di $\alpha$ ( $I_\alpha$ ) in cui $\forall_{x\in I_\alpha}|g'(x)< 1|$.\
Va scelto un $x_0$ all'interno di $I_\alpha$.

**Dimostrazione**: Per il *teorema del valore medio* vale l'equivalenza:

$$
e_{k+1}=x_{k+1}-\alpha=g(x_k)-g(\alpha)=g'(\xi_k)(x_k-\alpha)=g'(\xi_k)e_k
$$

Dove $\xi_k$ è il valore $\xi_{x_k,\alpha}$ del teorema del valore medio.\
Inoltre, se:

$$
|g'(\xi_k)|< 1\Leftrightarrow\left|\frac{g(x_k)-g(\alpha)}{x_k-\alpha}\right|< 1\Leftrightarrow\left|\frac{x_{k+1}-\alpha}{x_k-\alpha}\right|< 1\Leftrightarrow|x_{k+1}-\alpha|< |x_k-\alpha|\Leftrightarrow e_{k+1}< e_k
$$

Quindi, se nell'intorno la derivata è sempre minore di 1, anche $g'(\xi_k)$ sarà minore di 1 per tutti i $k$.
Quindi ad ogni passaggio l'errore assoluto diminuirà.
Quindi $x_k$ converge perché si avvicina ad $\alpha$.

Sia $\lambda=\max_{x\in I_\alpha}g'(x)$ e $\lambda< 1$, si ha $|e_{k+1}|\leq\lambda e_k$.

Infine, si può dimostrare "facilmente" che non possono esserci altri punti fissi all'interno di $I_\alpha$

### Osservazioni su $g'$ in $[a,b]$

Se la condizione del teorema è verificata in un intervallo $[a,b]$ contenente $\alpha$, le ipotesi valgono al massimo in un intervallo *circolare* $I_\alpha$ contenuto in $[a,b]$.\
Per assicurare la convergenza bisognerebbe quindi scegliere come $x_0$ l'estremo più vicino ad $\alpha$ (non si sa quale, poiché $\alpha$ non è noto). Talvolta ciò non è necessario:

* Se $g'(x)>0$ in $[a,b]$, la successione è monotona e $x_0$ può essere scelto indifferentemente.\
Inoltre in questo caso è sufficiente che la condizione sia verificata solo in nella metà dell'intervallo in cui scegliamo $x_0$
* Se $g'(x)< 0$ in $[a,b]$  la successione è alternata e non si può stabilire quale dei due estremi sia il più vicino.\
Si può scegliere uno degli estremi come $x_0$ e poi:
  * Se $x_1\in[a,b]$ si può continuare con la successione, se esce lo fa subito
  * Se $x_1\notin[a,b]$, si deve ripartire scegliendo l'altro estremo, stavolta funzionerà

### Osservazioni su $g'$ in $\alpha$

Il metodo è convergente anche con ipotesi più deboli. Visto che $\xi_k$ cade *tra* $x_k$ e $\alpha$, si possono escludere sia gli estremi che $\alpha$ dalla condizione del teorema.

* Se $|g'(\alpha)|\approx1$ (parallela all'identità, o perpendicolare all'identità) converge molto lentamente. Intuitivamente farà o dei salti molto piccoli (se >0) o troppo grandi (se <0) ad ogni iterazione, perché la funzione si avvicina all'identità.
* Se $|g'(\alpha)|< 1$ esiste un intorno circolare $I_\alpha$ dove $\forall_{x\in I_\alpha}~ |g'(x)|< 1$\
  Abbiamo convergenza locale
  * Se $0< g'(\alpha)< 1$ esiste un intorno circolare $I_\alpha$ dove $\forall_{x\in I_\alpha}~ 0< g'(x)< 1$\
    Abbiamo convergenza locale *monotona*
  * Se $-1< g'(\alpha)< 0$ esiste un intorno circolare $I_\alpha$ dove $\forall_{x\in I_\alpha}~ -1< g'(x)< 0$\
    Abbiamo convergenza locale *alternata*
  * Se $g'(\alpha)=0$\
    Converge (velocemente, visto che tende ad essere costante in $I_\alpha$), ma può non essere ne alternata ne monotona
* Se $|g'(\alpha)|>1$\
  Il punto fisso è repulsivo (non lo trattiamo)

### Fattore asintotico di riduzione e ordine di convergenza

Sia $x_k$ per $k\geq0$ una successione convergente ad $\alpha$ e sia:

$$
\lim_{k\rightarrow\infty}\frac{|x_{k+1}-\alpha|}{|x_k-\alpha|}=l
$$

Se:

* $l=1$ si parla di convergenza **sublineare**
* $0< l< 1$ si parla di convergenza **lineare** o **di ordine $p=1$**
* $l=0$ si parla di convergenza **superlineare**

$l$ è il *fattore asintotico di riduzione*

Per il metodi di iterazione funzionale risulta:

$$
\displaylines{
l=\lim_{k\rightarrow\infty}\frac{|x_{k+1}-\alpha|}{|x_k-\alpha|}=\lim_{k\rightarrow\infty}\frac{|e_{k+1}|}{|e_k|}=\lim_{k\rightarrow\infty}\frac{|g'(\xi_k)e_k|}{|e_k|}=\lim_{k\rightarrow\infty}|g'(\xi_k)|=|g'(\alpha)|\\
\\
l=|g'(\alpha)|}
$$

#### Ordine di convergenza

Una successione ha ordine di convergenza $p>1,p\in\mathbb{R}$ se:

$$
\lim_{k\rightarrow\infty}\frac{|x_{k+1}-\alpha|}{|x_k-\alpha|^p}=L>0
$$

In altre parole, se per $k>>0$ si ha $|e_{k+1}|\approx L|e_k|^p$.\
Se $p=2$ si ha convergenza quadratica, se $p=3$ è cubica, eccetera.

Se un'iterazione migliora di molto la precisione, $|x_k-\alpha|$ è molto più grande di $|x_{k+1}-\alpha|$, quindi, andando ad infinito, si ha un denominatore molto più grande del numeratore, e il limite tende a 0.\
Notare che per $k$ molto grandi, il valore $|x_k-\alpha|$ è vicino a 0. Se si trova limite 0, bisogna permettere al numeratore di "recuperare" il denominatore rimpicciolendo il denominatore. Possiamo rimpicciolirlo elevando a potenza, perché è minore di 1. Aumentiamo l'esponente $p$ finché non abbiamo un limite diverso da 0.\
Quindi, quando $p$ cresce, anche il limite cresce.\
Se $p$ è appena sufficiente da permettere al numeratore di recuperare il denominatore, il limite convergerà a qualcosa compreso tra 0 e 1, quel valore (che chiamiamo $L$) è paragonabile al coefficiente che si usa nella notazione asintotica.

Si può dimostrare che in particolare, il metodo di iterazione funzionale ha ordine di convergenza $p$ se:

$$
\displaylines{
g'(\alpha)=g''(\alpha)=...=g^{(p-1)}(\alpha)=0\\
g^{(p)}(\alpha)\neq0}
$$

E si ottiene, usando la formula di Taylor:

$$
L=\frac{|g^{(p)}(\alpha)|}{p!}
$$

$l$ è $L$ con $p=1$

### Criterio d'arresto

**Errore assoluto:**

Si può controllare l'errore assoluto con:

$$
|x_{k+1}-x_k|< \mathrm{tol}
$$

in questo caso si ottiene:

$$
\displaylines{
\begin{aligned}
  & |x_{k+1}-x_k|=|x_{k+1}-\alpha+\alpha-x_k| & [+\alpha-\alpha=0]\\
  & =|e_{k+1}-(x_k-\alpha)| & [x_{k+1}-\alpha=e_{k+1}]\\
  & =|g'(\xi_k)(x_k-\alpha)-(x_k-\alpha)| & [e_{k+1}=g'(\xi_k)e_{k}]\\
  & =|(x_k-\alpha)(g'(\xi_k)-1)| &
\end{aligned}\\
\\
|x_k-\alpha|=\frac{|x_{k+1}-x_k|}{|g'(\xi_k)-1|}< \frac{\mathrm{tol}}{|g'(\xi_k)-1|}}
$$

L'errore può essere molto grande quando $g'(\alpha)\approx1$, perché $\xi_k$ tende ad $\alpha$, e $\mathrm{tol}$ viene diviso per qualcosa che si avvicina a 0. Questo non succede con $g'(\alpha)\approx-1$, in quel caso l'errore relativo tenderà ad essere la metà di $\mathrm{tol}$.

Quando la serie $x_k$ è alternata, funziona bene, perché la distanza tra due iterazioni è sempre maggiore della distanza tra $x_k$ ed $\alpha$.
Quando è monotona potrebbe funzionare male, perché si ferma quando la serie rallenta, anche se rallenta in un punto distante da $\alpha$.\
Questo va d'accordo con le considerazioni su $g'(\xi_k)$ fatte sopra.

**Valore di $f$:**

Si può controllare se $f(x_k)$ è effettivamente vicino a 0:

$$
|f(x_k)|< \mathrm{tol}
$$

si ottiene:

$$
\displaylines{
\begin{aligned}
&f(\alpha)-f(x_k)=f'(\xi_k)(x_k-\alpha)\Leftrightarrow(x_k-\alpha)=\frac{f(\alpha)-f(x_k)}{f'(\xi_k)} & [\text{per il teorema del valore medio}]\\
\end{aligned}\\
\\
|x_k-\alpha|=\frac{|f(\alpha)-f(x_k)|}{|f'(\xi_k)|}=\frac{|0-f(x_k)|}{|f'(\xi_k)|}=\frac{|f(x_k)|}{|f'(\xi_k)|}\leq\frac{\mathrm{tol}}{|f'(\xi_k)|}}
$$

Nota che la questa $\xi_k$ è il valor medio tra $f(x_k)$ e $f(\alpha)$. (*non* è lo stesso tra $x_k$ e $\alpha$, che invece usava la funzione $g$)
