# Errori continua

## Tre errori

$$
\displaylines{
E_{totale}\approx E_{inerente}+E_{analitico}+E_{algoritmico}\\
E_{inerente}=\frac{f(x)-f(\tilde x)}{f(x)}\\
E_{analitico}=\frac{f(\tilde x)-g(\tilde x)}{f(\tilde x)}\\
E_{algoritmico}=\frac{g(\tilde x)-\tilde g(\tilde x)}{g(\tilde x)}}
$$

* $E_{inerente}$: effetto delle perturbazione nei dati
  * L'errore inerente è legato al problema, e studia l'effetto degli errori nei dati di input sull'output.
  * La scelta dell'algoritmo non ha effetti su tale errore
* $E_{analitico}$: effetto dell'approssimazione del problema
* $E_{algoritmico}$: effetto dell'aritmetica di macchina

## Condizionamento

Il problema si dice *ben condizionato* se non amplifica gli errori nei dati, ovvero se l'errore relativo nei dati di input e l'errore relativo nei dati di output sono dello stesso ordine di grandezza.

Il problema si dice *mal condizionato* se un errore anche piccolo nei dati di input causa un grande errore nei dati di output.

$$
\text{mal condizionato se: }
\mathrm{cond}\_f(x)=\frac{|errore_{risposta}|}{|errore_{dato}|}=\frac{E_{inerente}}{\epsilon_x}=\frac{\left|\frac{f(x)-f(\tilde x)}{f(x)}\right|}{\frac{|x-\tilde x|}{|x|}} \gg 1
$$

$\frac{|f(x)-f(\tilde x)|}{|x-\tilde x|}$ si può anche rappresentare come $f'(x)$ per  $|x-\tilde x|\rightarrow 0$ :

$$
\frac{|f(x)-f(\tilde x)|~ |x|}{|x-\tilde x|~ |f(x)|}=\frac{|f'(x)|~ |x|}{|f(x)|}
$$

## Errore inerente (della somma)

A differenza delle altre operazioni fondamentali, la somma è fortemente *mal condizionata* nel caso di numeri opposti.

$$
\sum^n_{i=1}|x_i|\gg|\sum^n_{i=1}x_i|
$$

Questo comportamento è il fenomeno della cancellazione.

### Condizionamento di un problema $F:\mathbb{R}^n\mapsto\mathbb{R}$

Dove $x=(x_1,...,x_n),\tilde x=(\tilde x_1,...,\tilde x_n)\in\mathbb{R}^n$.

Usando la formula di Taylor si trova, in prima approssimazione

$$
\frac{F(\tilde x)-F(x)}{F(x)}=\sum^n_{i=1}\left(\left(\frac{x_i}{F(x)}\frac{\partial F(x)}{\partial x_i}\right)\frac{(\tilde x_i-x_i)}{x_i}\right)
$$

$$
|E_{inerente}|\leq\sum^n_{i=1}(|c_i(x)|\epsilon_{x_i})
$$

Dove $\epsilon_{x_i}$ è l'errore relativo al dato $i$-esimo, e $c_i(x)=\frac{x_i}{F(x)}\frac{\partial F(x)}{\partial x_i}$ è il *coefficiente di amplificazione* associato.
I coefficienti di amplificazione ( $c_i(x)$ ) forniscono una misura del condizionamento del problema.

* nel caso della somma:

  ```math
  \displaylines{
  c_i(x_1,...,x_n)=\frac{x_i}{F(x)}\frac11=\frac{x_i}{x_1+...+x_n}\\
  |E_{inerente}|\leq\max\limits_{i=1,...,n}(\epsilon_{x_i})\cdot\frac{\sum\limits^n_{i=1}|x_i|}{\left|\sum\limits^n_{i=1}x_i\right|}}
  ```

* nel caso del prodotto ( $F(x_1,...,x_n)=\prod\limits^n_{i=1}x_i$ ):

  $$
  c_i(x_1,...,x_n) = \frac{x_i}{x_1\cdot...\cdot x_i\cdot...\cdot x_n}\cdot\frac{x_1\cdot...\cdot x_{i-1}  \cdot1\cdot x_{i+1}\cdot...\cdot x_n}{1}=\frac{x_i}{x_i}=1
  $$

  Il prodotto è sempre ben condizionato.

## Errore algoritmico (grafi computazionali)

Vogliamo valutare l'errore algoritmico della proprietà distributiva:

$$
\displaylines{
\mathrm{alg_1}=(x_1+x_2)x_3\\
\mathrm{alg_2}=(x_1x_3)+(x_2x_3)}
$$

* Consideriamo il grafo di $\mathrm{alg_1}$:

  $$
  \displaylines{
  \int\delta_2+1\left(\delta_1+\frac{x_1}{x_1+x_2}\epsilon_{x_1}+\frac{x_2}{x_1+x_2}\epsilon_{x_2}\right)+1\left(\epsilon_{x_3}\right)=\\
  =\delta_2+\delta_1+\left(\frac{x_1}{x_1+x_2}\epsilon_{x_1}+\frac{x_2}{x_1+x_2}\epsilon_{x_2}+\epsilon_{x_3}\right)}
  $$

  $$
  |\epsilon_{algoritmico}|=|\delta_2+\delta_1|\leq|\delta_2|+|\delta_1|\leq2u
  $$

* Consideriamo il grafo di $\mathrm{alg_2}$:

  $$
  \displaylines{
  \delta_3+\frac{x_1x_3}{x_1x_3+x_2x_3}\left(\delta_1+1\epsilon_{x_1}+1\epsilon_{x_3}\right)+\frac{x_2x_3}{x_1x_3+x_2x_3}\left(\delta_2+1\epsilon_{x_2}+1\epsilon_{x_3}\right)=\\
  =\delta_3+\frac{x_1}{x_1+x_2}\left(\delta_1+1\epsilon_{x_1}+1\epsilon_{x_3}\right)+\frac{x_2}{x_1+x_2}\left(\delta_2+1\epsilon_{x_2}+1\epsilon_{x_3}\right)=\\
  =\delta_3+\frac{x_1}{x_1+x_2}\delta_1+\frac{x_2}{x_1+x_2}\delta_2+\left(\frac{x_1}{x_1+x_2}\epsilon_{x_1}+\frac{x_2}{x_1+x_2}\epsilon_{x_2}+\frac{x_1+x_2}{x_1+x_2}\epsilon_{x_3}\right)=\\
  =\delta_3+\frac{x_1}{x_1+x_2}\delta_1+\frac{x_2}{x_1+x_2}\delta_2+\left(\frac{x_1}{x_1+x_2}\epsilon_{x_1}+\frac{x_2}{x_1+x_2}\epsilon_{x_2}+\epsilon_{x_3}\right)}
  $$
  L'errore *inerente* (tra parentesi nell'ultimo passaggio) è lo stesso dell'altro algoritmo, come deve essere.
  $$
  |\epsilon_{algoritmico}|=\left|\delta_3+\frac{x_1}{x_1+x_2}\delta_1+\frac{x_2}{x_1+x_2}\delta_2\right|\leq\left(1+\frac{|x_1|+|x_2|}{|x_1+x_2|}\right)u
  $$

## Stabilità

* Un algoritmo è *stabile in avanti* se l'errore algoritmico è un piccolo multiplo di $u$.
* Un algoritmo è *stabile all'indietro* se la soluzione in aritmetica di macchina è la soluzione in aritmetica esatta per un input vicino.

  Quindi, se $\forall_{\tilde x}g(\tilde x + \varepsilon)=\tilde g(\tilde x)$, e l'errore relativo $\frac{|\varepsilon|}{|\tilde x|}$ di $\tilde x+\varepsilon$ è un piccolo multiplo di $u$ (che significa che $\varepsilon$ si avvicina all'errore dell'approssimazione), allora $g$ è stabile all'indietro.
  * Nel caso della somma, si può usare l'errore relativo $\epsilon$ per valutare la stabilità, perché  $\mathrm{fl}(+)$ può essere vista come $(x_1+x_2)+\varepsilon$ con $\varepsilon$ piccolo, quindi si ottiene $\epsilon=\frac{(x_1\mathrm{fl}(+)x_2)-(x_1+x_2)}{x_1+x_2}\approx\frac{(x_1+\varepsilon+x_2)-(x_1+x_2)}{x_1+x_2}=\frac{\varepsilon}{x_1+x_2}$.\
  Questo perché l'errore relativo della somma è proporzionale all'errore relativo degli input.\
  Se $\epsilon\leq u$, $\varepsilon$ è piccolo e l'operazione si dice stabile all'indietro.

$$
|\mathrm{err}_{algoritmico}|\approx\mathrm{cond}_g(\tilde x)\frac{|\varepsilon|}{|\tilde x|}
$$

Ricordare che il condizionamento è una proprietà del problema, ed indica *quanto cambia il risultato se l'input cambia di poco*, e varia in base all'input. Quindi insieme a *quanto bisogna cambiare l'input per ottenere lo stesso risultato in aritmetica esatta*, ci dice l'errore algoritmico totale.\
È simile alla relazione tra errore inerente e condizionamento, in entrambi i casi l'errore è dato moltiplicando il condizionamento per uno sfasamento dato dall'utilizzo dell'aritmetica di macchina.

**Esempio:** consideriamo le soluzioni di un'equazione di secondo grado:

$$
x_{1,2}=\frac{b\pm\sqrt{b^2-4ac}}{2a}
$$

Se $b^2$ e $4ac$ sono molto vicini, si verifica il fenomeno della cancellazione e il risultato viene sbagliato, anche se la formula è esatta. Inoltre, uno tra i risultati di $\pm\sqrt{b^2-4ac}$ potrebbe essere l'opposto di $b$ portando sempre ad una cancellazione.
Mentre il secondo caso di può gestire, per la cancellazione con $4ac$ non c'è soluzione se non aumentare la precisione con cui si calcola.

### Stabilità di $\mathrm{alg_1}$ e $\mathrm{alg_2}$

* $\mathrm{alg_1}$ è sempre stabile, l'errore algoritmico è un piccolo multiplo di $u$.
* $\mathrm{alg_2}$ è instabile, l'errore algoritmico può crescere arbitrariamente a seconda degli input.
Quando gli input hanno segno concorde l'errore è $2u$ e l'algoritmo rimane stabile, quando hanno segno discorde diventa instabile, e se gli input lori sono opposti si ha l'errore più alto.

### Stabilità della deviazione standard

$$
\displaylines{
m_n=\frac{\sum\limits^n_{i=1}x_i}{n}\\
\sigma_n=\sqrt\frac{\sum\limits^n_{i=1}(x_i-m_n)^2}{n}}
$$

Si potrebbe anche scrivere in questa forma per fare una sola sottrazione alla fine, ma si amplifica l'errore, quindi è meglio la versione normale:

$$
\sigma_n=\sqrt\frac{\sum\limits^n_{i=1}x_i^2-\frac1n\left(\sum\limits^n_{i=1}x_i\right)^2}{n}
$$

Un'altro algoritmo numericamente stabile basato su relazioni ricorsive (non si entra nel dettaglio).

Un'altro modo è calcolarla incrementalmente:

definite le quantità:

$$
\displaylines{
m_k=\frac{\sum^k_{i=1}x_1}{k},\quad k=1,...,n\\
q_k=\sum^k_{i=1}(x_i-m_k)^2,\quad k=1,...,n}
$$

valgono le relazioni:

$$
\displaylines{
m_1=x_1,\quad m_k=m_{k-1}+\frac{x_k-m_{k-1}}k\\
q_1=0,\quad q_k=q_{k-1}+\frac{(x_k-m_{k-1})^2(k-1)}k\\
\sigma^2_n=\frac{q_n}n\\
\sigma_n=\sqrt{\frac{q_n}n}}
$$

### Stabilità dell'algoritmo di archimede per il calcolo di $\pi$

Dopo semplici calcoli si ottengono le relazioni:

$$
\displaylines{
l_1=\sqrt(2)\\
i=1,2,...\\
p_i=l_1\times2^i\\
l_{1+1}=\sqrt{2-\sqrt{4-l^2_i}}\\
p_i\rightarrow\pi,i\rightarrow\infty}
$$

Si verifica la cancellazione?\
Dipende se i due addendi, $2$ e $\sqrt{4-l^2_i}$, si avvicinano, ed ad un certo punto succede, quindi si ha la cancellazione. Per questo si usa un'altra versione.

$$
\displaylines{
  l_{i+1}=\frac{\sqrt{2-\sqrt{4-l^2_i}}}{\sqrt{2+\sqrt{4-l^2_i}}}\times\sqrt{2+\sqrt{4-l^2_i}}=\frac{\sqrt{4-(4-l^2_i)}}{\sqrt{2+\sqrt{4-l^2_i}}}\\
l_{i+1}=\frac{l_i}{\sqrt{2+\sqrt{4-l^2_i}}}}
$$

Questa versione è stabile, e dopo 25 iterazioni ci si avvicina alla precisione di macchina.
