# Metodo a pendenza costante e metodo di Newton

Una classe di metodi di iterazione funzionale sono quelli che usano:

$$
g(x)=x-\frac{1}{m(x)}f(x)
$$

Con questo metodo, calcolare $x_{k+1}$ equivale a trovare l'intersezione tra l'asse delle $x$ e una retta di coefficiente angolare $m(x)$ passante per il punto $(x_k,f(x_k))$.

Ad una iterazione $k+1$, al valore $x_k$ (base) viene sottratto un valore proporzionale ad $f(x_k)$ (altezza). Dato che $base=\frac{altezza}{inclinazione}$, lo scostamento è $-\frac{f(x)}{m(x_k)}$.

Due metodi semplici usano:

* $m(x)=m$ *metodo a pendenza costante*
* $m(x)=f'(x)$ *metodo di Newton* o *della tangente*

## Metodo a pendenza costante

Nel metodo a pendenza costante, la funzione $m(x)$ è una costante di valore $m$, e ogni retta tra $(x_{k+1},0)$ e $(x_k,f(x_k))$ ha la stessa inclinazione.\
Bisogna scegliere un valore appropriato a seconda della funzione. Se $|m|$ è troppo grande potrebbe convergere troppo lentamente (retta ripida, si sposta di poco), e se è troppo piccolo potrebbe non convergere (retta piana, che si allontana da $\alpha$). Se $m$ non è concorde ad $f'(x)$, non converge.

$$
\displaylines{
g(x)=x-\frac{f(x)}m~ \Leftrightarrow ~ g'(x)=1-\frac{f'(x)}m\\
\\
|g'(x)|<1~ \iff~ 0<\frac{f'(x)}m<2~ \Leftrightarrow ~ |m|>\frac12\max_{x\in I_\alpha}(|f'(x)|)}
$$

Il fattore di convergenza ( $l=|g'(\alpha)|$ ) dipende dal valore di $f'(\alpha)$ e di $m$. Solo se $f'(\alpha)=m$, si ha convergenza superlineare, ma non sempre si può scegliere $m=f'(\alpha)$, ad esempio quando $f'(\alpha)\not>\frac12\max_{x\in I_\alpha}(|f'(x)|)$.\
Inoltre, se $\alpha$ è una radice multipla, allora $l=|g'(\alpha)|=1-\frac0m=1$, quindi la convergenza è sublineare.

## Metodo di Newton (o metodo delle tangenti)

Per seguire meglio l'andamento di $f$, usiamo direttamente $m(x)=f'(x)$.
Converge sempre superlinearmente, tranne quando $\alpha$ è una radice multipla.

$$
\displaylines{
g(x)=x-\frac{f(x)}{f'(x)}~ \Leftrightarrow ~ g'(x)=\frac{f(x)f''(x)}{f'(x)^2}\\
\\
l=g'(\alpha)=\frac{0f''(\alpha)}{f'(\alpha)^2}=0}
$$

Se $\alpha$ non è multiplo (quindi $f'(\alpha)\neq0$), allora $g'(\alpha)$ esiste ( $0$ ), e la convergenza è superlineare (almeno quadratica).

Inoltre se $\alpha$ non è multiplo, ma $f''(\alpha)=0$ si ha:

$$
\displaylines{
g''(x)=\frac{f''(x)}{f'(x)}\\
\\
g''(\alpha)=0}
$$

Quindi la convergenza è superquadratica (ordine di convergenza $\geq3$).

### Radici multiple

Se $\alpha$ è una radice di molteplicità $\mu>1$, si perde la convergenza superlineare. La funzione $f'(x)$ al denominatore varrebbe 0 in $\alpha$, ma possiamo fare delle considerazioni aggiuntive per valutare la velocità di convergenza come un limite.\
In questi casi, si ha $l=g'(\alpha)=1-\frac1\mu$, quindi per $\mu=2$ abbiamo $l=\frac12$, per $\mu=3$ abbiamo $l=\frac23$.\
Sia $\alpha$ una radice multipla, allora esiste una funzione continua $h$ tale che:

$$
\displaylines{
f(x)=(x-\alpha)^\mu h(x)\\
\\
h(\alpha)\neq0}
$$

Un polinomio di grado $n$ può essere derivato $n$ volte prima di ottenere una costante. La funzione $(x-\alpha)^\mu h(x)$ è un prodotto tra $h$ ed il polinomio $(x-\alpha)^\mu$ di grado $\mu$, che vale 0 con $x=\alpha$.\
Ad ogni derivazione di $f$, si aggiunge un addendo dove la parte $(x-\alpha)$ ha grado più basso della derivata precedente. La derivata $f^{(\mu)}(x)$ conterrà anche un addendo con $(x-\alpha)^0$ e $h(x)$, che su $\alpha$ è diverso da zero.\
Quindi le derivate fino a $\mu-1$ valgono 0 su $\alpha$, ma la $\mu$-esima no. Ciò rispecchia il comportamento di $f$.

Ad esempio, con $\mu=2$:

$$
\displaylines{
f(x)=(x-\alpha)^2h(x)\\
\\
g'(x)=\frac{f(x)f''(x)}{f'(x)^2}=\huge(\normalsize\huge\cdots\huge)\normalsize=\frac{2h(\alpha)^2}{4h(\alpha)^2}=\frac12\neq0}
$$

### Condizioni sufficienti

Dato un intorno destro o sinistro di $\alpha$, $J_\alpha=]\alpha,\alpha+r]\lor[\alpha-r,\alpha[$, dove $f(x)f''(x)>0$ e $f'(x)\neq0$ ( $\alpha$ escluso, quindi nel caso di radici multiple $f'(\alpha)=0$ è valido), scegliendo l' $x_0$ in $J_\alpha$, la successione è monotona perché $g'(x)>0$.

$J_\alpha$ è un intervallo generico dove la successione converge, quindi potrebbe estendersi oltre $I_\alpha$, che invece è vincolato ad essere simmetrico.

Nel caso di radici semplici la condizione potrebbe verificarsi da un solo lato, e scegliendo $x_0$ dal lato opposto (dove $f(x)f''(x)\ngtr0$), al primo passaggio potrebbe finire in $J_\alpha$ e da lì continuare in modo monotono, o finire fuori da $J_\alpha$.\
Nel caso di radici multiple, converge sempre monotonamente sia scegliendo $x_0$ a destra che a sinistra, perché quando $f'(\alpha)=0$ (punto piano) allora $f$ e $f''$ avranno dello stesso segno.

## Metodi quasi-Newton

Quando non si può usare la derivata prima (perché è costosa o non è nota) la possiamo approssimare.

### Metodo delle secanti

Al posto della derivata approssimiamo il rapporto incrementale con i valori precedenti di $x_k$.

$$
f'(x_k)\approx\frac{f(x_k)-f(x_{k-1})}{x_k-x_{k-1}}
$$

Dati $x_0$ ed $x_1$ iniziali:

$$
x_{k+1}=g(x_k,x_{k-1})=x_k-\frac{f(x_k)}{f'(x_k)}=x_k-\frac{f(x_k)(x_k-x_{k-1})}{f(x_k)-f(x_{k-1})}
$$

L'iterazione successiva è l'intersezione con l'asse $x$ della secante tra i due punti $(x_k,f(x_k))$ e $(x_{k-1},f(x_{k-1}))$.

Si può dimostrare che converge in modo superlineare ma non quadratico. Più precisamente l'ordine di convergenza è $p=\frac{1+\sqrt{5}}2=1,618$.

## Condizionamento di Newton

Si considera $\tilde f(x)=f(x)+e(x)$ l'approssimazione della funzione $f$ in aritmetica di macchina, dove $\epsilon$ è un maggiorante di $e(x)$ che dipende da $\tilde f$.\
Sia $\tilde\alpha$ lo zero di $\tilde f$, in prima approssimazione si ottiene:

$$
\displaylines{
\begin{aligned}
& f(x)=\tilde f(x)-e(x)\\
\\
& f(\tilde\alpha)\approx f(\alpha)+f'(\alpha)(\tilde\alpha-\alpha) & [\text{Taylor}]\\
\\
& \tilde f(\tilde\alpha)-e(\tilde\alpha)\approx f(\alpha)+f'(\alpha)(\tilde\alpha-\alpha) & [\tilde f(\tilde\alpha)=f(\alpha)=0]\\
\\
& \Rightarrow e(\tilde\alpha)\approx-f'(\alpha)(\tilde\alpha-\alpha)
\end{aligned}}
$$

Quindi, assumendo $f'(\alpha)\neq0$ :

$$
|\tilde\alpha-\alpha|\approx\frac{|e(\tilde\alpha)|}{|f'(\alpha)|}\leq\frac\epsilon{|f'(\alpha)|}
$$

Il condizionamento dipende da $\frac1{|f'(\alpha)|}$, quindi:

* Se $|f'(\alpha)|\approx0$ il problema è *mal* condizionato
* Se $|f'(\alpha)|\gg0$ il problema è *ben* condizionato

Notare che il condizionamento dipende da $f$ mentre l'errore dipende da $\tilde f$.

Sia $[a_\epsilon,b_\epsilon]$ un intervallo dove $|f(x)|\leq\epsilon$.\
Si osserva che fuori da questo intervallo $f$ e $\tilde f$ hanno lo stesso segno, e che dentro all'intervallo possono non coincidere.\
Più è alto il condizionamento più è grande l'intervallo.

## Errori di arrotondamento di Newton

Si considera $\tilde x_k=g(\tilde x_{k-1})+\delta_{k-1}$ il valore effettivamente calcolato, dove $\delta$ è un maggiorante di $\delta_k$.

Siano:

$$
\lambda=\max_{x\in I_\alpha}(|g'(x)|)<1\\
\sigma=\frac\delta{1-\lambda}
$$

Preso $\tilde x_0$ in $I_\alpha$, si può dimostrare che:

$$
\begin{aligned}
& |\tilde x_{k+1}-\alpha|=|g(\tilde x_k)+\delta_k-g(\alpha)| & [g(\alpha)=\alpha]\\
& =|g'(\tilde\xi_k)(\tilde x_k-\alpha)|+\delta_k & [\text{teorema del valore medio tra }\tilde x_k\text{ e }\alpha]\\
& \leq\lambda|\tilde x_k-\alpha|+\delta & [g'(\tilde\xi_k)\leq\lambda,~ \delta_k\leq\delta]\\
& \leq\lambda(\lambda|\tilde x_{k-1}-\alpha|+\delta)+\delta=\lambda^2|\tilde x_{k-1}-\alpha|+(1+\lambda)\delta & [\text{all'iterazione successiva}]\\
& ... & [\text{raccoglimento }\lambda,~ \text{serie geometrica su }\delta\sum\limits^k_{i=0}\lambda^k]\\
& \leq\lambda^k|x_0-\alpha|+\frac{(1-\lambda^k)\delta}{1-\lambda}=\lambda^k|x_0-\alpha|+(1-\lambda^k)\sigma & \left[\frac{(1-\lambda^k)\delta}{1-\lambda}=(1-\lambda^k)\sigma\right]\\
& \leq\lambda^k(r-\sigma)+\sigma & [r\text{ è il raggio di }I_\alpha,~ \lambda^k\text{ tende a }0]\\
\\
& \Rightarrow |\tilde x_{k+1}-\alpha|=e_{\tilde x_{k+1}}\leq\lambda^k(r-\sigma)+\sigma
\end{aligned}
$$

$\sigma$ è una misura dell'incertezza del risultato.\
Finché  $\tilde x_k$ si trova in un intervallo $I_\alpha$ oltre $\sigma$, $\tilde x_{k+1}$ sarà sempre più vicino ad $\alpha$.\
Altrimenti non è detto che $\tilde x_{k+1}$ sia più vicino, pur restando in $[\alpha-\sigma,\alpha+\sigma]$.

## Esercizi

### Reciproco di un numero

Per trovare il reciproco di un numero $a$, $\alpha=\frac1a$ si può cercare la radice di $f(x)=a-\frac1x$.\
Scrivere la funzione di punto fisso per il metodo di Newton.

$$
\displaylines{
f'(x)=\mathrm D\left[a-\frac1x\right]=0+\mathrm D[-x^{-1}]=x^{-2}\\
f''(x)=\mathrm D[x^{-2}]=-2x^{-3}\\
\\
g(x)=x-\frac{f(x)}{f'(x)}=x-\frac{a-\frac1x}{x^{-2}}=x-(a-\frac1x)x^2=x-ax^2+x=2x-ax^2\\
}
$$

La successione $x_k$ converge nel range $]0,b[$, dove $b$ è il punto di $f$ dove la tangente passa per $(0,0)$.

$$
0=f(b)=f'(b)b\Rightarrow a-\frac1b=\frac1{b^2}b\Rightarrow ab=2\Rightarrow b=\frac2a
$$
