# Sistemi lineari di equazioni

Si possono rappresentare sistemi lineari come prodotto di matrici:

$$
\begin{cases}
\begin{aligned}
&a_{1,1}x_1+...+a_{1,n}x_n&&=b_1\\
&\vdots&&= \vdots\\
&a_{m,1}x_1+...+a_{m,n}x_n&&=b_m
\end{aligned}
\end{cases}
\\[3ex]
x_1
\left(\begin{matrix}
a_{1,1}\\
\vdots\\
a_{m,1}
\end{matrix}\right)
+...+x_n
\left(\begin{matrix}
a_{1,n}\\
\vdots\\
a_{m,n}
\end{matrix}\right)
=
\left(\begin{matrix}
a_{1,1}&...&a_{1,n}\\
\vdots&\ddots&\vdots\\
a_{m,1}&...&a_{m,n}
\end{matrix}\right)
~
\left(\begin{matrix}
x_1\\
\vdots\\
x_n
\end{matrix}\right)
=
\left(\begin{matrix}
b_1\\
\vdots\\
b_m
\end{matrix}\right)
$$

Se $x$ esiste, $b$ è una combinazione delle colonne di $A$. Quindi, l'equazione ha soluzioni se e solo se $b$ appartiene allo span di $A$.

Se le colonne di $A$ sono linearmente indipendenti, il sistema ha una sola soluzione, altrimenti o infinite o nessuna.

Verranno considerate solo matrici quadrate.

## Condizionamento

### Condizionamento su $b$

Introduciamo una perturbazione del termine noto $\delta b=\tilde b-b$, ed il rispettivo errore $\epsilon_b=\frac{||\delta b||}{||b||}$.\
Sia $\tilde x=x+\delta x$ la soluzione in aritmetica esatta del problema perturbato:
$$
\begin{aligned}
& A\cdot(x+\delta x)=b+\delta b &\\
& \Rightarrow A^{-1}\cdot(A\cdot(x+\delta x))=A^{-1}\cdot(b+\delta b)\Rightarrow x+\delta x=A^{-1}\cdot(b+\delta b) & [b=A\cdot x,~x=A^{-1}\cdot b]\\
& \Rightarrow\delta x=A^{-1}\cdot\delta b & 
\end{aligned} 
$$

Sapendo che $\mathrm{cond}(A)~\epsilon_b\geq\epsilon_x$, si vuole ottenere $\mathrm{cond}(A)$:
$$
\begin{aligned}
& \epsilon_x=\frac{||\delta x||}{||x||}=\frac{||A^{-1}\delta b||}{||x||} & [\text{definizione di }\epsilon_x]\\
& \leq\frac{||A^{-1}||~||\delta b||}{||x||} & [\text{regola 4 delle norme}]\\
& \leq\frac{||A^{-1}||~||\delta b||~||b||}{||x||~||b||} & [\text{moltiplicare e dividere per }||b||]\\
& \leq||A^{-1}||~\frac{||b||}{||x||}~\epsilon_b & \left[\frac{||\delta b||}{||b||}=\epsilon_b\right]\\
& =||A^{-1}||~\frac{||Ax||}{||x||}~\epsilon_b & [b=Ax]\\
& \leq||A^{-1}||~\frac{||A||~||x||}{||x||}~\epsilon_b & [\text{regola 4 delle norme}]\\
& =||A^{-1}||~||A||~\epsilon_b
\end{aligned}
$$

$\mathrm{cond}(A)=||A^{-1}||~||A||$ è chiamato numero di condizionamento ed è $\geq1$. Quando il numero di condizionamento è molto maggiore di 1 è mal condizionato, quando è piccolo è ben condizionato.

### Condizionamento su $b$ ed $A$

Introduciamo la perturbazione $\delta A$ della matrice $A$ ed il rispettivo errore $\epsilon_A=\frac{||\delta A||}{||A||}$, si assume che $A+\delta A$ non sia singolare.\
Si può dimostrare che:
$$
\epsilon_x\leq\frac{||A||~||A^{-1}||}{1-||A||~||A^{-1}||~\epsilon_A}(\epsilon_b+\epsilon_A)=\frac{\mathrm{cond}(A)}{1-\mathrm{cond}(A)\epsilon_A}(\epsilon_b+\epsilon_A)
$$

### Osservazioni 

Il condizionamento da una misura dell'accuratezza del risultato. Calcolare il condizionamento può essere più costoso che calcolare la soluzione, perché usa l'inversa.

## Trasformazioni

Risolvere un sistema generico è molto costoso, quindi si cerca un modo di trasformare un sistema in uno equivalente ma più facile da risolvere.\
In particolare si moltiplicano a sinistra sia $A$ che $b$ per una stessa *matrice invertibile* $M$.

$$
M\cdot A\cdot x=M\cdot b\iff x=(M\cdot A)^{-1}\cdot M\cdot b=A^{-1}\cdot M^{-1}\cdot M\cdot b=A^{-1}\cdot b
$$

Si usano:
* *matrici di permutazione*: $P$ ottenute scambiando righe dell'identità, rappresentano scambi di righe
* *matrici di scaling*: $D$ ottenute da matrici diagonali, rappresentano scaling di righe

### Casi facili

* $A$ diagonale:\
  Se $A$ è diagonale, tutte le $n$ equazioni del sistema sono nella forma $a_{i,i}x_i=b_i$, quindi è più veloce risolverle individualmente con $x_i=\frac{b_i}{a_{i,i}}$.
* $A$ triangolare:\
  Se $A$ è triangolare, ogni equazione coinvolge solo una variabile in più della precedente, e la prima sarà nella forma $ax=b$. È più veloce risolverle in ordine.
* $A$ è ortogonale:\
  Una matrice $A$ si dice ortogonale se la sua trasposta è la sua inversa $A\cdot A^T=I$.\
  È più veloce perché non serve calcolare l'inversa e basta fare il prodotto $A^T\cdot b$.

## Metodi diretti

Si riscrive $A$ come un prodotto $M\cdot N$ di matrici facili, dopodiché si risolve:
$$
\begin{cases}
\begin{aligned}
& M\cdot z &&=b & [\text{caso facile per }z]\\
& N\cdot x &&=z & [\text{caso facile per }x]
\end{aligned}
\end{cases}
$$

### Fattorizzazione $\textcolor{MediumSeaGreen}{L}\textcolor{LightCoral}{U}$

Si può ricondurre una matrice generica al prodotto di una matrice unitriangolare inferiore $\textcolor{MediumSeaGreen}{L}$ ed una triangolare superiore $\textcolor{LightCoral}{U}$.

Una matrice unitriangolare è una matrice triangolare con tutti 1 sulla diagonale.

Ogni elementi di $A$ è il prodotto di una riga di $\textcolor{MediumSeaGreen}{L}$ ed una colonna di $\textcolor{LightCoral}{U}$:
$$
\begin{aligned}
& a_{i,j}=\sum^{\min(i,j)}_{k=1}l_{i,k}u_{k,j} & [\text{oltre $\min(i,j)$, $l_{i,k}=0$ o $u_{k,j}=0$}]
\end{aligned}
$$

Si può scrivere un sistema di equazioni con ciascuna termine noto $a_{i,j}$. Si risolve in ordine per tutti gli $u_{i,j}$ e $l_{i,j}$ partendo da $a_{1,1}=l_{1,1}u_{1,1}$, dove $l_{1,1}=1$ e quindi $u_{1,1}=a_{1,1}$.

Non è sempre possibile fattorizzare la matrice.

### Metodo di eliminazione di Gauss

Il metodo di eliminazione di Gauss permette di fattorizzare $A=\textcolor{MediumSeaGreen}{L}\cdot\textcolor{LightCoral}{U}$, dove $\textcolor{MediumSeaGreen}{L}$ è costruita da diverse matrici $G_i$ facilmente invertibili e moltiplicabili.

Dato un vettore $v$ ed un suo componente $v_k\neq 0$ detto pivot, si può costruire una matrice unitriangolare inferiore $G_k$ che applicata a $v$ rende 0 tutti i valori sotto al pivot:
$$
G_k\cdot v=
\left(\begin{matrix}
1&...&0&0&...&0\\
\vdots&\ddots&\vdots&\vdots&\ddots&\vdots\\
0&...&1&0&...&0\\
0&...&\textcolor{DeepSkyBlue}{-\frac{v_{k+1}}{v_k}}&1&...&0\\
\vdots&\ddots&\textcolor{DeepSkyBlue}{\vdots}&\vdots&\ddots&\vdots\\
0&...&\textcolor{DeepSkyBlue}{-\frac{v_n}{v_k}}&0&...&1\\
\end{matrix}\right)
\cdot
\left(\begin{matrix}
v_1\\
\vdots\\
v_k\\
v_{k+1}\\
\vdots\\
v_n
\end{matrix}\right)
=
\left(\begin{matrix}
v_1\\
\vdots\\
v_k\\
\textcolor{DeepSkyBlue}{0}\\
\textcolor{DeepSkyBlue}{\vdots}\\
\textcolor{DeepSkyBlue}{0}
\end{matrix}\right)
$$

Per calcolare l'inversa di $G_k$, è sufficiente invertire il segno dei <span style="color: DeepSkyBlue">moltiplicatori</span> sotto la diagonale.
$$
G_1\cdot A=B,~G_2\cdot B=C,..., G_{n-1}\cdot Y=Z\\
\left(\begin{matrix}
1&0&0&...&0\\
0&1&0&...&0\\
0&\textcolor{DeepSkyBlue}{-\frac{b_{2,3}}{b_{2,2}}}&1&...&0\\
\vdots&\textcolor{DeepSkyBlue}{\vdots}&\vdots&\ddots&\vdots\\
0&\textcolor{DeepSkyBlue}{-\frac{b_{2,n}}{b_{2,2}}}&0&...&1\\
\end{matrix}\right)
\cdot
\left(\begin{matrix}
a_{1,1}&a_{2,1}&a_{3,1}&...&a_{n,1}\\
0&\textcolor{DarkOrange}{b_{2,2}}&\textcolor{DarkOrange}{b_{3,2}}&\textcolor{DarkOrange}{...}&\textcolor{DarkOrange}{b_{n,2}}\\
0&\textcolor{DarkOrange}{b_{2,3}}&\textcolor{DarkOrange}{b_{3,3}}&\textcolor{DarkOrange}{...}&\textcolor{DarkOrange}{b_{n,3}}\\
\vdots&\textcolor{DarkOrange}{\vdots}&\textcolor{DarkOrange}{\vdots}&\textcolor{DarkOrange}{\ddots}&\textcolor{DarkOrange}{\vdots}\\
0&\textcolor{DarkOrange}{b_{2,n}}&\textcolor{DarkOrange}{b_{3,n}}&\textcolor{DarkOrange}{...}&\textcolor{DarkOrange}{b_{n,n}}\\
\end{matrix}\right)
=
\left(\begin{matrix}
a_{1,1}&a_{2,1}&a_{3,1}&...&a_{n,1}\\
0&b_{2,2}&b_{3,2}&...&b_{n,2}\\
0&\textcolor{DeepSkyBlue}{0}&\textcolor{DarkOrange}{c_{3,3}}&\textcolor{DarkOrange}{...}&\textcolor{DarkOrange}{c_{n,3}}\\
\vdots&\textcolor{DeepSkyBlue}{\vdots}&\textcolor{DarkOrange}{\vdots}&\textcolor{DarkOrange}{\ddots}&\textcolor{DarkOrange}{\vdots}\\
0&\textcolor{DeepSkyBlue}{0}&\textcolor{DarkOrange}{c_{3,n}}&\textcolor{DarkOrange}{...}&\textcolor{DarkOrange}{c_{n,n}}\\
\end{matrix}\right)
$$

Per $i=1,...,n-1$ passaggi si costruisce $G_i$ con pivot $a_{i,i}$. Calcolando $G_i\cdot X=Y$, si ottiene la matrice $Y$ con le prime $i$ righe intatte, e le prime $i$ colonne con solo 0 sotto la diagonale. Il resto della matrice viene alterato, e l'algoritmo procede solo su di esso.\
Al passaggio $n-1$ si ottiene $Z$ triangolare superiore. La serie di trasformazioni si può rappresentare come un unica matrice $G=G_{n-1}\cdot...\cdot G_1$ unitriangolare inferiore.

La matrice inversa $G^{-1}=G^{-1}_1\cdot...\cdot G^{-1}_{n-1}$ è unitriangolare inferiore ed è facile da calcolare: il prodotto si ottiene compilando la parte inferiore di una $G^{-1}$ con i moltiplicatori di tutte le altre $G^{-1}$.

Si ottiene la fattorizzazione $A=\textcolor{MediumSeaGreen}{G^{-1}}\cdot\textcolor{LightCoral}{Z}$.

Applicando a $b$ le stesse trasformazioni $G$, si ottiene direttamente $z=\textcolor{MediumSeaGreen}{L}^{-1}\cdot b=G\cdot b$.

Il metodo di eliminazione di Gauss funziona solo se tutti i pivot sono $\neq0$. Si può dimostrare che vale solo se ogni sottomatrice quadrata in alto a sinistra è non singolare.

### Metodo di Gauss con Pivot Parziale

Il metodo di Gauss con pivot parziale funziona su ogni matrice non singolare, ma migliora la stabilità dell'algoritmo.

Ad ogni passo si sceglie una riga con pivot $\neq0$ tramite permutazioni.

Ad un passaggio $k$ con pivot $x_{k,k}=0$ si esegue una permutazione con una delle righe successive in cui $x_{k,i}\neq 0$, più precisamente si sceglie sempre la riga in cui $|x_{k,i}|$ ha il valore massimo.\
Scegliere il massimo permette di avere una stabilità maggiore.

La matrice $G=G_{n-1}\cdot P_{n-1}\cdot...\cdot G_1\cdot P_1$ e la sua inversa $G^{-1}$ non sono più unitriangolari inferiori a causa delle permutazioni, però vale ancora $G^{-1}\cdot Z\cdot x=b$, però moltiplicando la matrice di permutazione $P=P_{n-1}\cdot...\cdot P_1$ a $G^{-1}$ si ottiene di nuovo una unitriangolare inferiore (una matrice di permutazione è l'inversa di se stessa).\
Quindi moltiplicando $P$ ad entrambi i lati dell'equazione si ottiene $(P\cdot G^{-1})\cdot Z\cdot x=P\cdot b$ dove $P\cdot G^{-1}=L$ e $Z=U$.

Se ad ogni passaggio si eseguono le stesse trasformazioni sia su $A$ che su $b$, si subito già la soluzione di $P\cdot G^{-1}\cdot z=P\cdot b$. In questo modo è sufficiente risolvere $Z\cdot x=z$.