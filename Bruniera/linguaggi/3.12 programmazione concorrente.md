# Programmazione concorrente

Non è un paradigma a se stante, ma si può inserire in paradigmi già esistenti con una pletora di meccanismi.

* Utilizzabile in:
  * Linguaggi imperativi, ad oggetti
    * Con una vasta quantità di meccanismi
  * Linguaggi logici, funzionali
    * Avantaggiati dalla mancanza di stato
    * Tipico l'esempio di erlang, sviluppato per la gestione di messaggi
      * Permette di avere migliaia di processi in esecuzione contemporaneamente
* Motivazioni:
  * L'hardware moderno è orientato al parallelismo
    * È difficile aumentare ancora il clock
    * A volte sono previste istruzioni apposta per parallelizzare meglio
  * Alcuni programmi si descrivono meglio in modo concorrente
    * Esempio: browser
    * Si usa un thread per ogni elemento della pagina da caricare
    * In questo modo immagini grandi non bloccano il caricamento della pagina

## Vari livelli di concorrenza:

* Concorrenza fisica
  * Istruzioni simultanee
  * Diversi modelli e granularità
    * Pipeline
      * Separo l'istruzione in più fasi e le mando avanti come catena di montaggio
    * Superscalarità
      * Istruzioni che usano esecutori diversi vangono eseguite in contemporanea
    * Istruzioni vettoriali
      * Operazioni simultanee sugli elementi del vettore
    * Multicore
      * Flussi di istruzioni eseguiti in contemporanea da core diversi
    * Multithread
      * Flussi gestiti in contemporanea dallo stesso core
    * SIMD
      * Simile ad istruzioni vettoriali, ma più generiche
    * Multicomputer
      * Più processori che condividono parte della memoria memoria
    * Reti di calcolatori
      * Più computer che collaborano
* Concorrenza logica
  * Programmazione parallela
    * Si cerca di parallelizzare l'esecuzione di un singolo problema
    * Algoritmi paralleli
  * Programmazione multithreaded
    * Più thread attivi sulla stessa macchina
    * Memoria condivisa, oppure scambio di messaggi
  * Programmazione distribuita
    * Programmi indipendenti che collaborano
    * Eventualmente separati su macchine diverse
    * Non si può assumere che la memoria sia condivisa

Logico e fisico non corrispondono, un programma con una certa concorrenza logica può essere eseguito con diversi meccanismi fisici, distingubili solo per prestazioni, non per possibili risultati.

## Meccanismi per la programmazione concorrente:

* Metodi per introdurre la programmaizone concorrente
  * Prendere un linguaggio imperativo ed aggiungere delle chiamate di libreria per la gestione dei thread
    * Es: C e pthread
  * Estensioni supportate dal compilatore
    * Fortran ed OpenPM, usano direttive del compilatore
    * Sono chiamate "pragma"
  * Costrutti ad hoc del linguaggio
    * `async` ed `await` in Rust
* Aspetti 
  * //TODO
* Thread e processi:
  * Thread
    * Esegue una specifica computazione/sequenza di comandi
    * Memoria condivisa
  * Processi
    * Eseguono programmi diversi
    * Memoria separata
  * Generalmente i thread sono simili a processi leggeti
* Creazione di nuovi thread e processi:
  * Fork/Join
    * Primitive per lanciare un processo ed attenderne la terminazione
  * Co-begin
  * Parallel loops
    * Cicli eseguiti in parallelo (usano i pragma)
  * Early replay
    * La procedura restituisce il controllo prima della terminazione

## Meccanismi di comunicazione

* Memoria condivisa
  * Accesso rw ad una sona comune
  * Presuppone che una memoria in comune esista
  * Possibili differenze tra logico e fisico
* Scambio di messaggi
  * //TODO
* Blackboard
  * //TODO

## Meccanismi di sincronizzazione

Permettono di controllare l'ordine relativo delle operazioni tra i progetti. In pratica ferma alucni processi finché non è sicuro eseguire l'operazione successiva, in considerazione degli altri processi.

Sono necessari per gestire le race condition. Si tratta di quei casi in cui diversi processi cercano di accedere ad una risorsa comune.

Meccanismi:
* Con memoria condivisa
  * Mutua esclusione
  * //TODO
* Con scambio di messaggi
  * Sincronizzazione implicità nei meccanismi di attesa dei messaggi

### Mutua esclusione
La sincronizzazione può essere implementata in due modi principali:
* Attesa attiva
  * Spinlock e busy waiting
  * Ha senso solo sui multiprocessori (e anche anche)
  * Si basano su meccanismi di test-and-set, implementati ad alto livello come varaibili atomiche, e con istruzioni macchina ad hoc
* Sincronizzazione basata sullo scheduler
  * Interviene il sistema operativo od il runtime
  * Il processo viene autosospeso e viene svegliato quando forse può riprendere

### Attesa su condizione (barriera)

//TODO

### Semafori

Soluzione proposta da Dijkstra.

È un tipo astratto:
* Tipo di dato
  * Semaforo
  * Insieme dei numeri naturali (positivi)
  * Due istruzioni di base
* `P(S)`:
  * Accesso al semaforo
  * Se `S>0` decrementa atomicamente ed entra
  * Se `S=0` attende
* `V(S)`:
  * Incrementea atomicamente `S`
  * Eventualmente sveglia un processo in attesa

### Monitor

Vedi sistemi operativi.

#### Variabili condizionali

Variabili che indicano lo stato del monitor. È possibile antrare nel monitor per esaminare la risorsa (in sicurezza), ed eventualmente mettersi in attesa che lo stato cambi per liberare l'accesso al monitor.
Più processi possono accedere per leggere la variabile condizionale, ma non si possono eseguire altre operazioni sul monitor in contemporanea. Realizza una specie di `trylock`, ma con più stati possibili.
