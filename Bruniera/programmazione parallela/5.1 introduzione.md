# Introduzione

Cosa è il parallel computing:
* *Non* è programmazione seriale
  * Il programma è una sequenza di istruzioni
  * Le istruzioni sono eseguite una alla volta
  * Le istruzioni sono eseguite da un singolo processore
  * In ogni istante viene eseguita una sola istruzione
* Il programma è diviso in parti concorrenti
* Ogni parte è una successione di istruzioni
* Le istruzioni di una parte sono eseguite sequenzialmente da un processore
* In ogni istante più istruzioni vengono eseguite da diversi processori
* Serve un meccanismo di controllo

Le architetture tradizionali sono costituite da processori con molti core, o da grandi quantità di processori connessi ad una rete di qualche tipo.
Le architetture "moderne" sono le GPU. Si tratta di processori con core molto semplici, e con un sistema di controllo molto più base, ma con migliaia di core.

## Modelli

Un modello di programmazione può avere memoria condivisa o memoria distribuita.
* Shared memory
  * Tutti i processori accedono alla stessa memoria *direttamente*
  * UMA
  * Più facile da programmare, non bisogna stare attenti a dove si trovano i dati
  * In realtà gestire gli accessi concorrenti ed il caching è molto più complicato (false-sharing)
  * Gli accessi vanno coordinati per gestire le race condition
  * Li programmeremo con OpenMP
* Distributed memory
  * Ogni processore ha accesso *diretto* ad una porzione della memoria
  * NUMA
  * Risolve il problema dele race condition, ogni processore ha la sua memoria individuale
  * I processori devono comunicare per ottenere i dati dalle altre memorie (message passing)
  * Li programmeremo con MPI
  * Scalano meglio
  * Bisogna tenere conto della località dei dati
  * Bisogna tenere conto della topologia della rete

## Decomposizione di un problema

Ci sono principalmente due modi di partizionare un lavoro in task paralleli:
* Data decomposition
  * Il task è sempre lo stesso (più o meno) per ogni processore
  * Ogni processore si occupa di una porzione dei dati su cui eseguire il task
* Task decomposition
  * Ogni processore si occupa di un task diverso
  * Un task può dover attendere l'output di altri task
  * Dataflow programming (un task si attiva quando sono disponibili i dati)