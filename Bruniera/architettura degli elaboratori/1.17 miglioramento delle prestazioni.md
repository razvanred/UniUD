# miglioramento delle prestazioni

* utilizzare istruzioni più potenti a parità di clock
* ridurre la durata del ciclo di clock
* eseguire più istruzioni nella stessa unità di tempo
    * diminuire il numero di microistruzioni per istruzione
    * aumentare le operazioni svolte in un ciclo di clock

---
---
## ridurre il ciclo di clock

il periodo di clock deve essere maggiore della somma dei riardi necessari alla CPU per cunzionare

alcuni dei ritardi soo divuti al propagarsi dei segnali di controllo. altri sono dovuti al propagarsi dei ritardi delle porte logiche nella ALU
altri ancora al trasferimento delle word nei bus

si può ottenere in due modi:
* migliorado la tecnologia dei circuiti (transistor più veloci)
* migliorandone l'organizzazione (meno ritardo con lo stesso numero di transistor)

---
---
## più operazioni per ciclo di clock

aumentare la potenza di calcolo del datapath, ovvero, avere microistruzioni più potenti:

* utilizzando ALU con più funzioni
* aumentare i registri della cpu disponibili
* più possibilità di scambio dei dati (distinguere i bus)
* utilizzare un unità separata per il fetch delle istruzioni (IFU)

---
### IFU

si interfaccia con la memoria centrale e carica l'istruzione o le istruzioni che seguono quella corrente mentre quella attuale viene eseguite

non è detto che le istruzioni caricate siano quelle da eseguire successivamente, potrebbero esserci jump o potrei modificare i dati delle operazioni sucessive ma c'è una buona probabilità che lo siano

---
### Mic-3 parallellismo sui bus
pipeline:
* esecuzione parallela. spezza una microistruzione in pù stati che possono essere eseguiti contemporaneamente (pipelining)
* l'esecuzione viene divisa in 3 stadi Fetch Decode Execute
* ogni stadio eseguito in parallelo
* i diversi stadi eseguono più istruzioni in contemporanea
* i percorsi sui bus sono più brevi => meno ritardi => clock più brevi

se un'istruzione deve attendere i risultati della precedente per poter eseguire uno degli stadi gli tocca aspettare

ho bisogno di un agente specifico per svolgere ciascuna delle fasi per parallelizzare

si parallelizzano le microoperazioni

tutto ciò non migliora i tempi di risposta ma la banda passante (dati che attraversano il datapath):
faccio iniziare 3 operazioni nel tempo in cui eseguo una, ma ogni operazione ci metterà lo stesso tempo per essere eseguita

tecnica utilizzata da tutti i processori intel successivi al 486

processori diversi probabilmente scompongono la pipeline in modo diverso, l'esempio è con 3 o 5 fasi, ma tipicamente sono tra i 7 e i 14.

caso limite: il pentium IV aveva 20 stadi

---
---
## processori superscalari

aumentano il parallellismo ulteriormente: aumenta il rapporto istruzioni/ciclo

iniziano contemporaneamente l'esecuzione di più istruzioni:
più pipeline operanti nello stesso momento

i primi stadi prelevano più istruzioni e le decodificano
le istruzioni sono smistate si stadi successivi multipli
deve esserci uno stadio dinale singolo che termina ordinatamente le istruzioni

attualmente ci sono processori con 4-15 pipeline, eseguono decine di microistruzioni in contemporanea

soo vincolate  da due fattori che ne impediscono il completo sfruttamento
* dipendenza tra istruzioni
* istruzioni di salto

---
### dipendenza tra istruzioni

le istruzioni di un programma sono pensate per essere eseguite in ordine.
eseguirle in parallelo senza ocntrollo può causare:

* **RAW: Read After wWite**: R0 = R1 -- R2 = R0+1
* **WAR: Write After Read**: R1 = R0+1 -- R0 = R2
* **WAW Write After Write**: R0 = R1 -- R0 = R2+1

per risolvere le dipemdenze bisogna rilevarle stamite una tabella (scoreboard):
si tratta di una memoria interna al processore che conta per ogno registro le istruzioni in attesa del registro e ne controlla l'ordine di accesso.
si creano delle zone inattive della pipeline

esistono dellle tecniche per recuperare le prestazioni perse:
* **esecuzione fuori ordine**: eseguire le istruzioni non dipendenti
* **registi ombra**: cipie di registri per memorizzare dati temporanei
* **register renaming**: uovi registri non specificari dal codice
* **multithreading**: si eseguono più programmi contemporaneamente, servono copie di alcuni registri.

---
### salti condizionati

problematiche pre i processori con pipeline:
* il processore impiega più cicli per valutare una condizione
* nel frattempo non sa cosa eseguire

soluzioni:
* stall: aspetta che sia terminato il controllo, si perdono operazioni
* predizione di salto: si inizia l'esecuzione condizionata, si annulla se la predizione era sbagliata.

i programmi usano molti salti, la predizione assicura prestazioni migliori.
gli algoritmi di predizione sono molto complessi


#### Tecniche di predizione:

due tecniche:
* predizione statica sul codice:
    * semplice: si eseguono in anticipo  tutti i salti all'indietro
    * con suggerimento: il compilatore (profiling del codice) o il programmatore decidono cosa anticipare
* predizione dinamica: utilizza una history table per predirre più accuratamente i salti (controllare le diapositive per questa)

#### Esecuzione speculativa

non si predice, si eseguono entrambii rami.

si mandano le operazioni al processore anche se si sa che veranno scartate

problemi:
* l'esecuzione deve essere reversibile, si usano registri ombra e istruzioni che generano trap
* evitare l'esecuzione di costosi fetch dalla memoria principale. (speculative-load)

il pre-load delle variabili è delegato al compilatore, il pre-store non conviene

---
---
## memoria principale troppo lenta
la dofferenza di velocità tra memoria e processore è aumentata col tempo

l'accesso è una operazione costora perché il processore può dover aspettare diversi cicli di clock per il dato, anche decine di cicli

per questo si usa la memoria **cache** più costosa ed veloce, che diventa efficace se contiene i dati richiesti più frequentemete

la cache è più piccola e vicina al processore

### funzionamento della memoria cache

* prima si cerca nella cache (cache hit)
* in caso di fallimento (cache miss) si carica il dato in cache dalla memoria principale

tutto ciò è al di fuori del controllo del programmatore e del compilatore, si tratta di meccaniche del processore

le prestazioni migliorano solo se ci sono numerosi cache hit: quindi si sfrutta il principio di località spaziale e temporale

### più livelli

per evitare i cache miss si usano più livelli di cache aumentando la complessità algoritmica.

la cache di secondo livello è:
* più ampia
* tecnologia meno costosa
* più lenta
* contiene un sovrainsieme della cache di primo livello

spesso i livelli sono 3.

es: un i7 sandybridge ha 32kB, 128kB, 1 mB

### split della cache
le moderne cache sono divise tra dati ed istrizioni.

* posso parallelizzare l'accesso alla memoria:
    * l'unità IFU accede alle istruzioni
    * l'unità dispatch/execure accede ai dati
* raddoppio gli accessi per unità di tempo

### esempio di configurazione
una configurazione tipica è quella di avere i due chip di l1 già separati tra istruzionie dati, la l2 che li unifica e la l3 che unifica anche i dati dei vari controller e della memoria

---
---
## valutazione delle prestazioni
la velocità di un processore superscalare dipende fortemente da quanto viene sfruttato il parallellismo:

* la percentuale di istruzioni non bloccate per dipendenze
* la percentuale delle predizioni corrette
* la percentuale degli hit