# Algoritmi randomizzati

Libro: Raghavan Motvani "An introduction to randomized algorithms"

Caratteristiche degli algoritmi randomizzati:
* Run diverse, risultati diversi
* Il modello di calcolo ha un generatore di randomness, che ha un costo
* Esistono algoritmi Las Vegas e Monte Carlo

Intuizioni
* RandQS: portare l'ipotesi di uniformità dentro l'algoritmo
* Min-Cut: barattiamo complessità con probabilità di errore
  * Stiamo cercando il taglio di cardinalità minima in un grafo
  * Usiamo un algoritmo montecarlo, riproviamo più volte e man mano la probabilità che la soluzione sia sbagliata, diminuisce arbitrariamente
* RandAuto

## Randomized quicksort

Conosciamo qs classico. Assumendo che tutte le configurazioni siano equiprobabili, il caso medio di qs è $O(n\log n)$ (nel caso pessimo è $=(n^2)$). RandQS avrà come caso medio $O(n\log n)$ senza bisogno di alcuna assunzione.

Succederà spesso che gli algoritmi randomizzati raggiungono la complessità che algoritmi non random raggiungono con qualche assunzione, senza bisogno di fare assunzioni.

## Min-Cut

\\TODO

## RandAuto

> ***Problema generico***:
> 
> Dati $n$ segmenti in  $\R^2$ *non* intersecantesi $S=s_1,...,s_n$, voglio determinare una partizione del piano ottenuta mediante rette, semirette, segmenti, tale che ogni cella contenga un segmento di $S$ o una sua porzione.

In particolare, non cerchiamo qualsiasi suddivisione, ma una Binary Planar Partition (BPP).
Una BPP è ottenuta dividendo prima il piano in due semipiani con una retta, e ogni semipiano può essere a sua volta tagliato in due da una semiretta.
Forma una sorta di struttura ad albero binario.

> ***Definizione***:
>
> Una BPP è un albero binario in cui i nodi sono coppie $(l,r)$ con $r$ regione, e $l$ linea che partizione $r$ in $r_{left}$ e $r_{right}$. E la cui radice ha $r=\R^2$.
>
> Se $r_{left}$ o $r_{right}$ non contengono parte di $S$, la suddivisione è inutile. Cerchiamo le suddivisioni di cardinalità minima in cui ogni sezione foglia contiene solo un segmento o parte di esso.

Applicazione dei BPP: painter's algorithm. Organizziamo gli oggetti in ordine dal più "distante" al più "vicino", per ottimizzare il calcolo dell'occlusione.\
Un'altra applicazione è per decidere quali sezioni di una scena ridisegnare quando viene modificato un'oggetto.

Si chiama RandAuto perché non trova una qualsiasi BPP, ma una auto-partizione, ovvero una BPP in cui le linee coincidono con i segmenti di $S$

Algoritmo:
* Scelgo una permutazione $\pi$ di $s_1,...,s_n$
* While una regione contiene più di un segmento (o parti), taglio la regione con il primo segmento in $\pi$ che interseca $r$
* L'output è l'autopartizione $P_\pi$

> ***Teorema***: La dimensione attesa dell'output di RandAuto è $O(|S|\log|S|)$
>
> ***Corollario***: Esiste una autopartizione di dimensione $O(|S|\log|S|)$

Esercizio: dimostrare che la complessità con valore atteso di RandAuto è  $O(n\log n)$

> ***Dimostrazione***: Dati $u,v\in S$, definiamo $index(u,v)=i$ sse $l(u)$ interseca $i-1$ segmenti prima di intersecare $v$. $index(u,v)=\infty$ se $l(u)$ non interseca $v$.
>
> Indichiamo che $l(u)$ taglia $v$ con $u\dashv v$.
>
> Mi interessa stimare quanti segmenti vengono tagliati dall'introduzione delle $l$ che costituiscono la BPP.
>
> Nota: $u\dashv v$ solo se $u$ occorre prima di $u_1,...,u_{i-1}v$ in $\pi$. Questo avviene con probabilità $\frac{1}{i+1}$
>
> Calcolo il valore atteso di $|P_\pi|$:
> 
> $$E[|P_\pi|]=\\n+E[\sum_u\sum_{v\neq u}C_{u\dashv v}]=\\n+\sum_u\sum_{v\neq u}E[C_{u\dashv v}]=\\n+\sum_u\sum_{v\neq u}P[u\dashv v]=\\n+\sum_u\sum_{v\neq u}\frac{1}{index(u,v)+1}\leq\\n+\sum_u\sum_i\frac{2}{i+1}\leq\\n+n2H_i\in\\O(n\log n)$$

## Macchine di Turing "randomizzate"

Sono macchine di Turing normali, più un certo numero di "random bit" a piacere.

Quindi oltre alle classiche operazioni da macchina di Turing (muovi, scrivi, ...) abbiamo anche l'operazione flip, che lancia una moneta ed ha un esito casuale.

Ricordiamo la classica funzione di transizione delle macchine di Turing $\delta:(S\times\Sigma)\mapsto(S\cup\{halt,yes,no\})\times\Sigma\times\{\leftarrow,\rightarrow,stay\}$. Lo stato va in $halt$ per terminare nelle macchine *funzionali* e va in $yes$ o $no$ per terminare nelle macchine decisionali.

Definizione formale $M=(S,\Sigma,\delta,s)$:
* $S$ stati
* $\Sigma$ alfabeto
* $s$ stato iniziale
* $\delta:(S\times\Sigma)\mapsto(S\cup\{halt,yes,no\})\times\Sigma\times\{\leftarrow,\rightarrow,stay\}$
  * La "segnatura" è classica
  * Quando la definiamo, ad ogni passo posso anche generare *un* random bit che fa scegliere tra due output casuali della funzione di transizione
  * All'atto pratico, si usa una funzione pseudorandom con un "seed" che arriva da sorgenti esterne alla macchina

La macchina segue un ramo di un albero delle computazioni in modo simile ad una macchina non deterministica, ma segue un solo ramo, ed alla fine ha una certa probabilità che l'output sia corretto.
La complessità è l'altezza dell'albero.

Un algoritmo può essere:
* Las Vegas: la soluzione è corretta sempre, ma può non terminare
  * Ci interessa l'altezza media
* Monte Carlo: la soluzione può essere sbagliata ma termina sempre
  * Ci interessa la probabilità che sia corretto il $si$ e che sia corretto il $no$
  * Si dividono ulteriormente tra quelli che sbagliano solo sul $si$ o sul $no$, e quelli che sbagliano su entrambi

Quindi dobbiamo studiare un albero di computazioni, che intuitivamente ricorda il non determinismo.

### Modelli di costo

* Un passo = costo 1
  * È comodo, ma non è realistico, come per le macchine RAM
* Un passo = costo $\log(n)$
  * Più operazioni casuali faccio, più costa
  * Se ho ottenuto $O(f(n))$ col modello costo 1, con questo modello sarà sempre minore o uguale ad $O(f(n)\log(n))$, coincide col costo reale quando faccio $O(n)$ operazioni casuali

Una MdT-computazione di lunghezza polinomiale è sempre simulabile da una RAM-computazione di lunghezza polinomiale, *e viceversa* (random access machine).\
Giustifica l'interesse per le classi $P$, $NP$, etc...

### Classi di complessità

Classi importanti:
* $P$ classe de linguaggi $L$ per cui esiste un algoritmo $A(\circ)$ di complessità polinomiale tale che:
  * $x\in L\iff A(x)=yes$
  * $x\notin L\iff A(x)=no$
  * Non è ammesso che non si fermi
* $NP$ " " " " $A(\circ,\circ)$ di complessità polinomiale tale che
  * $x\in L\iff \exists y\in\Sigma^*.A(x,y)=yes\land|y|$ è polinomiale rispetto ad $|x|$
  * $x\notin L\iff \forall y\in\Sigma^*.A(x,y)=no$ senza altri requisiti

Definizioni importanti:
* Riduzione polinomiale di $L_1$ ad $L_2$: è una funzione $f:\Sigma^*\mapsto\Sigma^*$ tale che:
  * $f$ è polinomiale
  * $\forall x\in\Sigma^*.(x\in L_1\iff f(x)\in L_2)$
  * Se esiste una riduzione si scrive $L_1\preceq L_2$
* Hardness: $L$ è $NP$-hard (o qualsiasi altra classe) se $\forall L'\in NP.L'\preceq L$ 

> La classe $RP$ è la classe dei linguaggi $L$ per cui esiste un algoritmo randomizzato $A(\circ)$ che nel caso pessimo è polinomiale e tale che:
> * $x\in L\Rightarrow P[A(x)=yes]\geq\frac12$
> * $x\notin L\Rightarrow P[A(x)=yes]=0$
>
> Questi $A(\circ)$ algoritmi con one-sided error (se non appartiene non sbaglia).
>
> Sono problemi che ammettono un algoritmo Monte Carlo polinomiale nel caso pessimo e che sbaglia solo in un caso ($x\in L$).
>
> Quel $\frac12$ è arbitrario, possiamo reiterare l'algoritmo quanto vogliamo per abbassare la probabilità. Come min cut. Funziona solo perché è one-sided

> La classe co-$RP$ è la classe dei linguaggi $L$ tali che $\overline L\in RP$, ovvero quelli per cui esiste un algoritmo  tale che:
> * $x\in L\Rightarrow P[A(x)=no]=0$
> * $x\notin L\Rightarrow P[A(x)=no]\geq\frac12$

Ovviamente, se non commetto errori i nessuno dei due casi, è Las Vegas.

> La classe $ZPP$ (zero-probabilistic polynomial time) è la classe dei linguaggi $L$ per cui esiste un algoritmo $A(\circ)$ tale che $A(x)=yes\iff x\in L$ e $A(x)=not\iff x\notin L$, ed il *valore atteso* dell complessità è polinomiale

> La classe $PP$ (probabilistic polynomial) è la classe dei linguaggi $L$ per cui esiste un algoritmo randomizzato $A(\circ)$ che nel caso pessimo è polinomiale e tale che:
> * $x\in L\Rightarrow P[A(x)=yes]>\frac12$
> * $x\notin L\Rightarrow P[A(x)=yes]<\frac12$
>
> Per questi non funziona la tecnica di ripetere più volte per ridurre la probabilità di errore. Restano puramente probabilistici.
> Per questo viene considerata insoddisfacente.

> La classe (più popolare) $BPP$ (bounded probabilistic polynomial) è la classe dei linguaggi $L$ per cui esiste un algoritmo randomizzato $A(\circ)$ che nel caso pessimo è polinomiale e tale che:
> * $x\in L\Rightarrow P[A(x)=yes]\geq\frac34$
> * $x\notin L\Rightarrow P[A(x)=yes]\leq\frac14$
>
> Torna a funzionare la tecnica delle ripetizioni.
>
> La probabilità $\frac34$ vs $\frac14$ è arbitraria

Nascono un sacco di problemi aperti.

## "The law of disorder"

Libro: Gamow (1947) "one, two, three,... infinity". Il libro contiene molti problemi tra cui questo, contiene anche la torre di Hanoi.

> Immaginiamo una piazza con al centro un lampione ed attaccato un ubriaco. Ad un certo punto l'ubriaco decide che è ora di tornare a casa. Da buon ubriaco non si muoverà dritto, ma in modo disordinato.
>
> Dopo un certo tempo, sapendo che fa un passo al secondo, posso dire a che distanza dal lampione sarà?
>
> Esiste un teorema per sapere se prima o poi abbandonerà la piazza?

La risposta è si, il teorema esiste, e la distanza probabile è $\sqrt n$ ($n$ sono i passi/secondi)

Come ce ne convinciamo?

> Supponiamo che l'$i$-esimo passo mi porti $x_i$ metri in direzione $x$ ed $y_i$ metri in direzione $y$. Sia $R$ il raggio dal lampione all'ubriaco, abbiamo che (norma del raggio) $R^2=(x_1+...+x_n)^2+(y_1+...+y_n)^2=x_1^2+x_1x_2+...+y^2$.
> Notiamo che se i movimenti sono davvero casuali, per ogni $x_ix_j$ c'è una probabilita di trovare un valore uguale di segno opposto, gli unici componenti che non possono scomparire sono quelli del tipo $x_i^2$ ed $y_i^2$. Quindi per $n$ abbastanza grandi $R^2\approx n(x^2+y^2)\approx n$ quindi segue che $R\approx\sqrt n$