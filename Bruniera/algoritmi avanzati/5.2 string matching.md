# String matching

Noi studieremo partendo da algoritmi per la biologia.

Knuth: "My favourite way do define CS is to say that is the theory of algorithms".

Il problema è che non c'è una buona definizione di algoritmo, tutte quelle che abbiamo visto non sono ben precise su alcuni aspetti, un po' vaghe.
Come si fa quando non si sà qualcosa

Spesso si dice che un algoritmo deve terminare, però questo ad esempio esclude i sistemi operativi, che sono di per se dei grandi algoritmi.

## Overview

> ***Problema***: String matching *esatto*
>
> Definizioni:
> * $\Sigma$ è un alfabeto (per noi sarà $\{a,c,g,t\}$)
> * $\Sigma^*$ è la collezione delle stringhe finite su $\Sigma$
> * $P,T\in\Sigma^*$ sono due stringhe che chiameremo "pattern" e "testo"
> * $|\Sigma|=\sigma$ è una costante fissata a priori, la consideriamo $\sigma\in O(1)$
> * $|T|=m$
> * $|P|=n$
> * $S[i]$ indicizza una stringa (da 1)
> * $S[i,j]$ è una slice
>
> Produci tutte le occorrenze esatte di $P$ in $T$. (Esistono anche le occorrenze "approssimate")
>
> ***Definizione formale***:
> Input $P,T\in\Sigma^*$\
> Output $\{i\in[1,n]:T[i,i+n-1]=P\}$

Nota: vediamo soluzione ottimizzate a cercare lo stesso pattern su tanti testi.

* Ha una soluzione?
* È un lower bound?
* Esiste un lower bound?

Algoritmo naive $O(nm)$ (quadratico):
```
for i in 1..m-n {
    for j in 1..n {
        if T[i+j-q] != P[j]{
            break
        }
    }
    if j == n{
        match
    }
}
```

Questo è un upper bound.\
$O(m+n)=O(m)$ è il lower bound (lineare), perché bisogna almeno leggere tutto l'input.

Esistono algoritmi sub quadratici, e lineari? Sì.

L'idea è che dopo che ho fallito il match non riparto da zero, ma riposiziono il pattern e riprendo da "metà match".

Diciamo che ho già tentato di fare il match di $P[1,j]$ e al passo dopo devo ricominciare. Se avessi preprocessato il massimo $a$ per cui $P[j-a-1,j]=P[1,a]$ (prefisso del pattern e suffisso della parte processata). Allora potrei continuare il match dalla stessa posizione di $T$, e dalla posizione $a$ di $P$.
Questa scansione è lineare, ma qual è la complessità preprocessing di $P$?

Il primo metodo per il preprocessing non era lineare, ma lo era nella dimensione del pattern che è molto più piccolo.

## Soluzione di Knuth Morris e Pratt KMP

È la prima soluzione lineare del problema.

Notiamo che per dare la complessità dell'algoritmo è sufficiente contare i confronti.

La posizione da cui vogliamo ripartire con i confronti è quella dove la stringa finora, è uguale al suffisso della stringa fallita. quindi è un prefisso-suffisso proprio (diverso dalla stringa completa) della stringa $P[1,j]$.
Se il prefisso-suffisso non fosse proprio avremmo un loop al primo shift.

Vogliamo determinare il massimo prefisso-suffisso di $P[1,j]$. Il fatto che sia massimo mi permette di non perdere occorrenze dopo lo shift.
Perché se ci fosse un occorrenza al punto $T[i-j]$ e lo shift ci avesse fatto partire da $T[j-k]$ facendoci perdere l'occorrenza, allora $j>k$ quindi $k$ *non* sarebbe il massimo, che è assurdo.

Chiamiamo $sp_i[P]=k$ la lunghezza del massimo suffisso-prefisso in posizione $i$ del pattern $P$.

$$
sp_{i+1}[P]=\begin{cases}
sp_i[P]+1&P[sp_i[P]+1]=P[i+1]\\
sp_{sp_{i-1}[P]}[P]&\text{altrimenti}
\end{cases}
$$

Dobbiamo utilizzare la programmazione dinamica per implementarlo in modo efficiente.

Ci sono dei casi in cui calcolare $sp_{sp_{i-1}[P]}$ può avere costo lineare, quindi potrebbe sembrare che eseguire $n$ volte un'operazione $O(n)$ abbia costo $O(n^2)$.
Però se valutiamo il costo di fare tutte quelle operazioni e non il caso peggiore di una operazione (complessità ammortizzata), risulta che il costo delle operazioni pesanti viene "distribuito" su quelle leggere, quindi il totale resta lineare.

## Algoritmo Z

Prendiamo un simbolo fuori dall'alfabeto: $\$\notin\Sigma$. E costruiamo la stringa $P\$T=A$.

Se avessimo la funzione:

$$
Z_i[A]=\begin{cases}
0&i=1\lor A[i]\neq A[1]\\
\max\{j|A[1,j]=A[i,i+j-1]\}&\text{altrimenti}
\end{cases}
$$

Allora, se uso $P\$T=A$ e scorro $Z$ cercando i punti $i$ in cui $Z_i[P\$T]=n$, allora ho trovato le occorrenze di $P$ in $T$.

Oltre ad ottenere le occorrenze, abbiamo anche trovato un modo per comprimere la stringa $P\$T$ memorizzando i valori di $Z$ in modo da poter ricostruire parti della stringa usando il pattern.\
Questo metodo è alla base di LZ77.

> Il libro dice che questo algoritmo è *la* soluzione per questa versione del problema (con il preprocessing del pattern). Perché è molto elegante e posso ricondurre gli altri algoritmi (anche KMP) a questo. Si dice che è *generale*
> 
> Inoltre, funziona anche su un input fornito in modo incrementale (ad esempio uno stream). Si dice che è *online*. 
> Se un input è troppo grande perché sia disponibile tutto contemporaneamente in memoria, è importante che un algoritmo sia online.

Posso farlo in modo lineare?

Per determinare ricorsivamente $Z_i[A]$, manterrò la coppia $l,r$ che costituisce l'intervallo $A[l,l+Z_l[A]-1]=A[l,r]$ con $r$ *massimo*.
Per calcolare $Z_i[A]$ confronto $i$ con $r$:
1. Se $i>r$ sono in una zona ancora da esplorare
   * Può avere costo lineare, ma è ammortizzata
2. Se $Z_{i-l+i}[A]\geq r+i$...

## Strutture dati succinte

Quanto lungo è il testo in memoria? Quanto ci mettiamo ad accedere? Con $|\Sigma|=4$ sembra ovvio dire $2m$. E ci sembra di avere accesso in tempo $O(1)$

In realtà una macchina che accede in tempo costante è impossibile, a noi sembra di si perché lo facciamo con gli array, ma quando abbiamo input più grandi di quello che può entrare nella ram non è più così.

Se vogliamo avere la complessità "vera" dobbiamo considerare che la complessità non è uniforme ma logaritmica (con le macchine RAM nel corso di complessità), e l'accesso costa $O(\log i)$.

Su una MdT, è necessario spazio $O(m\log m)$, perché devo memorizzare anche gli indici per ottenere l'accesso "costante".

> ***Domanda***: Esistono strutture dati *succinte*? Nel senso che non hanno bisogno di questi dati aggiuntivi?
>
> Sì, ma sarebbero un corso a parte. Anche perché potremmo non fermarci la e comprimerle ancora.

## KMP online

Definizione:

$$
sp_{i+1}[P]=\begin{cases}
sp_i[P]+1&P[sp_i[P]+1]=P[i+1]\\
sp_{sp_{i-1}[P]}[P]&\text{altrimenti}
\end{cases}
$$

Sono in grado di calcolarla *efficientemente* online? Se riesco a calcolarla in tempo lineare posso implementare anche KMP in tempo lineare.

Diciamo che $j$ mappa su $i$ (e scrivo $i\sim j$) sse $i=j+Z_j[P]-1$.

$$
sp_i(P)=Z_{\min\{j:i\sim j\}}
$$

### Euristica

Possiamo fare di "meglio", usiamo una nuova funzione $sp_i'(P)$ che è uguale, ma con la richiesta aggiuntiva che $P[i+1]\neq P[sp_i'(P)+1]$.
In questo modo quando c'è un mismatch non dobbiamo più continuare a saltare finché non troviamo un match, ma il primo salto ci porta direttamente alla parte giusta del pattern, che però sarà più corta.\
Non migliora il caso medio, ma è un'euristica.

Ci sono comunque casi in cui provoca altri salti, ma dovrebbe essere meno frequente.

## Boyer-Moore

Usando KMP online con l'euristica non abbiamo ancora la soluzione che volevamo.
Vogliamo che aggiungere un carattere sia un'operazione $O(1)$ (tempo prevedibile). Si dice che è *real-time*. È importante per sistemi che devono essere affidabili.

Usiamo un nuovo $sp_{i,x}(P)$.

Supponiamo di controllare il pattern partendo dalla fine, incontriamo un mismatch, ed il mismatch è col carattere $x$, allora so che posso far scorrere il pattern almeno fino al carattere $x$. Questa regola di shift la chiamiamo BCR (bad character rule).

Supponiamo di aver trovato un mismatch dopo aver già controllato la sottostringa $\alpha$, allora possiamo shiftare il pattern per far combaciare la sottostringa $\alpha$ col prefisso del pattern. Chiamiamo questa regola di shifting GSR (good suffix rule).

Ad ogni passaggio applichiamo la più conveniente (quella che fa shiftare di *meno*, per non perdere match) tra le due.

Quando ci sono nessuna o poche occorrenze di $P$ in $T$, l'algoritmo è sublineare. Sfortunatamente quando ce ne sono tante diventa l'algoritmo naive.

I casi limite sono con $T=a^m$ e $P=a^n$ oppure $P=ca^{n-1}$.

### Implementazione BCR

Per ogni carattere mantengo una lista delle posizioni del pattern in cui occorre. Lo spazio occupato è $O(|P|)$ ($\sigma$ è costante). Accedere è più difficile, devo scorrere la lista per trovare la posizione che mi interessa.

Se scorro la lista man mano che controllo il pattern il costo è lineare. Non ci vogliono meno operazioni rispetto a farlo tutto in blocco dopo, ma così si nota che è lineare.

### Implementazione GSR

Si implementa usando Z.

Invece che usare $Z_j[P]$ usiamo $Z_j[P^r]$ più alcuni dettagli.