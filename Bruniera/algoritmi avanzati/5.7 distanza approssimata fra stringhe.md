# Distanza approssimata tra stringhe

La ricerca esatta è inutile nel mondo reale, ci serve la ricerca approssimata. Possiamo portare sfruttare il lavoro fatto per la ricerca esatta, per quella approssimata?

La prima intuizione è che se abbiamo una stima di quanti errori occorrono, significa che abbiamo anche delle porzioni esatte. Gli algoritmi più veloci si basano sulla ricerca di queste porzioni esatte.

## Distanza tra stringhe

Per parlare di matching inesatto bisogna innanzitutto definire "distanza" tra stringhe

La definiamo quindi come il numero di operazioni necessarie per convertire una sequenza nell'altra. Le operazioni consentite sono: Match $m$, Insert $i$, Delete $d$, Substitute $s$. Ciascuna operazione ha un suo costo o score

Vediamo un esempio

$$ \begin{align}

 && a && c && g && t && c && a && t && c && a \\
i && m && s && m && m && s && d && m && m && m \\
t && a && a && g && t && g && \  && t && c && a

\end{align} $$

Qua la linea di mezzo è la stringa di operazioni ed è chiamata ==edit-string==, mentre quella sopra e quella sotto sono le due stringhe per le quali calcoliamo la distanza

Il miglior algoritmo noto per calcolare la distanza ha complessità $O(m\times n)$ dove $m$ ed $n$ sono le lunghezze delle due stringhe

Il costo delle varie operazioni va scelto e ci sono diverse maniere per farlo, e non tutte danno un costo costante

Lavoreremo con l'assegnamento di costi più banale, ossia quello che da costo $0$ al Match e costo $1$ a tutte le altre operazioni

> [!abstract] Distanza di Levensthein
> 
> Minimo numero di $i,d,s$ per convertire una stringa in un'altra
> 
> Tipicamente indicata con $d_L(\sigma_1,\sigma_2)$

> [!abstract] Distanza di Hamming
> 
> Minimo numero di sostituzioni per convertire una stringa in un'altra
> 
> Tipicamente indicata con $d_H(\sigma_1,\sigma_2)$
> 


## Allineamento di sequenze

Parleremo di ==allineamento globale==, ovvero il matching di due stringhe nella loro interezza, e di ==allineamento locale==, che matcha tutta la prima stringa ad una porzione della seconda

Più intuitivamente l'allineamento globale è il calcolo della distanza (ovvero della [[String Matching#Distanza approssimata tra stringhe|edit-string]] ottimale). Invece l'allineamento locale chiede di trovare la sottostringa di $T$ la cui distanza è minima da $P$

In poche parole l'allineamento locale utilizza quello globale come subrutine

### Allineamento globale

Chiamiamo $\eta$ la nostra edit-string. Partiamo da un algoritmo esponenziale che verrà migliorato poi dalla [[Programmazione Dinamica|programmazione dinamica]]

```
allineamento_exp(s1,s2,i,j) {
	if i==0 or j==0 return i+j
	if s1[i] == s2[j] then
		cms <- allineamento_exp(s1,s2,i-1,j-1)
	else
		cms <- allineamento_exp(s1,s2,i-1,j-1) +1
	ci <- allineamento_exp(s1, s2, i-1, j) +1
	cd <- allineamento_exp(s1, s2, i, j-1) +1
	return min(cms,ci,cd)
}
```


Qua $cm, cs, ci, cd$ rappresentano la scelta di operazione per il prossimo step

Vediamo che questo algoritmo fa una quantità esponenziale di chiamate ricorsive alterando $i,j$ nell'algoritmo. Però esistono solo una quantità quadratica di coppie $i,j$ diverse, pertanto memorizziamo chiamate già fatte e poi le ritorniamo direttamente quando vengono chieste nuovamente

La programmazione dinamica è un tradeoff di tempo e spazio. Scegliamo i sprecare spazio per risparmiare tempo.

Tabuliamo i risultati in una tabella $A$ dove $A[i,j]=allineamento\underline{~}exp(\sigma_i,\sigma_j,i,j)$.

La tabella è grande $i\times j$ ed ogni cella va scritta una volta sola, quindi il costo di scriverla tutta sarà $O(ij)$ (quadratico).

```
allineamento_prog_din(s1,s2) {
	for i in 0..|s1| {
		for j in 0..|s2| {
			if s1[i] == s2[j] {
				A[i,j] = min(
					A[i-1,j-1],
					A[i,j-1] + 1,
					A[i-1,j] + 1
				)
			} else {
				A[i,j] = min(
					A[i-1,j-1] + 1,
					A[i,j-1] + 1,
					A[i-1,j] + 1
				)
			}
		}
	}
}
```

Se vogliamo risparmiare spazio ci basta tenere due righe della matrice alla volta.
Se vogliamo parallelizzare il processo possiamo far partire i thread sfasati.

### Allineamento locale

A distanza $\leq d$ di $\sigma_1$ e $\sigma_2$ è l'insieme di tutte le coppie $\langle i,\eta_i\rangle$ tali che $\eta_i$ è l'allineamento di $\sigma_1$ con un prefisso $\sigma_2[1..i]$ a distanza di $\leq d$

## Algoritmo di Landau Vishkin

> ***Problema***: Dato $T$ testo e $P$ pattern, trovare tutte le occorrenze di P in T a distanza di Levensthein $\leq d$

L'implementazione naive ha costo quadratico: Usiamo la soluzione  con programmazione dinamica dell'allineamento globale, che abbiamo già visto essere quadratica.

Però in realtà, questo algoritmo risolvere un problema più complesso, perche trova tutte le occorrenze con distanza minima. Vogliamo sfruttare questa differenza per ridurre il costo. Otterremo costo lineare $O((|P|+|T|)d)$ ($d$ è costante).

Usiamo la tecnica di hybrid dynamic programming.

Intuizione: per calcolare una cella della tabella di dynamic programming non mi servono solo informazioni da una porzione della tabella, non da tutta la parte precedente come nel caso della programmazione dinamica.


Osservazione: I valori delle celle delle tabelle relative ad un allineamento a distanza di $L\leq d$ *non* possono dipendere da celle a distanza $>d$.

Individuiamo le delle diagonali della tabella e le numeriamo a partire da quella che parte dalla cella $M[0,0]$, questa è la main diagonal ($0$). Quelle piu a sinistra sono $-1,-2,...,-|P|$, quelle a destra sono $1,2,...,|T|$.

> ***Definizione***: $d$-path
>
> Cammino che parte dalla *riga* 0 ed allinea $P$ ad una sottostringa di $T$ con al più $d$ errori.

Osservazione: guardiamo il punto più distante sulla diagonale $i$ che posso raggiungere con al più $d$ errori. Gli allineamenti che ci interessano sono quelli dove questo punto raggiunge la base della matrice. 
Dobbiamo individuare questi punti.

### Intermezzo: Longest Common Extension (LCE)

Dati $S_1$ $S_2$ e molte coppie $i,j$ indici dell stringhe. Vogliamo la stringa $\alpha$ di lunghezza massima tale che $S_1[i,i|\alpha|]=S_2[j,j+|\alpha|]=\alpha$.

Costruiamo il suffix tree di $S_1\$S_2€$. Cerchiamo il lower common ancestor dei nodi di profondità $i+2+S_2$ ed $j+1$.

Fine intermezzo

---

Dovrò eseguire $d$ iterazioni in cui per $k\leq d$ cercherò di estendere un farthest reaching $k$-path in un f.r. $(k+1)$-path. Nel farlo sceglierò tra 3 possibili estensioni: orizzontale, verticale, diagonale.

```
d = 0

# Inizializzazione
for i in 0..m {
	# Colonna finale del f.r. 0-path sulla diagonale i
	lce(P[1,m], T[i,m])
}

for d in 0..k {
	for i in -n..m {
		# Determino il f.r. d-path
		# usando il f.r. (d-1)-path sulle diagonali i, i-1, i+1
		
		a = lce(P[1,m], T[i-1,n])
		b = lce(P[1,m], T[i,n])
		c = lce(P[1,m], T[i+1,n])

	}
}
```