# Organizzazione fisica

Non parleremo più di righe o tabelle, parlemeremo di file di record e campi.

## File

Esistono due tipi di file:
* File non ordinati (heap files)
* File ordinati (sorted files)
* Hashed files (file che utilizzano tecniche di hashing)

L'insieme di dati di un database è costituito da record di alcuni file. Per semplicità immaginiamo unacorrispondenza diretta tra dile e tabelle, anche se in realtà non esiste. Più file possono coporre una tabella e più tabella possono essere memorizzate nello stesso file.

Fisicamente (ne abbiamo già parlato) i dati sono mantenuti in due tipi di dispositivi: primari (come la ram) e secondari (come il disco). Per essere manipolati i dati devono essere portati in memoria primaria, ma stabilmente risiedono in memori asecondaria.

### Operazioni sui file

Per accedere ai da ti sono necessarie tre operazioni principali:
* Localizzazione
  * Dei dati all'interno del file
* Copia
  * In memoria principale
* Riscrittura
  * In memoria secondaria, se sono stati fatti aggiornamenti

I record possono essere di lunghezza fissa (se tutti i record del file occupano lo stesso spazio) o di lunghezza variabile (ad esempio quando si utilizzano campi varcharo quando si hanno campi multivalore od opzionali). Se i campi sono variabili per utilizzao di varchar o campi opzionali in genere si tratta di un tradeoff tra spazio occupato e tempi di esecuzione. Se si tratta di campi multivalroe si tratta di una violazione della prima forma normale.
Possiamo avere record di lunghezza variabile anche in un altro caso: quando abbiamo diverse tabelle nello stesso file, in questo caso si parla di file mixed.

I record di lunghezza fissa sono pià facili da gestire, basta fare una moltiplicazione di un indice numerico per la dimensione per trovare qualsiasi record.

Nei file con record di lunghezza variabile bisogna utilizzare delle tecniche diverse, ad esempio aggiungere unterminartore dopo ogni campo variabile ed un altro separatore per inidicare la terminazione dei del record.

Per i campi opzionali si puo scegliere se mantenere la dimensione fissa e utilizzare un valore null, oppure se utilizzare lo stesso approccio dei record a lunghezza variabile. Serve inserire anche il nome del campo per indicare quali sono presenti. Diventa sensato quando i campi opzionali sono molti e spesso solo pochi sono presenti, ad esempio rimuovendo delle generalizzazioni disgiunte.

### Organizzazione dei record in blocchi

I record di un file devono essere allocati in blocchi (che sono l'unità di trasferimento tra memoria secondaria o primaria. In generale i record sono molto più piccoli di un singolo blocco, ma non sempre, se è così il blocco conterrà più record.
Supponiamo che la dimensione sia di $B$ byte e la dimensione del record sia $R$ con $R\leq B$. In ogni blocco possono essere presenti $bfr=\lfloor\frac BR\rfloor$ (blocking factor). Chiaramente se non sono sempre divisibili ci sarà uno spazio inutilizzabile (più piccolo di $R$).
Se i record sono più grandi dei blocchi, invece, serviranno più blocchi per record ed il blocking factor è $0$.

Le situazioni peggori sono quelle in cui il record è lungo poco più della metà di un blocco od ancora peggio quando il record è lungo poco più di un blocco. In questi casi il 50% dello spazio totale è sprecato.
In realtà la seconda situazione non è possibile, vediamo dopo.

#### Spanned record

I record spanned sono quelli che hanno la possibilià di uscire dal blocco ed essere contenuti in due blocchi diversi. Questo non solo permette di avere record più lunghi del blocco (vedi situazione sopra), ma risolve il problema dello spazio inutilizzato, posizionando i recordf a cavallo tra blocchi differenti.

Per semplicità assumeremo sempre di avere record di lunghezza fissa ed unspanned molto più piccoli del blocco.

#### Record di lunghezza variabile

Nel caso dei recordf di lunghezza variabile, il blocking factor è calcolato in base alla lunghezzza media dei record.

#### Dimensionamento del file

Quando so quanti record per blocco sono presenti e quanto grandi record abbiamo possiamo ricarare la dimensione del file necessaria a contenere tutti i blocchi.

Anche le prestazioni dei trasferimenti si misurano in base al numero di blocchi trasferiti.

#### Allocazionie dei file

Esistono diverse tecniche di allocazione:
* Contigua
  * I blocchi sono consecutivi sul disco
  * La lettura sequenziale è molto veloce
  * È difficile gestire l'espansione dei file
* Con collegamenti
  * Ogni blocco viene collegato arbitrariamente e sono linkati come una lista
  * L'accesso è lento
* Combinazione delle due precedenti
  * I blocchi sono raggruppati in cluster concatenati
  * mette insieme la flessibilità dei link con le prestazioni dell'allocazione contigua
* Indicizzata
  * Alcuni blocchi contengono i collegamenti ai blocchi di dati
  * Questi blocchi contengono anche delle informazioni per capire in quale blocco si trovano i dati
  * Finita la ricerca nella struttura di suppporto ci troviamo l'indirizzo del blocco che contiene il dato che cerchiamo
  * Per trarre vantaggio da questa serche che il file dei dati sia molto più piccolo della struttura di indicizzazione
  * (Saranno dei B-Tree nelle soluzioni più sofisticate)
  * Si chiamano indici perché ricordano gli indici dei libri

#### File header (descrittore del file)

Contiene le informazioni necessarie per determinare gli indirizzi sul disco dei blocchi del file.

#### Ricerca di record su disco

Come facciamo a trovare il record che ci interessa in un file?

Se il file è ordinato è facile, possiamo capire quale blocco contiene il record eseguendo una ricerca binaria.

Se il file è non ordinato servirebbe una ricerca lineare. È troppo lenta, ci serve una soluzione che sia logaritmica, almeno nel numero di blocchi trasferiti.

Le operazioni di ricerca devono permetterci di selezionare diversi record sulla base di una condizione di selezione. Può succedere che la condizione sia soddisfatta da più di un record ed anche su più blocchi o file diversi. Dipende dalla condizione e da come sono organizzati i blocchi ed i file.

Quando più record devono essere selezionati viene individuato solo il primo (record corrente) e le ricerche successive partiranno da quello per individuare gli altri

#### Operazioni per la ricerca

* Operazioni record-at-a-time (applicate ad un singolo record)
  * Find (o locate): ricerca il primo record che soddisfa la condizione e trasferisce il blocco che lo contiene in un buffer
  * Read (o get): copia il record corrente dal buffer in una fariabile del programma
  * FindNext indivudua il record successivo e copia in un buffer il blocco che lo contiene
  * Delete: cancella il record corrente ed eventualmente aggiorna il file
  * Modify: aggiorna il record corrente ed eventualmente aggiorna il file
  * Insert: inserisce un nuovo record nel file individuando il blocco in cui deve essere inserito e caricando tale blocco sul buffer, eventialmente copiando il contenuto del buffer sul disco
* Operazioni set-at-a-time (applicate ad un intero file)
  * FindAll: individua tutti i record che soddisfano la condizione di ricerca
  * FindOrdered: recupera tutti i record presenti nel file in un preciso ordine
  * Reorganize: riorganizza i record nel file (ad esempio in un particolare ordine)

#### Organizzazione dei file e metodi di accesso

Per organizzazione del file si intende l'organizzazione dei record e dei blocchi, e le strutture di accesso ai blocchi (indici ed altro).

Per metodi di accesso si intende l'insieme di programmi per eseguire le operazioni sui file, Ovviamente dipendono strettamente dall'organizzazione del file

 ### File non ordinati

I record sono inseriti nel file nello stesso ordine in cui sono inseriti.

L'inserimento è *estremamente* efficiente, se ho molti più inserimenti che altro, questa è la soluzione migliore. Ad esempio è perfetta per un archivio di log di sistema.

Cancellazione e ricerca sono lente, invece, hanno costo lineare. La modifica richiede prima una ricerca, quindi anche quella è lineare.
Per la cancellazione si può usare un deletion marker (come un null) per non dover riorganizzare il file dopo ogni cancellazione. In questo caso bisogna eseguire un ricompattamento periodico.

Per le letture ordinate è necessario eseguire un riordinamento in fase di lettura, si utilizza una sorta di MergeSort, in cui prima ordiniamo il blocco con un qualsiasi algoritmo (RadixSort o CountingSort se possibile, altrimenti MergeSort), e poi fondo tra loro i blocchi.

### File ordinati

In realtà non sono tanto migliori di quelli non ordinati. Hanno dei vantaggi per alcuni dei campi (quelli per cui sono ordinati) ma dei costi per mantenere l'ordinamento. Inoltre, per tutti gli altri campi non è ordinato, quindi restano solo i costi aggiuntivi per il mantenimento dell'ordinamento.

Ovvi vantaggi (per uno dei campi):
* La lettura ordinata è banale 
* La ricerca del successivo è efficiente
* Se la chiave è l'ordinante si può usare ricerca binaria
  * Quindi confrontanto le letture medie abbiamo $\log_2n_b$ contro $\frac {n_b}2$ che è un bel vantaggio
  * Ricordiamo che se facciamo una ricerca negli altri campi perdiamo il vantaggio

Notiamo che eseguire la ricerca binaria anche all'interno del blocco (dopo aver individuato il blocco) è poco rilevante perché una ricerca lineare su un blocco in memoria principale non sarà *mai* più lenta di *qualsiasi* operazione sul disco.

#### Inserimento

L'inserimento nel file  richiede costi aggiuntivi perché bisogna mantenere l'ordinamento del file.
L'inserimento richiede di: cercare la posizione in cui inserire il record, creare lo spazio per inserire il record (mediamente serve spostare metà dei blocchi del file).

Possibili soluzioni:
* Mantenere dello spazio libero in ogni blocco
  * Evitare di riempire del tutto il blocco prima di crearne uno nuovo, così si accumulano alcuni inserimenti gratuiti.
* Creare un file temporaneo non ordinato (file di overflow)
  * Inserisco in modo efficiente come nel caso del file non ordinato
  * Se è troppo grande diventa problematico, perché ogni volta che una ricerca nel file ordinato fallisce devo controllare linearmente quello di overflow
  * Periodicamente bisogna fondere i due file ricreando l'ordinamento
  * È una buona soluzione se gli inserimenti sono limitati

#### Modifica

La modifica richiede prima una ricerca e poi una riscrittura.

La ricerca può essere veloce se è coinvolto l'elemento di ordinamento, altrimenti diventa lineare.

Se il campo modificato è quello su cui si esegue la ricerca non è un problema, altrimenti bisogna ripristinare l'ordinamento successivamente alla modifica.

Dato che di solito la ricerca avviene sullo stesso campo che viene modificato, spesso almeno una delle due fasi (ricerca o riscrittura) ha costo lineare.

#### Conclusione

Sono raramente utilizzati perché hanno pochi vantaggi rispetto ai file non ordinati. Entrambi i tipi di file possono essere sensati in certi casi specifici, ma dipende da cosa serve a noi.

### Hashed files

Sono quelli più interessanti ed a volte sono anche utilizzati. Fornisce ricerca efficiente rispetto a determinate condizioni di ricerca.

La condizione di ricerca è tipicamente una uguaglianza su un singolo campo (spesso la chiave).
Una funzione di hash (hashing esterno) applicata al valore che stiamo cercanto restituisce l'indirizzo del blocco che contiene il record. Dopo di che si può procedere con una ricerca lineare sul blocco.

Per i file interni la tecnica è implementata su array di record. La funzione di hash (hashing interno) restituisce la posizione del record all'interno del vettore.

#### "Problema"

Analiziamo alcuni aspetti degli hash interni per poi trasportare i concetti agli hash esterni.

Alcuni valori hanno lo stesso hash, abbiamo una collisione. Come inseriamo un record se il suo posto è già occupato?
* Open addressing
  * Se la posizione è occupata inserisco sulla prima posizione libera successiva
  * Nel caso peggiore devo eseguire il giro compoleto fino alla fine della tabella
  * Se le collisioni sono tante, ogni ricerca diventa lenta perché mi trovo a dover scorrere tante volte
* Multiple hashing
  * Esattamente come l'open addressing, ma invece che guardarela posizione direttamente successiva si fa un secondo hash dell'hash
  * È più difficile che si formi un cluster di collisioni
  * Se si verificano comunque tante collisioni si cade nello stesso problema
* Hash chaining
  * L'hash non da la posizione dell'elemento, ma l'indirizzo di una lista di elementi
  * Quando inserisco un elemento lo metto in coda alla lista corrispondente
  * Quando cerco un elemento identifico la coda e cerco all'interno di questa
  * Possiamo vedere queste liste come dei file di overflow dei record con lo stesso hash

##### Soluzione ottimale

Una funzione di hashing si comporta bene quando distribuisce uniformemente i record nello spazio, in modo di minimizzare le collisioni senza lasciare troppe locazioni inutilizzate.

In simulazioni si è visto che la soluzione ottimale è quella di avere una tabella di hash piena tra il 70% ed il 90%.

Ad esempio, se vogliamo memorizzare $r$ record nella tabella, dobbiamo utilizzare uno spazio di $m$ locazioni in modo che $0.7\leq\frac rm\leq0.9$

#### Hashing esterno

Il problema delle collisioni è meno grave perché nello stesso blocco posso inserire più record. Il problema è quando finiscono le posizioni nel blocco.

Soluzione? Chaining:
* Uno potrebbe pensare di riservare l'ultima parte del file per contenere un puntatore ad un bucket di overflow che contiene altri record ed eventualmente un altro puntatore
* Il problema principale è il dimensionamento del file per riservare i bucket
* Vengono utilizzate delle tecniche di espansione dinamica del file
  * Servono delle tecniche di hashing che si adattino all'espensione dinamica
* Esistono almeno tre tecniche di hashing esterno con espansione dinamica

Tecniche con espansione dinamica:
* Dynamic hashing
  * Alloco al file una sola cella
  * Quando si riempie la splitto e distribuisco i record nelle altre due in base al MSB degli hash
  * Quando una delle due celle si riempie di nuovo, splitto di nuovo in base al secondo MSB
  * Viene mantenuta una struttura ad albero ausiliaria (molto efficiente, due bit più due indirizzi per nodo) per capire quale blocco contiene il record
  * La lunghezza dell'albero è limitata dalla l'unghezza dell'hash, limita anche il numero di elementi, ma sono molto grandi
  * Solo un accesso è fuori dalla struttura ausiliaria (molto veloce)
  * Generalmente l'albero (directory tree) non è bilanciato, ma le operazioni in memoria secondaria sono così lente a confronto che non fa veramente differenza
  * Ricompattamento dinamico
    * Quando alcuni blocchi adiacent sono rimasti con pochi elementi si possono ricompattare in un singolo blocco
* Extendible hashing
  * Simile ma basato su una tipo di directory differente, basato su un array di $2^d$ celle
  * $d$ è detto global depth della directory
  * Per accedere alla posizioneconsideriamo i primi $d$ bit dell'hash
  * In quella posizione dell'array troviamo l'indirizzo del blocco che contiene l'elemento
  * Non è detto che a tutti gli elementi corrisponda un blocco diverso, se i primi $d'$ (con $d'\leq d$) sono uguali
  * $d'$ è detta local depth della locazione
  * In almeno una locazione $d'=d$
  * Quando in una locazione finisce lo spazio si splitta in base al $d'+1$-esimo bit e si aggiorna la directory
  * Quando possibile si fondono le locazioni e si riaggiorna la directory
  * Quando si splitta una locazione con $d'=d$ di incrementa $d$ (raddoppio)
  * Quando nessuna locazione ha $d'=d$ si decrementa $d$ (dimezzamento)
  * Quando non devo fare ridimensionamenti è molto veloce, quando devo farli, vado ad agire solo su pochi blocchi
* Linear hashing
  * Permette ad un file hash di aumentare o diminuire dinamicamente senza l'ausilio di una directory
  * Si parte con $m$ blocchi iniziali. La funzione di hash iniziale è $h_0(k)=k\ \mathrm{mod}\ m$
  * Ogni blocco viene gestito temporaneamente con una catena di overflow
  * Quando ogni cella ha almeno un record di overflow (ipotizziamo che siano abbastanza uniformi), creo una nuova cella $m$ e splitto
  * Splitto partendo dalla prima cella in base alla nuova funzione di hashing $h_1(k)=k\ \mathrm{mod}\ 2m$. Notiamo che dato che in modulo $m$ valeva $0$ ora può valere o $0$ o $m$, per la seconda cella invece o $1$ o $m+1$
  * Lo split avviene man mano che saturo le celle. Ogni volta che inserisco in una cella che è ancora gestita con overflow, splitto una nuova cella (non quella richiesta, vado in ordine). In breve tempo tutte le celle saranno splittate
  * Quando tutte le nuove celle saranno piene ripeto con la funzione $h_2$
  * La funzione generica è $h_i(k)=k\ \mathrm{mod}\ 2^im$
  * Nella fase transitoria devo utilizzare entrambe le funzioni di hashing: calcolo con quella vecchia, se viene un valore minore di $s$ (celle già splittate) applico quella nuova
  * Se gli hash non sono ben distribuiti non funziona
  * Mi bastano poche informazioni ausiliarie, solo il conteggio di quali blocchi ho già splittato $s$ e la funzione $h_1$, quindi nientre strutture ausiliarie
  * A contrario delle altre due questa non ha un limite superiore
  * Quando le celle si svuotano posso ricompattarle annullando l'ultimo split man mano

## Indici

Gli indici sono strutture ausiliarie di accesso ai dati per rendere più veloce il recupero di record in determinate condizioni.
Alcuni (gli indici secondari) non influenzano la disposizione fisica dei dati, sono dei percorsi dei cammini di ricerca alternativi per la localizzazione di alcuni campi.
Altri (i più importanti) sono basati su file ordinati e strutture ad albero

Ricordano la tecnica del Dynamic Hashing, ma non sono la stessa cosa e non funzionano allo stesso modo (anzitutto non si applica nessuna funzione di hashing).

Gli indici possono essere costruiti su qualsiasi campo, non solo sulla chiave primaria, anche se risulta naturale costruirli almeno sulla chiave primaria.

### Indici ordinati di singolo livello

L'idea è simile a quella degli indici dei libri.

Solitamente la struttura è definita su un singolo campo del file e contiene, per ogni valore di tale campo presente nel file, contiene una lista di puntatori a blocchi che contengono record con quel valore per quel campo.
Se il campo è una chiave primaria, ovviamente, abbiamo un solo blocco per ogni valore.

L'indice è ordinato, quindi possiamo fare ricerca binaria anche se i file non è ordinato. Anche se il file è già ordinato, la ricerca sull'indice è molto più veloce quindi conviene utilizzarla comunque.

Esistono diversi tipi di indici di singolo livello:
* Primari
  * Specificato rispetto al campo chiave di un file che è già ordinato rispetto a quel campo
  * Ogni record è identificato univocamente dal valore che assume su tale campo.
* Indice di clustering
  * Si usa quando il file è ordinato, ma non rispetto alla chiave
  * Il valore non identifica il singolo record
  * (non posso avere indici primari e di clustering sullo stesso file)
* Indice secondario
  * Può essere definito su qualunque campo anche senza ordinamento
  * Se ne possono avere più di uno
  * È il più interessante perché è più libero anche se è un po' più lento

#### Indice primario

È un file ordinato i dui record di lunghezza prefissata possiedono due campi: $(\langle chiave\ primaria\rangle,\langle blocco\rangle)$
Nel file indice esiste una entry per ogni blocco del file di dati: $\langle k(i),p(i)\rangle$. Dove $k(i)$ è minore od uguale a tutte le chiavi del blocco (in genere è uguale alla chiave del primo record), e $p(i)$ è l'indirozzo del blocco.
Il campo $k(i)$ è chiave per ogni entry del file di indice, ed il file di indice è ordinato sia rispetto a $k(i)$ che $i$ (quindi il campo chiave deve essere opportuno per l'ordinamento, non basta che sia minore delle entry del blocco)

Si dice che l'indice primario è *non* denso perché ha una entry per ogni blocco e non per ogni record.

Un valore di chiave primaria $k$ si trova nel blocco di indirizzo $p(i)$ con $i$ tale che $k(i)\leq k<k(i+1)$

Per recuperate il record, noto il valore di $k$ possiamo effettuare una ricerca binaria sul file indice per trovare $i$ e poi andiamo a cercare nel blocco di indirizzo $p(i)$.

Il problema principale di questi indici è la necessità di mantenere non uno ma due file ordinati ad ogni inserimento o rimozione.

##### Esempio

Supponiamo di avere $r=30000$ record con dimensioni del blocco di $B=1024$ byte. I record sono di lunghezza fissa $R=100$ e unspanned. Ho un $bfr$ di 10 record per blocco con 24 byte sprecati. Quindi ho $n_b=3000$ blocchi.

Una ricerca binaria sul file di dati richiederebbe $\log_2n_b=12$ accessi al blocco.

Utilizzando un indice primario con lunghezza della chiave $V=9$ byte e puntatore a blocco di $P=6$ byte. la dimensione di ogni entry è $R_i=15$ byte e $bfr=68$ entry per blocco.
Il numero totale di entry è 3000 come il numero di blocchi. Il numero totale di blocchi per l'indice è $n_{bi}=45$.

Una ricerca binaria sul file indice richiede approssimativamente $\log_2n_{bi}=6$ accessi al blocco. Il costo è dimezzato (dimezzare la ricerca binaria è *tanto*).

#### Indice di clustering

L'idea è la stessa degli indici primari.
A contrario degli indici primari, abbiamo una entry per ogni valore del campo, non per ogni blocco. Questa entry contiene l'indirizzo del primo blocco che contiene il valore.

Non ci sono esercizi su questo perché servirebbe sapere quanti sono i diversi valori del campo, non solamente il numero di record.

Il problema degli indici di clustering è che l'inserimento può richiedere di aggiungere un'altra voce ad un file ordinato, e che la cancellazione non sempre richiede di rimuovere anche la voce dell'indice, perché più record sono associati alla stessa entry.

#### Indice secondario

Anche questo è un file ordinato con due campi $(\langle inderxing\ field\rangle,\langle blocco\rangle)$.
Più avanti vedremo come gestire il fatto che lo stesso valore può comparire in più blocchi.

A differenza degli altri due è possibile associare due indici allo stesso file, perché *non* richiede che i file siano ordinati. Questo è il principale vantaggio di questo tipo di indice.
NON va bene avere un indice per ogni campo senza una buona ragione, mantenere gli indici costa, e potrebbe essere inutile.

Quando l'indice è costruito sulla chiave primaria è denso, abbiamo una entry del file di dati.
È più lento di un indice primario, ma consideriamo che in questa situazione (con file non ordinato) l'alternativa era quella di avere una scansione lineare. Quindi il miglioramento è altissimo.

Perché usiamo puntatori a blocco e non a record?
* Perché avremmo record più grandi, quindi peggiorerebbe il blocking factor ed aumenterebbero gli accessi in memoria secondaria
* Il costo della scansione nel blocco è comunque trascurabile

##### Esempio con indice denso

Supponiamo gli stessi dati dell'altro esempio

La scansione lineare richiederebbe $\frac{3000}2=1500$ accessi a blocco

L'indice secondario avrebbe $n_{bi}\frac{30000}{68}=442$ blocchi. Quindi la ricerca richiederebbe $\log_2n_{bi}=9$ accessi a blocco.

Il miglioramento è evidente.

Considerato che non richiedono di ordinare il file di dati (che è un'operazione molto costosa) possiamo dire che sono per ora l'opzione migliore.
Si liberano da quella che è la più grande debolezza degli indici primari, che è mantenere gli ordinamenti, in cambio di un piccolo peggioramento delle performance in lettura.

---

Come facciamo se il campo su cui costruiamo l'indice non è una chiave?
* Includiamo una voce per ogni record anche nell'indice, mantenendolo denso
  * L'indice diventa molto grande
* Consideriamo i record di lunghezza variabile ed elenchiamo tutti i blocchi che lo contengono $\langle p(i,1),...,p(i,k)\rangle$
  * I record perdono regolarità
* Il puntatore $p(i)$ punta ad un blocco, questo blocco contiene puntatori ai blocchi che contengono il valore
  * Eventualmente l'ultima voce è un puntatore ad un altro blocco di puntatori (se davvero compare tante volte il valore, è un caso estremo)

### Indici multilivello (si legge "alberi")

Norlamente una ricerca su indici richiede $\log_2b_i$ accessi a blocco. Perché eseguo la ricerca dicotomica e ad ogni passo dimezzo la porzione di indice da analizzare.

L'idea degli indici multilivello è di ridurre la porzione da analizzare da un numero maggiore di 2. Cerciamo di ridurla di un fattore pari a $bfr_i$ che di solito è *molto* maggiore di 2. Questo fattore è detto fan-out ($fo$) dell'indice multilivello.

La ricerca su indice multilivello richiede circa $\log_{fo}b_i$ accessi al blocco.

#### Indici multilivello statici

L'idea è che se guardiamo bene l'indice di singolo livello, abbiamo un file ordinato, che può essere indicizzato a sua volta con un indice primario. E questo indice dell'indice può di nuovo essere indicizzato.
Tutti questi indici hanno lo stesso $bfr$ del primo indice. Possiamo proseguire finché non abbiamo un indice di un singolo blocco (radice di un albero).

Se l'indice del primo livello ha $r_1$ entry e $bfr=fo$ Ha bisogno di $r_2=\frac {r_1}{fo}$ blocchi, che sono le entry che servono all'indice di secondo livello.
Ripetiamo la stessa operazione per il prossimo livello che avrà $r_3=\frac{\frac{r_1}{fo}}{fo}=\frac {r_2}{fo}$ blocchi.
Quindi genericamente $r_i=\frac{r_1}{fo^i}$.
Questo è un albero bilanciato non binario (lol, enby-tree... ok, la smetto).

Questa struttura sarebbe perfetta se il nostro database fosse statico. Non si riesce a mantenere bene.
Notiamo che ci serve un indice di secondo livello solo se il primo richiede più di un blocco, e ne serve un terzo solo se il secondo ne richiede più di uno. A questo punto il file di dati è già gigantesco.
Il livello è il risultato dell'equazione $t=\lceil\log_{fo}r_1\rceil$

##### Esempio

Con i soliti dati degli altri esempi.

Al primo livello di indice secondario servivano $442$ blocchi.

Al secondo livello ne servono $\lceil\frac{442}{68}\rceil=7$.

Al terzo livello serve $\lceil\frac7{68}\rceil=1$ blocco.

Per accedere all'elemento che cerchiamo bastano 3 accessi agli indici e 1 al file di dati. Questo contro i 9+1 accessi necessari con indice secondario, e supera anche i 6+1 accessi dell'indice primario.

Servirebbe lo stesso numero di accessi anche se invece che $30000$ record ne avessimo $bfr*bfr_i^3=3144320$. Questa soluzione è impressionante e ottima.
Purtroppo non può essere usata perché dobbiamo mantenere il bilanciamento agli inserimenti e rimozioni.

#### Introduzione indici multtilivello dinamici (B-tree e B$^+$-tree)

Riprendono l'idea degli indici multilivello statici, ma cercano risolvono il problema del bilanciamento e l'espansione.

Sono dei casi particolari di alberi di ricerca in cui ogni nodo (tranne la radice) ha un padre e 0 o più figli.
I nodi senza figli sono detti foglie.

##### Ripasso alberi di ricerca

Gli alberi di ricerca sono un tipo speciale di albero utilizzato per guidare la ricerca di record dato il valore di uno dei loro campi. Possiamo vedere gli indici multilivello statici come una variante particolare degli alberi di ricerca.
Ad ogni passaggio della ricerca su albero di ricerca ripetiamo la stessa operazione su un sottoalbero figlio del nodo corrente, ignorando tutto il resto dell'albero. Il fattore di riduzione di ogni passaggio è il fan-out dell'albero.

Un albero di ricerca di ordine $p$ è un albero i cui nodi contengono al più $p-1$ search value e $p$ puntatori, disposti nel seguente ordine: $\langle P_1,K_1,P_2,K_2,...,P_{q-1},K_{q-1},P_q\rangle$ con $q\leq p$.
Ogni valore $P_i$ è un puntatore a nodo figlio, ed ogni $K_i$ è un *search value* da un insieme totalmente ordinato (se i dati non sono facilmente ordinabili diventa difficile, ad esempio coordinate geografiche).

Ogni albero di ricerca soddisfa due vincoli fondamentali:
* In ogni nodo $K_1<K_2<K_3<...K_{q-1}$
* Per tutti i valori di $X$ presenti nel sottoalbero puntato da $P_i$ vale:
  * $K_{i-1}<X<K_i$ per $1<i<q$
  * $X<K_i$ per $1=i$
  * $K_{i-1}<X$ per $i=q$

Gli alberi di ricerca possono essere adattati per la ricerca dei blocchi di memoria su disco:
* I valori del campo di ricerca diventano i valori del search value
* Ad ogni valore di ricerca è associato un puntatore al record (o, meglio, al blocco) che lo contiene
  * Nella spiegazione, per semplicità, usiamo puntatori al record, ma non importa
  * Il professore aggiunge sempre "(o al blocco)", io lo dò per scontato
* Ad ogni nodo dell'albero corrisponde un blocco di un file
* Quando inseriamo un nuovo record nel file dei dati, l'albero di ricerca deve essere aggiornato includendo il valore del puntatore al record ed il campo di ricerca
  * Servono algoritmi appropriati per gestire queste operazioni
  * Quelli tipici (come quello del dynamic hashing) non sono bilanciati
    * Noi li vogliamo bilanciati
  * La soluzione sono B-tree e B+tree

#### B-tree

È un albero di ricerca con alcuni vincoli addizionali che garantiscono il bilanciamento ed il controllo dello spazio inutilizzato (che non si può evitare).

Per questo servono algoritmi di inserimento e cancellazione più complessi soprattutto nei casi di:
* Inserimento di un record in un nodo pieno
* Cancellazione di un record da un nodo che diventa mezzo vuoto (pieno per meno del 50%)

Formalmente un B-tree di ordine $p$ soddisfa le seguenti:
* Ha forma $\langle P_1,\langle K_1,Pr_1\rangle,P_2,\langle K_2,Pr_2\rangle,...,P_{q-1},\langle K_{q-1},Pr_{q-1}\rangle,P_q\rangle$ con $q\leq p$ uguale a prima ma con i puntatori a record
  * $P_i$ è un *tree pointer*
  * $Pr_1$ è un *data pointer*
  * Non memorizzando tutto il record (che può essere molto grande), risulta molto più piccolo del file di dati
* In ogni nodo $K_1<K_2<K_3<...K_{q-1}$
* Ogni noto ha al più $p$ tree pointer
* Per tutti i valori di $X$ presenti nel sottoalbero puntato da $P_i$ vale:
  * $K_{i-1}<X<K_i$ per $1<i<q$
  * $X<K_i$ per $1=i$
  * $K_{i-1}<X$ per $i=q$
* Ogni nodo diverso dalla radice deve avere almeno $\lceil\frac p2\rceil$ tree pointer
* Ogni nodo con $q$ tree pointer ha $q-1$ campi chiave di ricerca
* Tutti i nodi foglia sono posti allo stesso livello
  * E tutti i tree pointer nelle foglie sono null (quindi non hanno figli)

Altezza di un B-tree (solo i risultati):
* $h\leq\log_t\frac{n+1}2$
* $n\geq1+(t-1)\sum\limits_{n=1,...,h}2t^{i-1}=1+2(t-1)\frac{t^h-1}{t-1}=$
  * $n\geq2t^h-1$
* L'altezza è il numero di passaggi e quindi di accessi a blocco da eseguire. Quindi notiamo che la complessità è logaritmica
  * In realtà notiamo che alcuni dei valori non si trovano sull'ultima foglia, quindi non sempre servono $h$ passaggi
  * Risulterà che con fan-out molto grandi, la maggioranza dei valori è sulle foglie, al punto che mediamente li troveremo sempre là

Le operazioni che vedremo per i B-tree saranno tutte di tipo one-pass (non serve tenere traccia del percorso e risalirlo), per i B+tree vedremo anche alcune two-passes, asintoticamente sono uguali, ma quella one-pass è migliore numericamente.

##### Operazioni

Notazione:
* `T` albero T
* `p=2t` ordine di T
* `root[T]` radice di T
* `leaf[x]` vero se x è una foglia, falso altrimenti
* `n[x]` numero di chiavi in x
* `key_i[x]` chiave di indice i nel nodo x
  * Con `i` che va da 1 a `n[x]`
* `c_i[x]` puntatore di indice i nel nodo x
  * Con `i` che va da 1 a `n[x]+1` (nota il +1)

Creazione
```pascal
BTreeCreate(T)
  x <- AllocateNode()
  leaf[x] <- true
  n[x] <- 0
  DiskWrite(x)
  root[T] <- x
```

Ricerca
```pascal
BTreeSearch(x,k)
  i <- 1
  while i <= n[x] and k > key_i[x] do 
    i <- i+1
  if i <= n[x] and k = key_i[x] then
    return (x,i)
  if leaf[x] then
    return NIL
  else
    DiskRead(c_i[x])
    return BTreeSearch(c_i[x],k)

BTreeSearch(root[t],k)
```

Inserimento
```pascal
BTreeInsert(T,k)
  r <- root[T]
  if n[r] = 2t-1 then
    s <- AllocateNode()
    root[T] <- s
    leaf[s] <- false
    n[s] <- 0
    c_1[s] <- r
    BTreeSplitChild(s,1,r)
    BTreeInsertNonfull(s,k)
  else
    BTreeInsertNonfull(r,k)

BTreeSplitChild(x,i,y)
  z <- AllocateNode()
  leaf[z] <- leaf[y]
  n[z] <- t-1
  for j <- 1 to t-1 do
    key_j[z] <- key_(j+t)[y]
  if not leaf[y] then
    for j <- 1 to t do
      c_j[z] <- c_(j+t)[y]
      n[y] <- t-1
  for j <- n[x]+1 downto i+1 do
    c_(j+1)[x] <- c_j[x]
  c_(i+1)[x] <- z
  for j <- n[x] downto i do
    key_(j+1)[x] <- key_j[x]
  key_(i)[x] <- key_t[y]
  n[x] <- n[x]+1
  DiskWrite(y)
  DiskWrite(z)
  DiskWrite(x)

BTreeInsertNonfull(x,k)
  i <- n[x]
  if leaf[x] then
    while i >= 1 and k < key_i[x] do
      key_(i+1)[x] <- key_i[x]
      i <- i+1
    DiskWrite(x)
  else
    while i >= 1 and k < key_i[x] do
      i <- i-1
    i <- i+1
    DiskRead(c_i[x])
    if n[c_i[x]] = 2t-1 then
      BTreeSplitChild(x,i,c_i[x])
      if k > key_i[x] then
        i <- i+1
    BTreeInsertNonfull(c_i[x],k)
```

Il modello one-pass si basa sullo splittare sempre quando incontro un nodo pieno. Questo causa lo split di nodi che non sarebbe necessario splittare, ma permette di non dover mai risalire la ricorsione.

La complessità è logaritmica e non ho mai bisogno di valutare più di un nodo ed il suo genitore (in particolare nel caso della radice non ho un genitore).
Il numero di pagine che devo tenere in memoria à costante, non più di 2.
Una possibile versione a due passaggi scenderei senza preoccuparmi che i nodi siano pieni, e quando trovo una foglia piena, risalgo splittando i nodi superiori, quado splitto la radice aumenta un livello.

##### Progettazione

Supponiamo di avere come campo di ricerca di dimensione $V=9$ byte, blocchi da $B=512$ byte, block pointer di $P=6$ byte e data pointer di $P_r=6$ byte. Come segliamo il parametro $p$ che è l'ordine dell'albero?

Calcoliamo con una semplice disequazione:
$$pP+((p-1)(P_r+V))\leq B\\
22p\leq528\\
p\leq24$$

Scegliamo il più grande $p\in\N$ che soddisfa la disequazione (ad esempio 24 o 23)

Calcolano la percentuale $x$ di spazio utilizzato (in verifica viene data dal professore), possiamo assumere che il fan out sia uguale a $fo=x\times p$. In questo caso con $x=69\%$ e $p=23$ abbiamo $fo=16$.

Col fan-out possiamo calcolare quanti livelli sono presenti e quanti nodi, entry, e puntatori abbiamo per livello.
In questo caso abbiamo:
* Radice: 15 entry
* Livello 1: 240 entry
* Livello 2: 3840 entry
* Livello 3: 62440 entry
* Totale 4 livelli: 65535 entry

Notiamo l'osservazione secondo cui con grandi fan-out la maggior parte delle entry si trova nelle foglie. E questo è con un fan-out relativamente piccolo, solo 16.
I puntatori che abbiamo nei livelli interni sono spazio sprecato, perché i vantaggio che offrono è trascurabile. I B+tree puntano su questo per migliorare i B-tree.

#### B$^+$-tree o B+tree

Differenze dai B-tree, tutti i valori di ricerca e data pointer sono presenti nelle foglie. *Alcuni* valori sono presenti anche nei nodi interni per la ricerca, ma senza i data pointer.
Prima abbiamo visto che anch se dobbiamo scendere fino alle foglie per trovare i puntatori non è un problema perché comunque la stragrande maggioranza delle volte dobbiamo farlo comunque.

Se il campo di ricerca è una chiave il data pointer è un puntatore a record, altrimenti è un puntatore ad una catena di blocchi di puntatori a record, come negli indici secondari.

Se osserviamo le foglie notiamo che tutti i valori presenti nelle foglie sono ordinati (anche se potrebbe non essere così fisicamente), quindi è conveniente mantenere nelle foglie dei puntatori alle foglie successive per eseguire una scansione ordinata senza dover salire e scendere sull'albero.
Questo rende le range query (in cui si chiedono i valori compresi in un intervallo) molto efficientemente, cosa che non si poteva fare con i B-tree.

Lo svantaggio è che alcuni valori devon essere ripetuti nei nodi interni.

Vincoli:
* Ogni nodo interno ha struttura $\langle P_1,K_1,P_2,K_2,...,P_{q-1},K_{q-1},P_q\rangle$ con $q\leq p$
* In ogni nodo interno $K_1<K_2<K_3<...K_{q-1}$
* Ogni noto interno ha al più $p$ tree pointer e $p-1$ valori
* Per tutti i valori di $X$ presenti nel sottoalbero puntato da $P_i$ vale:
  * $K_{i-1}<X<K_i$ per $1<i<q$
  * $X<K_i$ per $1=i$
  * $K_{i-1}<X$ per $i=q$
* Ogni nodo diverso dalla radice deve avere almeno $\lceil\frac p2\rceil$ tree pointer
* Ogni nodo con $q$ tree pointer ha $q-1$ campi chiave di ricerca
* Ogni nodo **foglia** ha struttura $\langle\langle K_1,Pr_1\rangle,\langle K_2,Pr_2\rangle,...,\langle K_{q-1},Pr_{q-1}\rangle,P_{next}\rangle$ con $q\leq p_{leaf}$
* In ogni nodo foglia $K_1<K_2<K_3<...K_{q-1}$
* Ogni nodo foglia ha almeno $\lceil\frac {P_{leaf}}2\rceil$ valori
* Tutti i nodi foglia sono sullo stesso livello

##### Progettazione

Come steabiliamo l'ordine dei nodi? Stavolta ci serve stabilire l'ordine non solo dei nodi interni ma anche di quelli foglia. Supponiamo gli stessi dati dell'esempio con i B-tree:
$$pP+((p-1)V)\leq B\\
15p\leq512\\
p\leq34$$

Notiamo che nella stessa situazione ci troviamo con un fan-out considerevolmente maggiore.
E per i nodi foglia?
$$(p_{leaf}(P_r+V))+P\leq B\\
16p_{leaf}\leq506\\
p_{leaf}\leq31$$

Annche in questo caso abbiamo un miglioramento rispetto ai B-tree, però non così marcato come era per i nodi interni.

Assumendo la stessa percentuale di riempimento dell'altro esempio (quindi 69%, quindi 23 puntatori nei nodi interni e 21 nelle foglie). Per calcolare quante entry abbiamo con quanti livelli dobbiamo assumere che il prossimo livello sia il livello delle foglie e calcolare. Non serve poi sommare le entry di tutti i livelli, perche i puntatori stanno solo sul livello delle foglie.
* Radice: 23 puntatori
* Livello 1: 529 puntatori
* Livello 2: 12167 puntatori
* Livello 3 (foglie): 255507

Il miglioramento rispetto al caso del B-tree è evidente, passiamo da 65535 a 255507 con lo stesso numero di livelli. Senza contare il vantaggio delle range query che sono accelerate.

##### Operazioni

La notazione per qualche motivo è leggermente diversa da quella che abbiamo visto per i B-tree però sembar essere più intuitiva, quindi non serve spiegarla. A scopo dimostrativo mostriamo una soluzione a due passaggi in cui prima scendiamo fino alla foglia e poi se serve splittare risaliamo.

Per semplicità useremo delle descrizioni semantiche per alcune operazioni

Ricerca
```pascal
Search(T,k)
  x <- T.root
  DiskRead(x)
  while not x.leaf do #scorro fino alla foglia
    if k <= x.K then
      x <- x.P_1
    else if k > x.k_(x.n) then
      x <- x.P_(x.n+1)
    else
      j <- 1
      while x.k_j > k do
        j <- j+1
      x <- x.P_j
    DiskRead(x)
  j <- 1
  while x.k_j > k do #cerco nella foglia
    j <- j+1
  if j.k_j = k then #se trovo leggo, altrimenti nullo
    DiskRead(x.Pr_j)
    return x.Pr_j
  else
    return NIL
```

Inserimento
```pascal
Insert(T,k,Pr)
  S <- AllocateStack() #stack per la risalita
  x <- T.root
  while not x.leaf do
    S.push(x)
    scorro fino alla foglia cercando il valore
    ad ogni passaggio inserisco nello stack il nodo
  j <- 1
  while x.k_j > k do #cerco nella foglia
    j <- j+1
  if j.k_j = k then #se trovo non inserisco
    retutn
  else if x.n < P_leaf
    Inserisci nella posizione j e trasla il resto
  else
    Copia x in un nodo temporaneo che contiene una entry in più del normale
    Inserisci la entry (k,Pr) in temp nella posizione corretta
    new <- AllocateNode()
    new.P_next <- x.P_next
    j <- ceiling((P_leaf+1)/2)
    Copia in x le prime j entry di temp
    x.P_next <- new
    new <- le altre entry in temp

    #devo copiare il valore nel nodo padre tenendone anche una copia nella foglia
    #k è il più grande valore nel nodo x (non il più piccolo di new)
    k <- temp.K_j
    finito <- false
    repeat
      if S.empty then
        T.root <- AllocateNode()
        T.root <- (x,k,new)
        finito <- vero
      else
        x <- pop(S)
        if x.n < P then
          inserisci (k,new) in x
          finito <- true
        else
          Devo fare lo split del nodo
          Uso lo stesso split ch usavo nei B-tree normali
          Non serve tenere una copia del valore nel nodo splittato

          Copia x in temp
          Inserisci (k,new) in temp
          new <- AllocateNode()
          j <- floor((p+1)/2)
          x <- le prime j entry di temp
          new <- le restanti entry di temp
          k <- temp.K_j
    until finito
```

Il vantaggio della soluzione adue passaggi è che eseguo solamente gli split necessari. Lo svantaggio è chedevo mantenere anche lo stack dei genitori. In genere la soluzione one-pass è preferita anche se causa più split del necessario.
