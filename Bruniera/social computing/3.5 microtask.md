# Microtask design

Come progettiamo un (micro)task su mturk?

Notazione di Omar Alonso dei questionari:
* () radio button
  * (o) selezionata
  * Scelta singola
* [] checkbox
  * [v] selezionata
  * Scelta multipla
* [inputbox] input text
  * [...] riempito con ...
  * Fill in the blank
    * Testuale
    * Numerico
  * Raccolta dati
* \$variable\$ variabili del worker

## Esempietti

Esempietto 1:
* How do you feel torward XYZ?
  * () Love it
  * () Like it
  * () Neither
  * () Dislike it
  * () Hate it

Esempietto 4:
* You enter the query \$query\$ on the web, what is it about?
  * [] music
  * [] sport
  * ...
  * [] other [inputbox]

### Survey

Esempietto 3:
* How often do you xyz?
  * () Often
  * () Sometimes
  * () Never

### Relevance assesment

Vogliamo sapere quanto è rilevante un risultato od un documento ad una ricerca od un topic. Non bisogna valutare se ci è stato utile, solo se è rilevante, quindi se è un documento che ho già visto, ma è il documento giusto, va bene comunque. Non bisogna basarsi su titolo o tag, ma sul contenuto.

Si può fare 

Esempietto 2:
* You enter the query \$query\$ on the web, and get \$webpages\$
  * () Perfect
  * () Excellent
  * () Good
  * () Fair
  * () Bad

Spesso non si permette di non esprimersi e si inseriscono un numero pari di opzioni.

Esempietto 5:
* Rate the relevances of the \$document\$ to the space program as follow. Note that the satk is about how relevant to the topic the document is.
  * () Relevant
  * () Not relevant

Si può chiedere un commento sulla rilevanza, ma è poco funzionale perché i worker possono mettere robaccia per finire subito, e richiede elaborazione umana. Un testo livero è utile, così ti arrivano informazioni a cui non avevi pensato.
Si può aggiungere come commento opzionale oltre ad altre domande per avere più informazioni, ma senza perdere troppo tempo a scartare le risposte finte e valutando solo alcuni commenti scelti.

### Relevance assesment A/B test

Un metodo alternativo è confrontare la rilevanza di due articoli, invece che solo di uno. Per classificare gli articoli in base alla "rilevanza relativa".

* You enter the query \$query\$ on the web, and get \$webpageA\$ and \$webpageB\$
  * () A is better
  * () B is better
  * () Both irrelevant

Ques'ultimo è utile anche per la valutazione delle fakenwes ("quale è più falsa?"), od in generale la qualità di degli items.

### Scale fini e bias

Chi valuta può avere un bias, per sapere che bias ha si presentano degli statement da un pool etichettato da esperti, includendo anche degli statemente indubbiamente falsi e veri per sapere se inserisce a caso.

Per controllare che non inserisca a caso si fanno anche dei controlli:
* Si cercano stringhe senza senso come "asdf"
* Si controlla che i testi abbiano una lunghezza minima
* Si controlla che gli URL siano validi
* ...

Ovviamente i controlli non sono noti al worker, altrimenti potrebbe sfruttarli per fare una risposta appena sufficiente a passare i controlli.

Per valutare se conviene una scala fine od una grossolana (6 opzioni o 100 opzioni?) si può usare la stessa tecnica dei dataset etichettati da esperti, e fornire ad alcuni worker la scala fine ed alcuni la scala grossolana.
Conviene usare quella che si è avvicinata di più alle etichettature eseguite dagli esperti.

## Microtask design workflow (Omar Alonso)

"Progettare un task su mturk non è molto diverso da scrivere codice" - Alonso

Prograttiamo il task con un processo come quelli visti ad IS.

Processo di Alonso:
* Coding
  * Prototipo HIT
  * Release in laboratorio
* Controllo qualità
  * Test pilota
* Miglioramento della qualità
  * Produzione

A noi piace l'idea, ma non piace il processo.

Workflow design:
* Problema
  * Eventualmente torno ad una fase precedente
* Idea
  * Eventualmente torno ad una fase precedente
* Prototipo HIT (Human Intelligence Task)
  * Eventualmente torno ad una fase precedente
* Rilascio in laboratorio
  * Eventualmente torno ad una fase precedente
* Test pilota
  * Eventualmente torno ad una fase precedente
* Produzione
  * Torno ad una fase precedente

Usa un modello simile al waterfall. Non abbiamo cicli, abbiamo cicli, abbiamo solo una sequenza di sottoprocessi, e ad ogni sottoprocesso, se ci sono errori torniamo indieto.
Le fasi di prototipo, rilascio in laboratorio e test pilota diventano una sorta di ciclo di prototyping error-driven, ma non è dichiaratamente un ciclo, è solo l'effetto degli errori.

I passaggi indietro non erano espliciti nel modello di Alonso, ma inevitabilmente è quello che viene fatto quando si incontra un problema.

### 1 Problema

Capire bene il problema e definirlo formalmente.
Non risolvere il problema sbagliato (se si definisce male si ottiene una soluzione diversa).

*Meme con l'albero e l'altalena*

### 2 Idea di soluzione

Capire che esperimento devo fare per risolvere il problema.

Progetto l'esperimento

Altri dettagli:
* Cosa mostrare/chiedere
  * Faccio vedere degli articoli
  * Faccio vedere un riassunto/valutazione degli articoli
* Quanti/quali dati
  * Quanti/quali presentiamo ai worker
    * Provo su 1/50/100/... articoli
  * Quanti/quali richediamo dal worker
    * Ogni worker vede 1/2/3/... articoli
* Quanta ridondanza
  * Quanti worker ricevono lo stesso task
* Quanto pagare
  * Si valuta quanto tempo ci mettono in media il laboratorio e calcolano in base al minumum wage degli USA
* Controllo qualità
  * Come controlliamo che non abbiano ripsosto a caso?
* ...

Si usano schemi e schizzi

È importante scrivere bene le istruzioni del task:
* Chi?
  * Non un tecnico
* Brevi e concise
* Non troppo vaghe
* Evitare lunghi elenchi di cose da non fare
  * *Ironic...*
* Esempi se possibile
* Chiari Concisi Sintetici Focalizzati
* Pensa a chi legge

Si può scegliere di randomizzare l'ordine delle domande per non influenzare il giudizio. Magari legge una cosa banalmente falsa/vera e sottovaluta le domande successive.

Bisogna capire come raccogliere le risposte nel modo più semplice per il worker. Ad esempio uno slider per valori tra 0 e 100, un radio od una textbox per valori tra 0 e 6.

È buona pratica prevedere un campo di testo libero per un commento opzionale.

Ricorda HCI.

### 3 Prototipo HIT

Si devono implementare tutte le situazioni, eventualmente non su tutti i dati e facendo meno controlli di validazione.

Conviene preparare versioni alternative dell'interfaccia, per vedere quale si comporta meglio.

È una buona idea utilizzare strumenti già pronti e non fare tutto ad hoc:
* Linguaggi di markup e scripting
* Collegamenti con un server, magari con protocolli standard
* Pagine responsive
  * Molte persone lavorano da dispositivi mobili.

### 4 Rilascio in laboratorio

* Si fa svolgere il task ad un set ristretto di persone
  * Collaboratori
  * Amici
  * ...
* Senza pagamento
* Serve a trovare i bug
* Ci si prepara all'analisi dei dati

### 5 Test pilota

Come il rilascio in laboratorio, ma stavolta è aperto al pubblico (limitato, si accettano solo pochi worker). Quindi è svolto su worker veri e pagandoli.

Si usa un insieme di dati diverso, magari anche fittizzio. L'importante è che non sia il set che usiamo nell'esperimento vero.

L'obbiettivo è capire se è scalabile (prima avevamo una decina di worker), collaudare la GUI, collaudare le istruzioni, etc.

### 6 In produzione su mturk

Lancio l'esperimento finale, pagando veramente i worker, e spendendo di più.
Lancio le parti del test un po' per volta, facendo attenzione agli orari e le date (trovo persone diverse).

Bisogna essere pronti ad avere sbagliato qualcosa di stupido e dover rifare tutto da capo. Ricorda il modello di processo, si torna indietro anche dalla produzione.

### Risultati

Analisi dei dati! YEEEEEEH!

### Software programming vs wetware programming

Software:
* Non servono motivazioni
* Istruzioni atomiche chiare
* Competenze chiare
* Difficoltà di eseguire compiti semantici
* Errori inesistenti
* Onesti

Wetware:
* Motivazioni fondamentali
* Istruzioni non chiare
* Possibile incompetenza
* Capacità di eseguire compiti semantici
* Errori frequenti
* Maliziosi, opportunistici e disonesti

"La disumanità del computer sta nel fatto che, una volta programmato e messo in funzione, si comporta in maniera perfettamente onesta." - Isaac Asimov

"Il calcolatore è incredibilmente veloci, accurato e stupido. L'uomo è lento impreciso e creativo. L'insieme dei due costituisce una forza incalcolabile." - Non Albert Einstein