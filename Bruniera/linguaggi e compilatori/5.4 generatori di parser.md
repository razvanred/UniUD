# Generatori di lexer/parser

## BNF Converter (BNFC)

Il BNFC è un compilatore che genera un lexer e un parser da una grammatica in BNF etichettata, per un certo linguaggio target tra cui Java, C e Haskell.

Una grammatica bnf etichettata è una sequenza di regole formattate come `etichetta . NonTerminale ::= (Terminale | NonTerminale)`.

BNFC genera:
* Lexer (come programma per un altro strumento, flex/alex)
* Parser (come programma per un altro strumento, bison/happy)
* Un tipo di dato di sintassi astratta
* Un serializzatore per la sintassi astratta

Ha alcuni tipi di non terminale predefiniti:
* `Integer` interi
* `Double` virgola mobile
* `Char` singoli apici
* `String` doppi apici
* `Ident` identificatori

Escluso `Ident`, gli altri sono esposti al linguaggio target con il tipo corrispondente.

Altre feature:
* Semantic dummies
  * Un'etichetta può essere `_` quando la sua produzione ha un solo non terminale (ed eventuali terminali), non verrà generalizzato un tipo per quella produzione.
* Indexed non-terminals
  * Quando un non terminale finisce con un intero, viene usato per la precedenza con gli altri non terminali con lo stesso nome
* Macro
  * Sono abbreviazioni per un gruppo di regole che vengono generate, magari per fare qualcosa su più livelli di precedenza

Esempio:

```bnfc
AddOp. E ::= E "+" E1;
_. E ::= E1;

MulOp. E1 ::= E1 "*" E2;
_. E1 ::= E2;

NegOp. E2 ::= "-" E3;
_. E2 ::= E3;

IntVal. E3 ::= Integer;
Identifier. E3 ::= Ident;
_. E3 ::= "(" E ")";
```

La `coercion NT INT` macro semplifica la produzione di regole per la precedenza nelle espressioni. Con questa possiamo facilmente espandere l'esempio di prima come:

```
Add. E ::= E "+" E1;
Sub. E ::= E "-" E1;

Mul. E1 ::= E1 "*" E2;
Div. E1 ::= E1 "/" E2;
Mod. E1 ::= E1 "%" E2;

Pow. E2 ::= E3 "^" E2;

Neg. E3 ::= "-" E4;

Int. E4 ::= Integer;
Id. E4 ::= Iden;
Float. E4 ::= Double;

coercion E 4;
```

La `terminator NT T` macro define le appropriate regole per il simbolo `NT` per produrre sequenze anche vuote di `NT`, in cui ogni `NT` è terminato da `T`.
Opzionalmente possiamo aggiungere `nonempty` se la sequenza deve essere piena. Se come terminale usiamo `""` otteniamo una sequenza di terminali contigui.

```
terminator Stm ";";

[]. [Stm] ::= ";" ;
(:) [Stm] ::= Stm ";" [Stm];


---

terminator nonempty Stm ";";

(:[]). [Stm] ::= Stm ";" ;
(:). [Stm] ::= Stm ";" [Stm];
```

Nota: Il nome `[Stm]` è un nome qualsiasi, è solo più chiaro. Le etichette `(:),(:[])` funzionano perché il linguaggio target è Haskell. Comunque le regole non sono esattamente così, ma avremo qualcosa di simile.

La macro `separator` è simile a `terminator`, però l'ultimo elemento della lista non ha il separatore, vengono inseriti solo tra due elementi.
Anche questo ha il `nonempty`. Se come terminatore usiamo `""` è identico a `terminator`.

La macro `rules` produce un set di regole con etichette generate automaticamente. Possiamo usarlo quando non ci interessa il nome dell'etichetta.

```
rules Type ::= Type "[" "]"  | "float" | "int";

Type1. Type ::= Type "[" "]";
Type_float. Type ::= "float";
Type_int. Type ::= "int";
```

La macro `comment T` genera le regole per trattare il testo tra `T` e `$` come commento. La macro `comment T T'` genera le regole per trattare il testo tra `T` e `T'` come commento.

BNFC genera di default, un parser per ogni non terminale della grammatica (per usarli come simboli iniziali). Possiamo usare il pragma `entrypoint Stm, Exp, ... ;` per definire quali parser sono effettivamente generati. Non tutti i target (bison) la supportano, in quel caso fa quello che vuole.

Estendiamo ancora il linguaggio di prima, *aggiungendo* queste regole per le espressioni booleane:

```
Or. BoolExpr ::= BoolExpr "||" BoolExpr1;

-- copia dal sito, sono tante
```

Dobbiamo stare attenti perché il parser viene prodotto in modo meccanico ignorante. Se scrivi grammatiche che non sono veramente LALR, o che sono ambigue o non deterministiche, il parser viene prodotto comunque, ma non fa quello che vogliamo.
Se è ambigua, ci sono sempre stringhe che per pura fortuna vengono parsate correttamente, ma ce ne sono anche alcune che non lo saranno.
Quando la grammatica non va bene, happy riporta un conflitto, ma sceglie una soluzione e produce comunque il programma incorretto.

## Alex

Alex è un generatore di lexer, non di parser. Prende in input una descrizione di token e produce il programma haskell che esegue la tokenizzazione.

Struttura del sorgente
* `{ <header del modulo haskell> }`
* `<direttive>`, in particolare `%wrapper "<nome>"` per specificare alcuni codici ausiliari prodotti da alex
* `<definizione macro>`
* `<identificatore> :-`
* `<definizioni di token>` nella forma `<regex> {<codice haskell>}`
* `{<codice del modulo>}` opzionale

### Macro

Le macro sono di due tipi: quelle che definiscono insiemi di caratteri e quelle che definiscono espressioni regolari.

* `$<identificatore> = <set>`, un set può essere:
  * `char` Un singolo carattere unicode
  * `char-char` un range di caratteri
  * `.` tutto tranne lf
  * `set1 # set2` sottrazione tra insiemi
  * `[sets]` unione di set
  * `~set` complemento del set
  * `[^sets]` complemento di unione
  * `$ident` espande con a definizione di un altro set
* `@<identificatore> = <regex>`
  * `set` un carattere dal set
  * `@ident` espande una regex
  * `"..."` matcha la sequenza esatta
  * `r1 r2` r1 seguito da r2
  * `r1|r2` o r1 o r2
  * `r*` 0 o più occorrenze
  * `r+` 1 o più occorrenze
  * `r?` 0 o 1 occorrenze
  * `r{n}` n occorrenze
  * `r{n,}` n o più occorrenze
  * `r{n,m}` da n a m occorrenze

### Definizioni di token

Sono nella forma `<regex> {<azione>}` dove l'azione è un pezzo di codice haskell, che dovrebbe restituire il valore da inserire nella lista di token. Se un token vogliamo scartarlo mettiamo `;` al posto di `{...}`

Dobbiamo assicurarci che le azioni siano tutte dello stesso tipo. Alex non se ne accorge se sbagliamo, però poi non compila ghc.

Per ogni regola, alex cerca quella che matcha il prefisso più lungo, rimuove l'input ed esegue l'azione.
Se due regole matchano una stringa lunga uguale, segue l'ordine della definizione.

### Wrapper

Il wrapper "basic" è il più semplice, si aspetta che l'azione sia una funzione di tipo `String -> t` ed il lexer risultante sarà una funzione `String -> [t]`.
All'azione viene passata la stringa matchata per estrarne le informazioni sul lessema, ad esempio se viene matchata una costante numerica, per convertirlo da stringa a numero del tipo giusto.

Il wrapper "" chiede di definire un tipo posizione (chiamiamolo `Posn`), che viene inizializzato (dal programmatore), e ad ogni carattere letto viene aggiornato (da una funzione definita dal programmatore).
Le azioni non saranno funzioni `Posn -> String -> Token`, ma serve anche una azione per i match falliti che sia di tipo `Posn -> Token`.
È utile che il tipo token abbia due varianti principali, una per i match con successo e una per i match falliti, e che quella per i match con successo contenga un dato interno che è solo il token senza informazioni aggiuntive.
BNFC usa questo.

## Happy

È un generatore di parser LALR. Prende un file con una grammatica BNF annotata e produce un modulo haskell con un parser per la grammatica.
I parser di happy sono molto veloci. Ci sono opzioni per il debugging.

Struttura del sorgente
* `{ <header del modulo haskell> }`
* `<direttive>` in particolare `%tokentype {<tipo>}`
* `%token`
* `<definizione token>` costituite da un simbolo da usare nella grammatica e un pattern haskell per matchare il token `<simbolo> {<pattern>}`
  * Le variabili del pattern che non servono dovrebbero essere anonime
  * Se le marchiamo con `$$`, quella variabile è il valore del token
* `%%`
* `<produzioni>` nella forma `<regola BNF> {<codice haskell>}`
  * All'interno del codice haskell usiamo gli identificatori `$i` che vengono espansi al valore dell'iesimo token
* `{<codice del modulo>}` opzionale

Potrebbe essere una buona idea (noi non lo facciamo) scrivere delle produzioni di errore.

Possiamo aggiungere delle notazioni di tipo sulle produzioni, che happy non controlla, ma quando compiliamo con ghc si, quindi capiamo meglio dove sono i problemi.

Quando il parser finisce in una casella vuota della tabella viene chiamata la funzione `happyError` (definita dal programmatore) e gli viene passato lo stream di token non consumati.
A funzione dovrebbe cercare di capire cosa è successo e restituire un tipo di errore.

### Precedenze-associatività

In BNF non è possibile stabilire le precedenze, ma in happy si. Sono un modo per affrontare i conflitti shift/reduce e reduce/reduce (senza modificare le grammatica).

Le opzioni di associatività sono:
* `%noassoc <token> ...` per non associare le costruzioni
* `%left <token> ...` per associare a sinistra
* `%right <token> ...` per associare a destra

L'ordine della precedenza dipende dall'ordine in cui sono specificate le associatività.

Se due produzioni usano lo stesso token, ma devono avere precedenze diverse (esempio: sottrazione e negazione) possiamo associare la precedenza/associatività ad un placeholder, ed annotare la produzione con `%prec <placeholder>`.

## Flex

È un generatore di lexer per c, è il clone free di un programma più vecchio (lex). Le espressioni regolari sono simili a quelle di alex. (È alex che assomiglia a flex, più che il contrario).

Struttura:
* `%{ <intestazione> %}` Si possono inserire gli include ed eventuale codice c
* `%%`
* `<definizioni>` di regex ed insiemi di caratteri
* `<regex> <azione>;`
* `%%`
* `<epilogo>` altro codice c

Come matcha:
* Trova il match più lungo
* Se due regole matchano sulla stessa lunghezza, vince quella che compare prima.

Cosa fa:
* Dopo aver fatto il match, nella variabile globale `yytext` troviamo il puntatore al lessema, ed in `yyleng` troviamo la sua lunghezza.
* Dopo il match, esegue l'azione corrispondente, che ha accesso alle variabili globali di sopra
* Le azioni possono settare la variabile `yylval`, ci inserisce attributi del token che il parser può utilizzare
* Se non c'è nessun match esegue una regola di default. Il carattere successivo è considerato un match e viene rimosso dall'input e stampato in output.

## Bison

È un generatore di parser LALR(1), è il successore free di un generatore più vecchio (yacc) con cui è compatibile. Happy assomiglia a questo.
Può generare anche codice java, ma non è il massimo.

Struttura:
* `%{ <intestazione> %}`
* `<direttive>` come in happy
* `%token <token>` dichiarazione dei token, lo standard è usare token in caps
* `%%`
* `<produzioni>`
  * Possiamo usare caratteri ascii come `'+'` senza dichiararli, il parser li associa al token che è il valore intero corrispondente al carattere. (Il lexer evita gli interi piccoli per le cose che non sono caratteri singoli).
  * Le produzioni sono nella forma `<NT> : <espansione 1> { <azione 1> } |  ... ;`
  * Le azioni possono usare `$i` per accedere all'iesimo lessema
  * Per definire il calore della produzione (opzionale) si usa un assegnamento a `$$`
  * Il tipo di dato generato (il tipo di `$$`) è di default `int`, a meno che non usiamo `#define YYSTYPE <altro tipo>`
  * Se non mettiamo una azione il default è `{ $$ = $1 }`, per evitare confusione si consiglia di scriverlo comunque
  * Se non mettiamo la produzione ma solo l'azione è la produzione vuota. Si consiglia di scriverla comunque come `{-empty-} { <azione> }`
  * Si può inserire codice tra i simboli di una produzione, vengono eseguite prima che sia completata la produzione.
* `%%`
* `<epilogo>`

Ci sono delle differenze tra come funzionano flex/bison rispetto ad alex/happy, perché haskell è funzionale puro:
* flex/bison
  * Operano come master-slave. Quando al parser serve un nuovo lessema, chiama il lexer che lo consuma, e poi restituisce il controllo al parser
  * Questo modello è basato sui side effects del lexer che consuma l'input
  * Bison può modificare il comportamento di flex cambiando i valori delle sue variabili
  * Possiamo avere programmi che non producono alberi di parsing ma fanno esclusivamente cose con side effects
* alex/happy
  * Per come è scritto il codice, prima tutto l'input va al lexer, che lo consuma e passa tutti i lessemi al parser. Ma con la laziness in realtà i lessemi vengono consumati man mano che servono al parser
  * Tuttavia, senza side effect, il parser non può modificare il comportamento del lexer
  * Blah Blah Blah monadi Blah Blah

Le precedenze sono esattamente identiche a quelle di happy.

Error recovery:
* Il parser genera un token speciale `error` quando c'è un errore di sintassi, e lo inserisce nello stack. Questo simbolo è sempre definito ed è riservato.
* Si definiscono delle produzioni $A\rightarrow.~\mathrm{error}~\alpha$, dopo aver generato `error` il parser continua a scartare lessemi finche non trova un match.

$|\R^\omega|=|\R|$