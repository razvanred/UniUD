# Codifica

Come possiamo codificare il messaggio comprimendo al massimo l'informazione.

Il messaggio attraversa due codifiche:*
1. Messaggio
2. (possibile crittografia)
3. Codifica di sorgente
   * Comprime il messaggio
   * Applica bzip
4. (Possibile crittografia)
5. Codifica di canale
   * Aggiunge ridondanza per la correzione degli errori
   * Reed-Solomon
6. Canale
7. Decodifica canale
8. Decodifica sorgente
9. Messaggio

Vedremo solo la *codifica di sorgente*

## Codifica di sorgente

Abbiamo due alfabeti:
* Alfabeto di input $A$
* Alfabeto di output $B$
  * Di solito $\{0,1\}$ in informatica

La codifica è una funzione $\varphi:A^*\mapsto B^*$, che mappa un messaggio in alfabeto $A$ ad uno in alfabeto $B$.

Proprietà importanti di $\varphi$:
* È iniettiva, uniquely decodable
  * Devo poter calcolare $\varphi^{-1}$, quindi devo riconoscere i messaggi in $B^*$ invalidi 
* È calcolabile in modo efficiente
  * $\Theta(n)$
  * Le più semplici associano ad ogni carattere in $A$ una sequenza di caratteri $B$ ed eseguono una sostituzione
    * $\varphi(a_1a_2....a_n)=\varphi(a_1)\varphi(a_2)...\varphi(a_n)$
    * Esempio: ASCII, Morse
    * Se la sequenza corrispondente ad ogni carattere è sempre lunga uguale si chiamano codici Blocco-Blocco

Esempi:

> > a - 0\
> > b - 01\
> > c - 10
>
> Non è UD, 010

> > a - 0\
> > b - 01\
> > c - 011
>
> È UD. Può essere decodificato da un DFA. Per decodificarlo bisogna guardare avanti di 1, si dice *con ritardo 1*.

> > a - 00\
> > b - 1\
> > c - 10
>
> *Ritardo unbounded*.
> Per sapere se il primo 0 dopo una sequenza di 1 è quello di una c o di una a devo andare avanti fino alla fine degli 0 e vedere se sono pari o dispari.

## Prefix (free) Code: codice aprefisso

$$
\forall a_1,a_2\nexists a_3~(\varphi(a_1)\varphi(a_3)=\varphi(a_2))
$$

La codifica di *nessun* carattere è prefissata dalla codifica di un altro carattere.
È UD senza ritardo.

Vuol dire che nell'albero di decodifica, etichetto solo foglie. Se $\varphi$ è prefisso, devo percorrere un cammino radice-foglia per ogni carattere che decodifico. Non serve che siano bilanciati.


## Codici che comprimono

$$
A=\{a_1,...,a_k\}\\
l_i=|\varphi(a_i)|\\
l_1\leq...\leq l_k
$$

In un codice che comprime voglio minimizzare la lunghezza media $EL(\varphi)=\sum\limits_{i=1}^kp_il_i$

Esempio semplificato:
> a, b, c\
> 1/2, 1/4, 1/4
> > a - 111\
> > b - 01\
> > c - 001
> 
> $EL(\varphi)=\frac52$ male
>
> > a - 1\
> > b - 01\
> > c - 001
>
> $EL(\varphi)=\frac32$ meglio.
> Con 3 lettere non si può fare tanto meglio

Notare che anche se 'a' è molto probabile non significa che "aaaa" sia molto probabile.

## Teorema Kraft-McMillan

Formulato nel 1958, noto anche come inverse theorem.

> Se una codifica $\varphi$ è UD allora.
>
> $$\sum^k_{i=1} D^{-l_i}~\leq1$$
>
> Dice che i codici UD tendono ad essere più lunghi.
> 
> Non è un SSE, non basta verificare che sia vera per dire che il codice è UD, ma basta verificare sia falsa per dire che non è UD

### Dimostrazione 1

Non è la vera dimostrazione, ma è più facile da capire. Poi la usiamo per la seconda dimostrazione.

Assumiamo che $\varphi$ sia prefisso (quindi anche UD). E che $l_1\leq...\leq l_k=l$

Supponiamo di avere 'albero di decodifica del codice, la lunghezza del percorso dalla radice al nodo $a_i$ è $l_i$.

Visto che il codice è prefisso, sotto il nodo di un carattere l'albero non ha più nodi corrispondenti a caratteri. Questi nodi sono $D^{l-l_i}$.

Quindi mettendo insieme i nodi "sprecati" da ogni carattere otteniamo:

$$
D^l-\sum_{i=1}^{k-1}D{l-l_i}~\geq1
$$

Con una serie di trasformazioni algebriche si torna alla disuguaglianza del problema

### Dimostrazione 2

Sia $N(n|h)$ il numero di stringhe in $A^n$ con codifica di lunghezza $h$.

$N(n|h)\leq D^h$ perché $\varphi$ è UD.

Quindi, per ogni stringa di lunghezza $n$ (naturale), vogliamo dimostrare  è la sua codifica ha lunghezza $O(n)$.
$$
\begin{aligned}
& (D^{-l_1}+...D^{-l_k})^n\leq1\\
& D^{-l_1n}+D^{-l_1(n-1)}+...\leq1&[\text{espansione della potenza}]\\
& N(n|1)D^{-1}+...+N(n|l_1n)D^{-l_1n}+...+N(n|l_kn)D^{-l_kn}& [N(n|h)\leq D^h]\\
& \leq D^1D^{-1}+D^2D^{-2}+...+D^{l_kn}\leq l_kn & [\text{quindi è lineare}]\\
& D^{-l_1}+...D^{-l_k}\leq1\\
& \sum^k_{i=1} D^{-l_i}\leq1&[\text{CVD}]\\
\end{aligned}
$$

## Direct theorem

> Se $l_1,...,l_k$ e $D$ Sono tali che $\sum\limits^k_{i=1} D^{-l_i}~\leq1$.\
> Allora esiste un codice prefisso $\varphi$ con queste lunghezze di encoding.

Questo significa che i codici UD con ritardo non comprimono più dei codici senza ritardo. Perché ogni volta che le codifiche dei caratteri sono abbastanza lunghe da avere un codice UD, possiamo averne uno prefisso.

## Teorema di Shannon (1948)

Definiamo la lunghezza media (attesa) del codice: $EL(\varphi)=\sum\limits^k_{i=1}p_i|\varphi(a_i)|$. Vogliamo un codice prefisso che minimizzi questa quantità.

È utile se assumiamo una sorgente senza memoria e con distribuzione stazionaria.\
Nel senso che se il primo carattere generato è $x_i$ e il secondo è $x_2$, la probabilità che il secondo sia $x_2$ non dipende ne da $x_1$ (senza memoria), ne dal tempo passato (stazionaria).\
È il caso più semplice possibile.

Definiamo l'entropia D-adica $H_D(P)=-\sum\limits^k_{i=1}p_i\log_D(p_1)$.

> Se $\varphi$ è UD, allora:\
> $EL(\varphi)\geq H_D(P)$

In cose di ingegneria questo teorema è in una forma che non riconosciamo perché è messo insieme ad altri concetti.

### Dimostrazione

Sapendo che $-\ln x\geq -(x-1)$ (facilmente dimostrabile).

$$
\begin{aligned}
& EL(\varphi)-H_D(P)\\
& =\sum^k_{i=1}p_il_i+\sum^k_{i=1}p_i\log_D(p_1)\\
& =\sum^k_{i=1}p_i\log_D(D^{l_i}p_i)\\
& =\frac1{\ln D}\sum^k_{i=1}p_i\ln(D^{l_i}p_i)\\
& =-\frac1{\ln D}\sum^k_{i=1}p_i\ln(\frac1{D^{l_i}p_i})\\
& \geq-\frac1{\ln D}\sum^k_{i=1}p_i(\frac1{D^{l_i}p_i}-1)\\
& = -\frac1{\ln D}(\sum^k_{i=1}\frac1{D^{l_i}}+\sum^k_{i=1}p_i)\\
& =\frac1{\ln D}(1-\sum^k_{i=1} D^{-l_i})\\
& \geq0
\end{aligned}
$$

## Codifica di Shannon

Shannon cerca di ottimizzare la lunghezza attesa del codice per eguagliare l'entropia.\
Un idea iniziale è quella di scrivere un codice dove la lunghezza dei codici dei caratteri è vicina all'entropia del carattere. Si può fare sempre col valore esatto? no, bisogna usare ceil: $l_i=\lceil\log_D\frac1{p_i}\rceil$.

Usiamo il direct theorem per dimostrare $\sum\limits^k_{i=1}D^{-\lceil\log_D\frac1{p_i}\rceil}\leq1$:

$$
\begin{aligned}
& \lceil\log_D\frac1{p_i}\rceil=\log_D\left(\frac1{p_i}\right)+\beta_i & [0\leq\beta_i<1]\\
& \Rightarrow\sum\limits^k_{i=1}D^{\log_D\left(\frac1{p_i}\right)-\beta_i}\\
& =\sum\limits^k_{i=1}D^{\log_D\left(\frac1{p_i}\right)}\frac1{D^{\beta_i}}\\
& =\sum\limits^k_{i=1}p_i\frac1{D^{\beta_i}}\\
& \leq1
\end{aligned}
$$

Non è un buon modo perché viene assegnato un codice lungo ai caratteri poco probabili anche quando non ce n'è bisogno.

### La codifica di Shannon è subottimale

Per i codici di Shannon vale:

$$H_D(P)\leq EL(\varphi)<H_D(P)+1$$

Quando proviamo a calcolare Shannon per tuple di caratteri ($A=\{aa,ab,ba,bb\}$) l'efficienza aumenta rispetto a quella della codifica dell'alfabeto.

$$
\mathrm{Eff}=\frac{H_D(p)}{EL(\varphi)}\leq1
$$

proviamo a dimostrarlo:\
Siano $X,Y$ variabili casuali indipendenti. Vogliamo dimostrare che $H(X\land Y)=H(X)+H(Y)$
$$
\begin{aligned}
& H(X\land Y)=-\sum_{i,j}p_iq_j\log p_iq_j\\
& =-\sum_{i,j}p_iq_j\log p_i-\sum_{i,j}p_iq_j\log q_j\\
& =\sum_j q_j\sum_i p_i\log p_i-\sum_i p_i\sum_i q_j\log q_j\\
& =H(X)+H(Y)
\end{aligned}
$$

Segue che $EL(\varphi_n)\geq H_D(p^n)=nH_D(P)$. Quindi:
$$
\frac{\not nH_D(P)}{\not n}\leq EL(\varphi_n)<\frac{\not nH_D(P)}{\not n}+\frac1n
$$

La componente $\frac1n$ diventa sempre più piccola all'aumentare di $n$, quindi più $n$ è grande, più la lunghezza attesa si avvicina al lower bound.

> **Se aumentare la lunghezza aumenta l'efficienza, il codice si dice *subottimale***

## Codifica di Shannon-Fano

Cerchiamo di "distribuire le probabilità" tra i rami dell'albero.
Separiamo ricorsivamente i rami sul mediano in modo che: $\left|\sum\limits^h_{i=1}p_i-\sum\limits^k_{i=h+1}p_i\right|\simeq0$. Non si può sempre separare esattamente a metà, ma dopo averli ordinati è facile trovare punto quasi ottimale. Separarli esattamente nel modo ottimale è un problema NP-hard.

Shannon-Fano è subottimale

## Codifica di Huffman

Data la probabilità di ogni carattere, costruisco un albero di un singolo nodo da ogni carattere, e li inserisco in una min-heap.
Estraggo i due meno probabili e li unisco in un albero con come peso la somma dei pesi.
Quando è rimasto solo un albero, quello è l'albero di codifica.

Huffman è ottimale, ma nella pratica solo a volte è più corto di fano. È quello che dà la lunghezza attesa più bassa.

## L-ZIV

Carrellata storica:
* 1949 Shannon-Fano
* 1952 Huffman (progetto per il corso di Fano)
* Huffman diventa popolare nel 1970
* 1977 Lempel-Ziv LZ77
  * Usa un dizionario dinamico
  * Ci concentreremo su questo
* 1978 LZ78
  * Dizionario statico
  * 1981 Brevettata da Sperry Corporation, che diventa Unisys
* 1983 Lempel-Ziv-Welch LZW
  * Come LZ78 ma semplice da implementare
  * 1983 richiesto il brevetto da Sperry
  * 1984 pubblicato senza menzionare il brevetto
  * 1985 accettato il brevetto
  * Era già in uso da:
    * Unix compress
    * Compuserve GIF
  * 24-12-1994 Unisys inizia a riscuotere (Tassa sul GIF)
  * 2004 scade il brevetto
* ~1994 compress inizia il passaggio a deflate
  * Basato su LZ77, precursore di gzip

La parte di messaggio già codificata è il "search buffer". La prima parte del messaggio ancora da codificare è il "look ahead buffer", la lunghezza dipende dall'implementazione.
Cerco il più lungo prefisso del look ahead che È anche sottostringa del search buffer, la sottostringa può sforare nel look ahead buffer. Versioni più generiche permettono anche al look ahead di sforare.

Esempio:


| S   |     |     |     |     | L   |     |     |     |     |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| a   | a   | a   | x   | y   | x   | y   | x   | y   | z   |
|     |     |     | ^   | ^   | ^   | ^   |     |     |     |
|     |     |     | j   |     | i   |     |     |     |     |

| S   |     |     |     |     |     | L   |     |     |     |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| a   | a   | a   | x   | y   | x   | x   | y   | z   | z   |
|     |     |     | ^   | ^   |     |     |     |     |     |
|     |     |     | j   |     |     | y   |     |     |     |

Memorizzo, `(o,l,s)` che sono la distanza tra `i` e `j`, la lunghezza del prefisso-suffisso, il primo carattere di mismatch. `(2,6,z)`.

Esempi:
* Codifica di `babbababbabbabba`, look ahead 5
  * `(0,0,b)`
  * `(0,0,a)`
  * `(2,1,b)`
  * `(3,2,a)`
  * `(5,4,b)`
  * `(3,4,#)`
* Decodifica:
  * `(0,0,b)` -> `b`
  * `(0,0,a)` -> `a`
  * `(2,1,b)` -> `bb`
    * A distanza 2 prendi 1 e aggiungi `b`
  * `(3,2,a)` -> `aba`
    * A distanza 3 prendi 2 e aggiungi `a`
  * `(5,4,b)` -> `bbabb`
    * A distanza 5 prendi 4 e aggiungi b
  * `(3,4,#)` -> `abb` + `a`
    * A distanza 3 prendi 4 e finisce il messaggio
    * L'ultimo `a` è preso dalla parte appena decodificata

È ottimale, non sotto le ipotesi di assenza di tempo e memoria, ma sotto altre ipotesi, ma non ne parleremo in questo corso.

## Kolmogorov

Carrellata storica:
* Shannon 1940
  * MIT
  * Princeton
  * Bell labs
* Fano
  * PoliTo
  * MIT
* Huffman
  * Ohio State University
* Kolmogorov 1965
  * Mosca
  * Non conosce il lavoro degli altri
  * Sta studiando la casualità di una stringa

Consideriamo un modello di calcolo (noi usiamo una MdT), definisco come complessità di una stringa, la lunghezza del più corto programma che la genera.

Esempio:

```rust
for _i in 0..1000000 {
  print!("1");
}
```

Le uniche parti di questo programma che contano sono `1` e `1000000`, quindi potrei avere un modello di calcolo a cui bastano quei due numeri per generare la stringa.
Quindi per questo modello di calcolo, la lunghezza del programma è 8+30 bit.

Ripasso MdT $M=\langle K,\Sigma,\delta,s\rangle$:
* $K$ sono stati
* $\Sigma$ è l'alfabeto
  * $\triangleright,\Box\in\Sigma$
* $\delta:\Sigma\times K\mapsto\Sigma\times(K\cup\{y,n,h\})\times\{\leftarrow,\rightarrow,-\}$ è la funzione di transizione 
  * Quello che conta, il mio programma, è la $\delta$
* $s\in K$ È lo stato iniziale

Useremo le macchine con $k$ nastri, quindi con $\delta:\Sigma^k\times K\mapsto\Sigma^k\times(K\cup\{y,n,h\})\times\{\leftarrow,\rightarrow,-\}^k$.

Le funzioni $\delta$ sono numerabili, quindi posso trasformare ogni macchina $m$ in un mostruoso numero binario $\mathrm{bin}(m)$ che un'altra macchina di turing può riconoscere ed eseguire.

> Definiamo la *complessità di Kolmogorov di $x$* come:
> $$K_u(x)=\min_{u(\mathrm{bin}(m))=x}|\mathrm{bin}(m)|$$
> Possiamo definire la complessità solo dopo che abbiamo fissato una macchina universale $u$, quindi il modello di calcolo.\
> Cambiare macchina cambia la complessità.

Notiamo subito che $K_u$ non è calcolabile. Possiamo calcolare con un algoritmo dei maggioranti della complessità, ma non possiamo sapere se la complessità è quella o è più bassa.
Ovviamente possiamo provare la complessità di stringhe specifiche, ma non possiamo scrivere un algoritmo che la calcoli.

> Definiamo la *complessità di Kolmogorov di $x$ dato $y$* come:
> $$K_u(x|y)=\min_{u(\mathrm{bin}(m),y)=x}|\mathrm{bin}(m)|$$
> (Nota che $m$ prende in input $y$)\
> È intuibile che $K_u(x|y)\leq K_u(x)$

La complessità condizionale più utilizzata è $K_u(x~|~|x|)$.

Siano $A$ ed $u$ due macchine universali, posso usare $u$ per calcolare $A$ che calcola un'altra macchina $m(y)=x$, sarà più lento di eseguire direttamente $A$ ma di un tempo costante (il costo di ogni passo), allora:
$$
K_u(x|y)\leq K_A(x|y)+c^u_A
$$

Possiamo intuire che vale la stessa disuguaglianza se scambiamo $u$ ed $A$

> **Invarianza della complessità di Kolmogorov**\
> Se due macchine $A$ ed $u$ sono universali, allora la differenza della complessità calcolata per entrambe è al più costante:
> $$|K_u(x|y)-K_A(x|y)|\leq c$$

> La complessità di Kolmogorov (KC) di una stringa è al più la lunghezza della stringa, più una costante. La costante è dovuta ai simboli start e finish della MdT, che possono essere visti come il terminatore di una stringa C o altri meccanismi in altri linguaggi, ma anche come la fine del codice di un programma.
> 
> Dimostrazione: sia una *macchina universale stampante* $u$ una macchina che quando legge in input un numero che inizia con 0 si comporta come una normale macchina universale per il resto del numero, ma quando inizia con 1 stampa il resto del numero fino al blank.
> Allora per questa macchina vale $K_u(x)\leq1+|x|$.
> Possiamo immaginare il comportamento di questa macchina come la possibilità di scrivere il programma `print("stringa")`.\
> Qualsiasi macchina universale $A$ può eseguire questa macchina $u$, allora $K_A(x)\leq |\mathrm{bin}(u)|+1+|x|$ e *fissata la macchina $A$* si ha $|\mathrm{bin}(u)|+1$ costante

Il numero di stringhe con KC minore di $h$ è minore di $2^h$. Perché il numero dele MdT in totale di lunghezza minore di $h$ è $2^h-1$ e comunque non è detto che tutte siano vere macchine che terminano.

C'è sempre almeno una stringa $x$ tale chè $K(x)\geq|x|$. Perché ci sono meno di $2^{|x|}$ macchine di lunghezza $|x|$, ma ci sono $2^{|x|}$ stringhe di lunghezza $h$.

Tutte queste informazioni su KC ovviamente non la rendono più calcolabile.

Possiamo definire la codifica $\varphi_K(x)=\mathrm{bin}(m):m()=x$. Questa codifica è UD, e KC è correlato all'entropia di $\varphi_K$.

### Legame tra KC ed entropia

Cerchiamo di limitare la KC in base all'entropia.\
Definiamo la codifica di Kolmogorov come $\varphi_K:A^*\mapsto\{m:m~\text{è la codifica binaria di una MdT}\}$.
La lunghezza media della codifica è $EL(\varphi_K)=\sum\limits_{x\in A^*}p(x)|\varphi_K(x)|$.\
Per il teorema di shannon vale $EL(\varphi_k)\geq H(P^n)=nH(P)$.\
Otteniamo che $E_n(K(x))\geq nH(P)$.

Ora cerchiamo di limitare l'entropia in base a KC.\
Ogni codifica UD $\varphi$ può essere vista come un set di macchine per produrre stringhe. La codifica di un messaggio, insieme alla decodifica sono una MdT che produce $m$.\
$K(x)\leq|\mathrm{decoder}\varphi|+|\varphi(x)|$, quindi $E_n(K(x))\leq|\mathrm{decoder}\varphi|+EL_n(\varphi)$.\
Allora $E_n(K(x))\leq|\mathrm{decoder}\varphi|+EL_n(\varphi)$.
Se $\varphi$ è uan codifica di Shannon allora abbiamo $EL_n(\varphi)\leq nH(p)$.

Possiamo concludere che:

$$
\frac{E_n(K(x))}n\leq\frac{K(p)}n+H(P)+\frac1n
$$