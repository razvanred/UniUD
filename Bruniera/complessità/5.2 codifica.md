# Codifica

Come possiamo codificare il messaggio comprimendo al massimo l'informazione.

Il messaggio attraversa due codifiche:*
1. Messaggio
2. (possibile crittografia)
3. Codifica di sorgente
   * Comprime il messaggio
   * Applica bzip
4. (Possibile crittografia)
5. Codifica di canale
   * Aggiunge ridondanza per la correzione degli errori
   * Reed-Solomon
6. Canale
7. Decodifica canale
8. Decodifica sorgente
9. Messaggio

Vedremo solo la *codifica di sorgente*

## Codifica di sorgente

Abbiamo due alfabeti:
* Alfabeto di input $A$
* Alfabeto di output $B$
  * Di solito $\{0,1\}$ in informatica

La codifica è una funzione $\varphi:A^*\mapsto B^*$, che mappa un messaggio in alfabeto $A$ ad uno in alfabeto $B$.

Proprietà importanti di $\varphi$:
* È iniettiva, uniquely decodable
  * Devo poter calcolare $\varphi^{-1}$, quindi devo riconoscere i messaggi in $B^*$ invalidi 
* È calcolabile in modo efficiente
  * $\Theta(n)$
  * Le più semplici associano ad ogni carattere in $A$ una sequenza di caratteri $B$ ed eseguono una sostituzione
    * $\varphi(a_1a_2....a_n)=\varphi(a_1)\varphi(a_2)...\varphi(a_n)$
    * Esempio: ASCII, Morse
    * Se la sequenza corrispondente ad ogni carattere è sempre lunga uguale si chiamano codici Blocco-Blocco

Esempi:

> > a - 0\
> > b - 01\
> > c - 10
>
> Non è UD, 010

> > a - 0\
> > b - 01\
> > c - 011
>
> È UD. Può essere decodificato da un DFA. Per decodificarlo bisogna guardare avanti di 1, si dice *con ritardo 1*.

> > a - 00\
> > b - 1\
> > c - 10
>
> *Ritardo unbounded*.
> Per sapere se il primo 0 dopo una sequenza di 1 è quello di una c o di una a devo andare avanti fino alla fine degli 0 e vedere se sono pari o dispari.

## Prefix (free) Code: codice aprefisso

$$
\forall a_1,a_2\nexists a_3~(\varphi(a_1)\varphi(a_3)=\varphi(a_2))
$$

La codifica di *nessun* carattere è prefissata dalla codifica di un altro carattere.
È UD senza ritardo.

Vuol dire che nell'albero di decodifica, etichetto solo foglie. Se $\varphi$ è prefisso, devo percorrere un cammino radice-foglia per ogni carattere che decodifico. Non serve che siano bilanciati.


## Codici che comprimono

$$
A=\{a_1,...,a_k\}\\
l_i=|\varphi(a_i)|\\
l_1\leq...\leq l_k
$$

In un codice che comprime voglio minimizzare la lunghezza media $EL(\varphi)=\sum\limits_{i=1}^kp_il_i$

Esempio semplificato:
> a, b, c\
> 1/2, 1/4, 1/4
> > a - 111\
> > b - 01\
> > c - 001
> 
> $EL(\varphi)=\frac52$ male
>
> > a - 1\
> > b - 01\
> > c - 001
>
> $EL(\varphi)=\frac32$ meglio.
> Con 3 lettere non si può fare tanto meglio

Notare che anche se 'a' è molto probabile non significa che "aaaa" sia molto probabile.

## Teorema Kraft-McMillan

Formulato nel 1958, noto anche come inverse theorem.

> Se una codifica $\varphi$ è UD allora.
>
> $$\sum^k_{i=1} D^{-l_i}~\leq1$$
>
> Dice che i codici UD tendono ad essere più lunghi.
> 
> Non è un SSE, non basta verificare che sia vera per dire che il codice è UD, ma basta verificare sia falsa per dire che non è UD

### Dimostrazione 1

Non è la vera dimostrazione, ma è più facile da capire. Poi la usiamo per la seconda dimostrazione.

Assumiamo che $\varphi$ sia prefisso (quindi anche UD). E che $l_1\leq...\leq l_k=l$

Supponiamo di avere 'albero di decodifica del codice, la lunghezza del percorso dalla radice al nodo $a_i$ è $l_i$.

Visto che il codice è prefisso, sotto il nodo di un carattere l'albero non ha più nodi corrispondenti a caratteri. Questi nodi sono $D^{l-l_i}$.

Quindi mettendo insieme i nodi "sprecati" da ogni carattere otteniamo:

$$
D^l-\sum_{i=1}^{k-1}D{l-l_i}~\geq1
$$

Con una serie di trasformazioni algebriche si torna alla disuguaglianza del problema

### Dimostrazione 2

Sia $N(n|h)$ il numero di stringhe in $A^n$ con codifica di lunghezza $h$.

$N(n|h)\leq D^h$ perché $\varphi$ è UD.

Quindi, per ogni stringa di lunghezza $n$ (naturale), vogliamo dimostrare  è la sua codifica ha lunghezza $O(n)$.
$$
\begin{aligned}
& (D^{-l_1}+...D^{-l_k})^n\leq1\\
& D^{-l_1n}+D^{-l_1(n-1)}+...\leq1&[\text{espansione della potenza}]\\
& N(n|1)D^{-1}+...+N(n|l_1n)D^{-l_1n}+...+N(n|l_kn)D^{-l_kn}& [N(n|h)\leq D^h]\\
& \leq D^1D^{-1}+D^2D^{-2}+...+D^{l_kn}\leq l_kn & [\text{quindi è lineare}]\\
& D^{-l_1}+...D^{-l_k}\leq1\\
& \sum^k_{i=1} D^{-l_i}\leq1&[\text{CVD}]\\
\end{aligned}
$$

## Direct theorem

> Se $l_1,...,l_k$ e $D$ Sono tali che $\sum\limits^k_{i=1} D^{-l_i}~\leq1$.\
> Allora esiste un codice prefisso $\varphi$ con queste lunghezze di encoding.

Questo significa che i codici UD con ritardo non comprimono più dei codici senza ritardo. Perché ogni volta che le codifiche dei caratteri sono abbastanza lunghe da avere un codice UD, possiamo averne uno prefisso.

## Teorema di Shannon (1948)

Definiamo la lunghezza media (attesa) del codice: $EL(\varphi)=\sum\limits^k_{i=1}p_i|\varphi(a_i)|$. Vogliamo un codice prefisso che minimizzi questa quantità.

È utile se assumiamo una sorgente senza memoria e con distribuzione stazionaria.\
Nel senso che se il primo carattere generato è $x_i$ e il secondo è $x_2$, la probabilità che il secondo sia $x_2$ non dipende ne da $x_1$ (senza memoria), ne dal tempo passato (stazionaria).\
È il caso più semplice possibile.

Definiamo l'entropia D-adica $H_D(P)=-\sum\limits^k_{i=1}p_i\log_D(p_1)$.

> Se $\varphi$ è UD, allora:\
> $EL(\varphi)\geq H_D(P)$

In cose di ingegneria questo teorema è in una forma che non riconosciamo perché è messo insieme ad altri concetti.

### Dimostrazione

Sapendo che $-\ln x\geq -(x-1)$ (facilmente dimostrabile).

$$
\begin{aligned}
& EL(\varphi)-H_D(P)\\
& =\sum^k_{i=1}p_il_i+\sum^k_{i=1}p_i\log_D(p_1)\\
& =\sum^k_{i=1}p_i\log_D(D^{l_i}p_i)\\
& =\frac1{\ln D}\sum^k_{i=1}p_i\ln(D^{l_i}p_i)\\
& =-\frac1{\ln D}\sum^k_{i=1}p_i\ln(\frac1{D^{l_i}p_i})\\
& \geq-\frac1{\ln D}\sum^k_{i=1}p_i(\frac1{D^{l_i}p_i}-1)\\
& = -\frac1{\ln D}(\sum^k_{i=1}\frac1{D^{l_i}}+\sum^k_{i=1}p_i)\\
& =\frac1{\ln D}(1-\sum^k_{i=1} D^{-l_i})\\
& \geq0
\end{aligned}
$$

## Codifica di Shannon

Shannon cerca di ottimizzare la lunghezza attesa del codice per eguagliare l'entropia.\
Un idea iniziale è quella di scrivere un codice dove la lunghezza dei codici dei caratteri è vicina all'entropia del carattere. Si può fare sempre col valore esatto? no, bisogna usare ceil: $l_i=\lceil\log_D\frac1{p_i}\rceil$.

Usiamo il direct theorem per dimostrare $\sum\limits^k_{i=1}D^{-\lceil\log_D\frac1{p_i}\rceil}\leq1$:

$$
\begin{aligned}
& \lceil\log_D\frac1{p_i}\rceil=\log_D\left(\frac1{p_i}\right)+\beta_i & [0\leq\beta_i<1]\\
& \Rightarrow\sum\limits^k_{i=1}D^{\log_D\left(\frac1{p_i}\right)-\beta_i}\\
& =\sum\limits^k_{i=1}D^{\log_D\left(\frac1{p_i}\right)}\frac1{D^{\beta_i}}\\
& =\sum\limits^k_{i=1}p_i\frac1{D^{\beta_i}}\\
& \leq1
\end{aligned}
$$

Non è un buon modo perché viene assegnato un codice lungo ai caratteri poco probabili anche quando non ce n'è bisogno.

### La codifica di Shannon è subottimale

Per i codici di Shannon vale:

$$H_D(P)\leq EL(\varphi)<H_D(P)+1$$

Quando proviamo a calcolare Shannon per tuple di caratteri ($A=\{aa,ab,ba,bb\}$) l'efficienza aumenta rispetto a quella della codifica dell'alfabeto.

$$
\mathrm{Eff}=\frac{H_D(p)}{EL(\varphi)}\leq1
$$

proviamo a dimostrarlo:\
Siano $X,Y$ variabili casuali indipendenti. Vogliamo dimostrare che $H(X\land Y)=H(X)+H(Y)$
$$
\begin{aligned}
& H(X\land Y)=-\sum_{i,j}p_iq_j\log p_iq_j\\
& =-\sum_{i,j}p_iq_j\log p_i-\sum_{i,j}p_iq_j\log q_j\\
& =\sum_j q_j\sum_i p_i\log p_i-\sum_i p_i\sum_i q_j\log q_j\\
& =H(X)+H(Y)
\end{aligned}
$$

Segue che $EL(\varphi_n)\geq H_D(p^n)=nH_D(P)$. Quindi:
$$
\frac{\not nH_D(P)}{\not n}\leq EL(\varphi_n)<\frac{\not nH_D(P)}{\not n}+\frac1n
$$

La componente $\frac1n$ diventa sempre più piccola all'aumentare di $n$, quindi più $n$ è grande, più la lunghezza attesa si avvicina al lower bound.

> **Se aumentare la lunghezza aumenta l'efficienza, il codice si dice *subottimale***

## Codifica di Shannon-Fano

Cerchiamo di "distribuire le probabilità" tra i rami dell'albero.
Separiamo ricorsivamente i rami sul mediano in modo che: $\left|\sum\limits^h_{i=1}p_i-\sum\limits^k_{i=h+1}p_i\right|\simeq0$. Non si può sempre separare esattamente a metà, ma dopo averli ordinati è facile trovare punto quasi ottimale. Separarli esattamente nel modo ottimale è un problema NP-hard.

Shannon-Fano è subottimale

## Codifica di Huffman

Data la probabilità di ogni carattere, costruisco un albero di un singolo nodo da ogni carattere, e li inserisco in una min-heap.
Estraggo i due meno probabili e li unisco in un albero con come peso la somma dei pesi.
Quando è rimasto solo un albero, quello è l'albero di codifica.

Huffman è ottimale, ma nella pratica solo a volte è più corto di fano. È quello che dà la lunghezza attesa più bassa.