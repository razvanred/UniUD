# Teoria dell'informazione

> 1948 Claude Shannon, a mathematical theory of communication

Shannon introduce la nozione di entropia, vuole studiare il concetto di quanta informazione porta un certo messaggio. Il termine "informazione" era già occupato, "entropia" lo ha suggerito Von Neumann perché «assomiglia al concetto in fisica, e tanto ne parlano tutti e nessuno sa cos'è».

Piacere, prendiamo la frase "Fatima, piove!". Questa informazione è più o meno informativa a seconda del contesto in cui si trova Fatima, se sono a Klagenfurt è normale e Fatima non farà niente, se sono nel deserto del Sahara è un grande evento e Fatima scatterà in piedi.
Shannon dice che matematicamente, la quantità di informazione della frase è legata alla probabilità che l'evento succeda.

> Messaggio con probabilità bassa ($p\in[0,1]$)= alta quantità di informazione

Formalizziamo considerando uno spazio di eventi $E$ di cardinalità $n$ e con insieme delle probabilità $P$.

$$p\Rightarrow\log_n\left(\frac1p\right)=-\log_n(p)$$

La probabilità di un evento certo è 1, quindi la sua quantità di informazione (entropia) è 0. Il risultato del lancio di una moneta ha probabilità 0.5 e entropia 1 (corrisponde al bit che serve per rappresentarlo), il risultato del lancio di due monete ha probabilità 0.25 e entropia 2, sempre come i bit.

Shannon definisce due tipi di entropia, quella "normale" che usa sempre base 2 per il logaritmo (per avere un numero di bit), e l'entropia diadica, che invece usa come base la cardinalità dell'alfabeto.

Shannon definisce l'entropia di $E$ come la media delle entropie degli eventi $H(P)=\sum\limits^n_{i=1}p_i(-\log_2(p_i))$.
Definisce anche l'entropia D-adica come $H_D(P)=\sum\limits^n_{i=1}p_i(-\log_D(p_i))$, la differenza è la base del logaritmo (sarà usata per la compressione).

Esempi:
* Lancio di una moneta regolare con $P=\{\frac12,\frac12\}$
  * $H(P)=1$
* Lancio di una moneta truccata $P=\{\frac1{10},\frac9{10}\}$
  * $H(P)=0.1$, mediamente il messaggio è inutile perché uscirà quasi sempre croce
* Lancio di una moneta truccatissima $P=\{0,1\}$
  * $H(P)=0$, non porta nessuna informazione perché il risultato è sempre lo stesso
* Lancio di $n$ monete regolari
  * La probabilità di ogni risultato è $\frac1{2^n}$
  * $H(P)=n$
  * Se contassimo solo il numero di teste e di croci, il risultato sarebbe diverso
* Esperimento con $k$ eventi non uniformi
  * $H(p_1,...,p_k)\leq H(\frac1k,...,\frac1k)=\log(k)$
  * Le distribuzioni uniformi hanno l'entropia massima
    * Disuguaglianza di Jensen
    * $f(\sum \lambda_ix_i)\geq\sum\lambda f(x_i)$ per tutte le $f$ convesse
* $E=\{T,C\}$ e $P=\{p,1-p\}$, l'entropia dipende da un solo $p$
  * $H(P)=-p\log(p)-(1-p)\log(1-p)=f(p)$
  * $f(0)=0$
  * $f(1)=0$
  * $f(\frac12)=1$
  * ...

Shannon ha definito la nozione di entropia in modo che soddisfacesse alcuni assiomi:
* $H(p)$ è continua in $P$
* $H(\{\frac1n,...,\frac1n\})<H(\{\frac1{n+1},...,\frac1{n+1}\})$
* Se "spezzo gli eventi", l'entropia è additiva
  * Bayes
* altri...