# Distributed computing

Nel parallel computing si utilizzano sistemi composti da più processori omogenei che vengono presentati come una singola macchina, in realtà il problema viene suddiviso tra i processori.
Possono essere multiprocessori, gpu, cluster di computer, etc. E possono avere memoria condivisa o distribuita.

Nel distributed computing si utilizzano invece multipli computer connessi da una rete di qualche tipo.

Definizione: un sistema distribuito dove l'hardware ed il software comunicano e si coordinano solo per scambio di messaggi.

Non hanno memoria condivisa, quindi bisogna considerare tutti i problemi della normale comunicazione via rete.

L'obbiettivo principale è di fornire accesso a risorse distribuite, non parallelizzare l'esecuzione.

Problemi (ignorati nella computazione classica):
* Processi diversi accedono concorrentemente alle risorse distribuite
  * Risorse appartenenti ad altri processi della rete, non della stessa macchina
* I processi hanno solo informazioni parziali e potenzialmente inconsistenti sul sistema
  * Il sistema potrebbe essere troppo grande per essere contenuto in una sola macchina
  * Non c'è un vero "stato globale"
  * Pensa ai protocolli di routing
    * RIP ha informazioni incomplete
    * OSPF ha informazioni complete ma inconsistenti
* Non c'è un clock globale
  * È impossibile tenere dei clock sincronizzati
  * Generalmente non serve, è più importante l'ordine degli eventi
* Un processo potrebbe fallire all'insaputa degli altri
  * Ci sarà sempre qualcosa che si rompe

Parti di un sistema distribuito:
* Service: La parte del sistema che si occupa della gestione delle risorse e di rendere disponibili le funzionalità
* Server: Il processo che implementa il service in modo affidabile
* Client: Il processo utente che ottiene le risorse dal server

Errori comuni dei nuovi programmatori:
* La rete è affidabile
* La latenza è zero
* La banda è infinita
* La rete è sicura
* La topologia non cambia
* C'è un amministratore
* Il trasporto è zero-cost
* La rete è omogenea

## Problemi dei ds

### Eterogeneità

Il modo più comune di gestire l'eterogeneità è creare un layer di compatibilità. Un inseme di software, chiamati middleware, che devono essere reimplementati per ogni architettura. Poi, utilizzando questo middleware, il programmatore può realizzare i sistemi senza preoccuparsi di far comunicare le diverse architetture.

I sistemi possono essere estesi in vari modi. Ma principalmente in due modi: gli standard forniscono un'interfaccia comune per l'estensione senza influenzare il sistema nel complesso, oppure gli standard sono aperti ed è possibile proporre nuove versioni degli standard.

### Fallimento

Gli agenti di un sistema distribuito falliscono, spesso. Di solito non fallisce tutto il sistema, ma solo alcune parti mentre le altre continuano a girare. Spesso non si riesce a rilevare il guasto.
* Si può *nascondere* il problema e provare a gestirlo, in genere ritrasmettendo
  * Esempio: perdere un segmento in TCP
  * Non sempre possibile
* Si può *tollerare* il guasto e non fare niente
  * Esempio: perdere un pacchetto in UDP
* Si può *recuperare* da un guasto, usando i log per eseguire un rollback
  * Esempio: fallimento di una transazione SQL
* Si può avere *ridondanza* mantenendo repliche dei server
  * È difficile mantenere le repliche sincronizzate

La *availability* è la misura di tempo della disponibilità del servizio. La gestione dei guasti è fondamentale per aumentare la disponibilità.

Il sistema è inerentemente concorrente. Le risorse in condivisione devono essere rese accessibili coerenti ai diversi client, per mantenere la consistenza.
Le stesse risorse possono essere su diversi server e ci possono accedere diversi client e bisogna tenerle sincronizzate. È difficile, le tecniche che si usano nella programmazione locale non possono essere scalate ai sistemi distribuiti.

### Proprietà desiderate

Proprietà desiderate dei sistemi distribuiti:
* Consistenza, tutti i nodi devono avere una visione consistente dei dati
  * Bisogna mantenere degli invarianti tra i dati locali e remoti
* Availability, il servizio deve essere sempre accessibile
  * Ogni richiesta deve ricevere una risposta, anche negativa
  * Richieste possono essere inviate a nodi diversi
* Tolleranza alla partizione, si può fallire solo se fallisce tutta la rete
  * Se un nodo si disconnette, gli altri devono continuare a funzionare

Il CAP theorem (Consistency Availability Partition) dimostra che un sistema non può garantire tutte e tre le proprietà contemporaneamente. Bisogna scegliere quale rinunciare a seconda delle situazioni.
* Consistent-available systems
  * Non un vero distributed systems
  * Fallisce subito
* Consistent-partition tolerant systems
  * Sospendono il servizio se non possono garantire la consistenza
  * Esempio: ATMs, RDBMSs
* Available-partition tolerant systems
  * Il servizio non viene mai sospeso, ma i client possono vedere informazioni diverse
  * Quando i nodi tornano disponibili bisogna gestire le inconsistenze, a volte viene fatto dall'utente
  * Esempio: GIT, molti NoSQL, dropbox

Il teorema non impedisce altri tipi di consistenza, ad esempio *consistenza eventuale*. Il sistema resta inconsistente per un po', ed eventualmente torna consistente. I sistemi bancari ad esempio accettano inconsistenza temporanea per piccoli importi.
In generale, mantenere gli invarianti durante una partizione può essere impossibile, ma quando il sistema torna online si può recuperare la consistenza. Ad esempio nelle blockchain.

### Trasparenza

Il sistema dovrebbe nascondere il fatto di essere un ds, per il programmatore dovrebbe sembrare un singolo computer.

ISO fornisce un modello di riferimento delle trasparenze da garantire:
* *access transparency*: non si deve notare la differenza tra accesso ad una risorsa locale o remota
  * Come i file on demand
  * NFS
  * Non FTP
* *Location transparency*: non serve sapere la posizione fisica di una risorsa per accedere
  * Torrent
  * Non HTTP
* *Concurrency transparency*: i processi possono operare concorrentemente senza interferenze
* *Replication transparency*: posso esistere più copie delle risorse, per affidabilità, senza che si veda
* *Failure transparency*: gli utenti non sanno dei guasti
* *Mobility transparency* gli utenti non sanno se i dati si sono spostati
  * Torrent di nuovo
  * Non HTTP
* *Performance transparency*: gli utenti non sanno se il sistema viene potenziato per gestire più utenti
  * AWS
* *Scaling transparency*: simile a sopra

### QoS

Dati time critical devono essere processati ad un rate fissato. Ad esempio audio e video.