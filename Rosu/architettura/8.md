# Architetture per il calcolo parallelo

Anche se non è possibile realizzare un calcolatore con una sola CPU e un ciclo di 0.001 ns, sarebbe sicuramente possibile costruirne uno con 1000 CPU, ciascuna con un ciclo di clock di 1ns. Sebbene il secondo progetto si avvalga di CPU più lente in teoria la sua capacità totale di calcolo è la stessa del primo.

L'obiettivo è quello di migliorare le prestazioni in ambiti in cui:

* un algoritmo richiede più potenza di calcolo possibile
* un programma tende a sfruttare tutte le risorse disponibili
* l'aumento delle prestazioni non può più pesare sull'integrazione nel chip

Inoltre:

* un'architettura parallela può aumentare la tolleranza ai guasti attraverso la **ridondanza dei componenti**
* abbiamo un aumento proporzionale delle prestazioni aggiungendo nuove unità di calcolo (**scalabilità**)

Il parallelismo può essere introdotto a vari livelli

* a quello più basso possiamo ottenere un incremento prestazionale per un fattore di 10 rispetto ad un progetto puramente sequenziale
  * può essere incorporato nel chip della CPU attraverso un progetto superscalare a pipeline con molteplici utili funzionalità
  * può essere introdotto con l'adozione di istruzioni basate su parole molto lunghe e dotate di parallelismo implicito
  * può essere equipaggiata la CPU con caratteristiche speciali che le consentano di gestire il controllo di più thread alla volta
  * possono essere integrate più CPU nello stesso chip
* al livello successivo è possibile ottenere un incremento prestazionale per un fattore che va da 5 a 10, ma **solo nel caso di applicazioni specializzate**
  * è possibile aggiungere al sistema delle schede dotate di CPU che arricchiscano le sue capacità di calcolo (CPU plug-in che svolgono funzioni specializzate, come l'elaborazione multimediale o la crittografia)
* L'unico modo per ottenere un incremento prestazionale nell'ordine delle centinaia, delle migliaia o dei milioni è di replicare il numero delle CPU facendole cooperare in modo efficiente (idea base dei multiprocessori e dei multicomputer)
  * Oggi su Internet è possibile strutturare intere organizzazioni di computer come reti di calcolo con connessioni lasche (le **griglie**)

Due CPU o elementi di calcolo si possono dire legati:

* **strettamente** se svolgono calcoli in modo molto interrativo e la cui connessione ha un'elevata larghezza di banda e un ritardo trascurabile
* **debolmente** se si trovano molto distanti, hanno una banda ridotta, un ritardo elevato e svolgono i loro calcoli in modo poco interattivo

## Parallelismo nel chip

Un modo per incrementare la produttività di un chip è di fargli svolgere più compiti alla volta, in altre parole, sfruttare il parallelismo:

* parallelismo a livello delle istruzioni
* multithreading
* coabitazione di più CPU sullo stesso chip

### Parallelismo a livello di istruzioni

Un modo per ottenere il parallelismo è di emettere più istruzioni per ciclo di clock. Ci sono due tipi di CPU a emissione multipla, i processori:

* **superscalari** c'è un momento lungo la pipeline in cui l'istruzione è pronta per essere eseguita. Queste CPU sono in grado di emettere veso le unità di esecuzione più istruzioni in un solo ciclo, che dipende dal progetto del processore e dalle circostanze contingenti (il numero massimo in generale va da 2 a 6 in base all'hardware). In ogni caso un'istruzione non verrà emessa se attende un calcolo che non è stato ancora completato o se necessita di un'unita funzionale non disponibile.

* **Very Long Instruction Word** queste macchine avevano parole molto lunghe per contenere istruzioni che usavano diverse unità funzionali
  * nel caso in cui la macchina disponga di 5 unità funzionali (due operazioni intere, una in virgola mobile, un caricamento e una memorizzazione) un'istruzione VLIW conterrebbe 5 codici operativi e 5 coppie di operandi: un codice e una coppia di operandi per ogni unità funzionale
  * il progetto si rivelò troppo rigido perché non tutte le istruzioni riuscivano a sfruttare tutte le unità funzionali, il che conduceva all'utilizzo di molte inutili0 NO-OP (no operation)
  * per questo motivo le macchine moderne di questo tipo dispongono di una modalità di costruzione dei pacchetti (bundle) d'istruzioni, conclusi per esempio dal bit di fine pacchetto. Il processore è in grado di effettuare il fetch e l'emissione di un intero pacchetto per volta, spetta al compilatore preparare pacchetti d'istruzioni compatibili
  * questa tecnica sposta dall'esecuzione alla compilazione il difficile compito di stabilire quali istruzioni possono essere avviate alla esecuzione contemporanea
    * consente di avere un hardware più semplice e veloce
    * il compilatore può far durare l'ottimizzazione per tutto il tempo necessario, permettte di assemblare pacchetti d'istruzioni migliori di quanto potrebbe fare l'hardware durante l'esecuzione
    * questo progetto comporta un cambio radicale nell'architettura delle CPU difficile da accettare (dimostrato dalla scarsa accoglienza suscitata dall'Italium)

#### CPU VLIW TriMedia

DSP (Digital Signal Processor, sistema hardware per operare l'elaborazione numerica dei risultati) progettato da Philips per applicazioni audio, video o immagini (usato nei videoregistratori, lettori DVD ecc.). Ogni sua istruzione può contenere 5 operazioni. In condizioni ideali ad ogni ciclo di clock viene avviata l'esecuzione di un'istruzione e l'emissione delle cinque operazioni. Il clock ha una frequenza di 266 o 300 MHz, che però equivale ad una frequenza cinque volte maggiore, visto che può eseguire 5 operazioni contemporaneamente.

Ci sono 128 registri di 32 bit per uso generale, tra cui il registro R0 ed R1 sono rispettivamente cablati al valore 0 e 1, mentre tutti gli altri sono equivalenti dal punto di vista funzionale e possono essere utilizzati per qualsiasi scopo.

Tutte le operazioni multimediali usano l'**aritmetica satura** invece dell'aritmetica in complemento a 2 usata dalle operazioni intere. Quando un'operazione produce un risultato che non può essere espresso a causa di un overflow non viene sollevata un'eccezione, ma si ottiene il numero valido più vicino a quello corretto. Se si usano i numeri di 8 bit senza segno la somma tra 130 e 130 ritorna come risultato 255 invece di 260

A causa del fatto che non tutte le operazioni possono occupare tutti i posti accade frequentemente che un'istruzione non contenga tutte e cinque le operazioni che può ospitare. Se un posto non è utilizzato la sua dimensione viene ridotta per minimizzare lo spreco di spazio.

TriMedia non effettua controlli in fase d'esecuzione per verificare che le operazioni in un'istruzione siano compatibili. Se non lo sono, vengono eseguite comunque e producono un risultato errato. L'assenza del controllo è stata una scelta progettuale, in mdo tale da risparmiare tempi e transistor (l'i7 invece esegue il controllo durante l'esecuzione di tutte le operazioni superscalari, ma paga questa certezza in termini di complessità, tempo e transistor).

Le operazioni TriMedia, come nell'Italium 2, sono **predicative**, dove ogni operazione (apparte eccezioni trascurabili) specifica un registro da esaminare prima dell'esecuzione dell'operazione: se il bit meno significativo del registro è asserito (1), l'istruzione viene eseguita, viceversa viene saltata. A ognuna delle (al massimo) 5 operazioni viene attribuito un predicato individuale.

Esempio:

```assembly
IF R2 IADD R4,R5 -> R8
```

Operazione predicativa che effettua il test su R2 e, se il suo bit meno significativo vale 1, somma R4 ed R5 e salva il risultato in R8. Un'operazione diventa incondizonata se usa R1 (che vale sempre 1) come registro predicativo. Se usa R0 (che vale sempre 0) diventa una NO-OP.

#### Italium IA-64

In questo processore VLIW più istruzioni possono essere accodate ed eseguite contemporaneamente, senza controllarne l'indipendenza. Le istruzioni sono condizionate, per minimizzare i salti (le operazioni predicative vista prima). Possiede centinaia di registri, per minimizzare gli accessi in memoria e la dipendenza tra le istruzioni. Inoltre, ci sono anche caricamenti speculativi, che cercano di anticipare gli accessi in memoria (a meno di page-fault).

Questo processore ebbe poco successo commerciale, avendo prestazioni sotto le aspettative, e risultava difficile la progettazione dei compilatori. Il mercato ha preferito AMD64 (x86-64), una scelta conservativa.

### Multithreading nel chip

Il multithreading nel chip cerca di mascherare degli stati di stallo consentendo alla CPU di gestire contemporaneamente più thread di controllo. Se il thread 1 è bloccato (supponiamo che l'istruzione impieghi 2 cicli di clock) ha ancora la possibilità di eseguire il thread 2 e di mantenere l'hardware occupato.

Ci sono diversi approcci per permettere ciò: il primo si chiama **multithreading a grana fine**, che nasconde gli stalli grazie all'esecuzione a turno dei thread, con una commutazione a ogni ciclo.

Per esempio, _considerando CPU con capacità di emettere un'istruzione per ciclo di clock_, se lo stallo possibile di un'operazione del thread durasse al massimo 2 cicli e avremmo in esecuzione 3 thread tutte le istruzioni verrebbero sempre completate in tempo, se ci fossero stati 3 cicli di stallo avremmo avuto bisogno di 4 thread in esecuzione e così via.

Dal momento che non vi è alcuna relazione tra i thread, ciascuno ha bisogno del proprio insieme di registri. All'emissione di un'istruzione è nessario accludere all'istruzione stessa un puntatore al suo insieme di registri così che, se viene referenziato un registro, l'hardware potrà sapere quale registro usare. Per questa ragione il numero massimo di thread che può essere eseguito è stabilito in fase di progettazione del chip.

---

Se la pipeline ha k stadi e ci sono almeno k thread in esecuzione a turno, nella pipeline non ci sarà mai più di un'istruzione per thread in esecuzione e perciò non si possono verificare conflitti. In queste condizioni la CPU può girare a pieno ritmo senza stalli.

---

Potrebbero non esserci tanti thread quanti sono gli stati della pipeline, perciò alcuni progettisti prefericono l'approccio del **multithreading a grana grossa**.
Il primo thread continua ad emettere istruzioni fin quando non va in stallo, causando uno spreco di ciclo. A quel punto l'esecuzione viene commutata sul secondo thread, ma poiché la sua prima istruzione va subito in stallo, si verifica un'altra commutazione di thread (causando un'altro spreco di ciclo) e va in esecuzione il terzo thread con la sua prima istruzione.

Dato che si perde un ciclo ad ogni stallo questa tecnica è potenzialmente meno efficiente rispetto a quella a grana fine, ma presenta il vantaggio di richiedere meno thread per mantenere la CPU occupata. Questa tecnica è preferibile in tutte quelle situazioni in cui c'è un numero insufficiente di thread attivi, perché garantisce di trovare almeno uno da mandare in esecuzione.

Tuttavia, il multithreading a grana grossa non esegue commutazione solo sugli stalli: un'altra possibilità è di effettuare la commutazione immediatamente tutte le volte in cui un'istruzione potrebbe causare uno stallo (la commutazione occorrebbe in anticipo secondo questa strategia).