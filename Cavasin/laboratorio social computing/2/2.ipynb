{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Progetto Social Computing no.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "import random\n",
    "import itertools\n",
    "import math\n",
    "from collections import defaultdict\n",
    "from functools import reduce\n",
    "from copy import deepcopy\n",
    "from IPython.display import HTML, display\n",
    "from typing import Any\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File hits.json already exists. Skipping HITs generation and hits.json generation\n"
     ]
    }
   ],
   "source": [
    "hits_filename = \"hits.json\"\n",
    "mturk_vars_filename = \"mturk_variables.csv\"\n",
    "\n",
    "# Variabili come nel file .env\n",
    "task_name = \"SC_tasktest3\"\n",
    "batch_name = \"SC_batchtest3\"\n",
    "aws_region = \"us-east-1\"\n",
    "aws_deploy_bucket = \"my-sc-project-bucket\"\n",
    "\n",
    "cwd = Path.cwd()\n",
    "task_folder = cwd / \"task\"\n",
    "data_dir = cwd / \"result\" / \"SC_task1\" / \"Data\"  # Un file json è un worker\n",
    "dataframe_dir = cwd / \"result\" / \"SC_task1\" / \"Dataframe\"  # Risposte\n",
    "\n",
    "hits_file = task_folder / hits_filename\n",
    "mturk_vars_file = cwd / mturk_vars_filename\n",
    "answers_file = dataframe_dir / \"workers_answers.csv\"\n",
    "\n",
    "skip_mturk = False\n",
    "\n",
    "if os.path.exists(hits_file):\n",
    "    print(\n",
    "        f\"File {hits_filename} already exists. Skipping HITs generation and {hits_filename} generation\"\n",
    "    )\n",
    "    skip_mturk = True\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def float_atoi(x: str):\n",
    "    return math.trunc(float(x))\n",
    "\n",
    "\n",
    "def create_task_matrix(answers: dict[str, dict[int, dict[str, int]]], scale: str):\n",
    "    task_matrix: dict[int, dict[str, int]] = defaultdict(lambda: dict())\n",
    "    for (worker_id, docs) in answers.items():\n",
    "        for (doc_id, scales) in docs.items():\n",
    "            task_matrix[doc_id][worker_id] = scales[scale]\n",
    "    return task_matrix\n",
    "\n",
    "\n",
    "def calc_pairwise_agreement(\n",
    "    pairs: list[tuple[str, str]], task_matrix: dict[int, dict[str, int]]\n",
    "):\n",
    "    pairwise_agreement: dict[int, dict[str, float]] = defaultdict(lambda: dict())\n",
    "    for (task_id, answers) in task_matrix.items():\n",
    "        agreement_count = 0\n",
    "        for (w1, w2) in pairs:\n",
    "            if answers[w1] == answers[w2]:\n",
    "                agreement_count += 1\n",
    "        pairwise_agreement[task_id][\"pairwise agreement\"] = round(\n",
    "            agreement_count / len(pairs), 2\n",
    "        )\n",
    "    return pairwise_agreement\n",
    "\n",
    "\n",
    "def calc_average_agreement(pairwise_agreement: dict[int, dict[str, float]]):\n",
    "    return round(\n",
    "        sum(\n",
    "            [\n",
    "                pairwise_agreement[x][\"pairwise agreement\"]\n",
    "                for x in pairwise_agreement.keys()\n",
    "            ]\n",
    "        )\n",
    "        / len(pairwise_agreement),\n",
    "        2,\n",
    "    )\n",
    "\n",
    "\n",
    "def print_tables(*tables: dict[Any, dict[Any, Any]]):\n",
    "    width_keys = tables[0].keys()\n",
    "    html = f\"<table><tr><td></td><td>{'</td><td>'.join(map(str,width_keys))}</td></tr>\"\n",
    "    for table in tables:\n",
    "        height_keys = next(iter(table.values())).keys()\n",
    "        html += (\n",
    "            \"<tr>\"\n",
    "            + \"</tr><tr>\".join(\n",
    "                f\"<td>{y}</td><td>\"\n",
    "                + \"</td><td>\".join(str(table[x][y]) for x in width_keys)\n",
    "                + \"</td>\"\n",
    "                for y in height_keys\n",
    "            )\n",
    "            + \"</tr>\"\n",
    "        )\n",
    "    html += \"</table>\"\n",
    "    display(HTML(html))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate hits.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not skip_mturk:\n",
    "    with open(\"group_3-cavasin_cimador_faion.csv\", newline=\"\") as f:\n",
    "        statement_list = list(\n",
    "            csv.reader(f, delimiter=\",\", quotechar='\"')\n",
    "        )  # Importa il dataset in una lista (si guarda dall'indice 1 in poi)\n",
    "\n",
    "    chars = \"QWERTYUIOPASDFGHJKLZXCVBNM\"  # Per generare gli ID token\n",
    "    dict_array = []\n",
    "    choose_human_for = 1  # A quale statement viene assegnata explanation_human\n",
    "\n",
    "    for i in range(0, 12):\n",
    "        statements = []\n",
    "\n",
    "        for statement_number in range(1, 4):  # Gli statement da 1 a 3\n",
    "            # Si assegna explanation_human a solo uno statement su tre\n",
    "            statements += [\n",
    "                {\n",
    "                    \"id\": statement_list[statement_number][0],\n",
    "                    \"statement\": statement_list[statement_number][1],\n",
    "                    \"explanation\": statement_list[statement_number][\n",
    "                        2 if choose_human_for == statement_number else 3\n",
    "                    ],  # explanation_human o explanation_model\n",
    "                    \"label\": statement_list[statement_number][4],\n",
    "                }\n",
    "            ]\n",
    "        choose_human_for = choose_human_for % 3 + 1\n",
    "        random.shuffle(statements)  # Ordina i tre statement a caso\n",
    "        dict_array += [\n",
    "            {\n",
    "                \"unit_id\": \"unit_\" + str(i),\n",
    "                \"token_input\": \"\".join(random.sample(chars, 10)),\n",
    "                \"token_output\": \"\".join(random.sample(chars, 10)),\n",
    "                \"documents_number\": 3,\n",
    "                \"documents\": statements,\n",
    "            }\n",
    "        ]\n",
    "\n",
    "    with open(hits_file, \"w\") as json_file:\n",
    "        json.dump(dict_array, json_file, indent=4)  # Serializza su file json\n",
    "        print(\"File \", hits_filename, \" generated\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate variables for MTurk task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not skip_mturk:\n",
    "    with open(mturk_vars_file, \"w\") as mturk_vars:  # Genera variabili per mturk\n",
    "        mturk_vars.write(\n",
    "            \"aws_deploy_bucket,aws_region,task_name,batch_name,tokens\\n\"\n",
    "        )  # Colonne\n",
    "        for i in range(0, 12):\n",
    "            mturk_vars.write(\n",
    "                f\"{aws_deploy_bucket},{aws_region},{task_name},{batch_name},{dict_array[i].get('token_input')};{dict_array[i].get('token_output')}\\n\"\n",
    "            )\n",
    "        print(\"File \", mturk_vars_filename, \" generated\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate annotation percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker AQQPMWFVE949YW\n",
      "Worker AQQPMWFVE949YW.json is empty\n",
      "\n",
      "Worker B2PU87OELNL97V\n",
      "\tDocument 0: 93.0%\n",
      "\tDocument 1: 100.0%\n",
      "\tDocument 2: 46.3%\n",
      "\n",
      "Worker BM539KK2W9AIRX\n",
      "Worker BM539KK2W9AIRX.json is empty\n",
      "\n",
      "Worker HIOF5FD2FK3W9F\n",
      "\tDocument 0: 17.1%\n",
      "\tDocument 1: 21.3%\n",
      "\tDocument 2: 50.2%\n",
      "\n",
      "Worker IMDORB1186UKOF\n",
      "\tDocument 0: 92.9%\n",
      "\tDocument 1: 50.2%\n",
      "\tDocument 2: 42.7%\n",
      "\n",
      "Worker KFNXJP7XVSCPRX\n",
      "\tDocument 0: 35.2%\n",
      "\tDocument 1: 15.3%\n",
      "\tDocument 2: 12.9%\n",
      "\n",
      "Worker NMXJ0KIWOLKHNV\n",
      "\tDocument 0: 10.9%\n",
      "\tDocument 1: 88.1%\n",
      "\tDocument 2: 42.3%\n",
      "\n",
      "Worker OWAM2TV1SP4TU5\n",
      "\tDocument 0: 14.7%\n",
      "\tDocument 1: 92.9%\n",
      "\tDocument 2: 18.7%\n",
      "\n",
      "Worker RBG8K61JCTLLQD\n",
      "\tDocument 0: 50.7%\n",
      "\tDocument 1: 44.0%\n",
      "\tDocument 2: 69.0%\n",
      "\n",
      "Worker SKDKX9GZD4UH7L\n",
      "Worker SKDKX9GZD4UH7L.json is empty\n",
      "\n",
      "Worker TO5SRSVEVSUPUJ\n",
      "\tDocument 0: 12.1%\n",
      "\tDocument 1: 41.6%\n",
      "\tDocument 2: 95.7%\n",
      "\n",
      "Worker TWNMYWGHKNZTW0\n",
      "\tDocument 0: 10.5%\n",
      "\tDocument 1: 27.6%\n",
      "\tDocument 2: 95.7%\n",
      "\n",
      "Worker VZCC04FR6OASDY\n",
      "Worker VZCC04FR6OASDY.json is empty\n",
      "\n",
      "Worker XV8DEPZONNFP2D\n",
      "Worker XV8DEPZONNFP2D.json is empty\n",
      "\n",
      "Average of Document 0: 37.5%\n",
      "Average of Document 1: 53.4%\n",
      "Average of Document 2: 52.6%\n"
     ]
    }
   ],
   "source": [
    "docs = [0.0, 0.0, 0.0]  # Le quantità di testo annotato per ogni documento\n",
    "workers = 0\n",
    "\n",
    "\n",
    "def to_percent(number):\n",
    "    return round(number * 100, 1)\n",
    "\n",
    "\n",
    "for worker_file in data_dir.iterdir():\n",
    "    with open(worker_file, encoding=\"utf8\") as f:\n",
    "        worker_data = json.load(f)\n",
    "        print(\"Worker\", worker_file.stem)\n",
    "        try:\n",
    "            for i in range(0, 3):  # Per ogni document\n",
    "                annotations_data = worker_data[0][\"data_partial\"][\"documents_answers\"][\n",
    "                    i\n",
    "                ][\"serialization\"][\"notes\"][\n",
    "                    -1\n",
    "                ]  # Le informazioni di annotazione\n",
    "                annotation_length = len(\n",
    "                    annotations_data[\"raw_text\"]\n",
    "                )  # Lunghezza del testo annotato\n",
    "                statement_length = (\n",
    "                    annotation_length\n",
    "                    + len(annotations_data[\"text_left\"])\n",
    "                    + len(annotations_data[\"text_right\"])\n",
    "                )  # Lunghezza dell'intero statement\n",
    "                annotation_fraction = (\n",
    "                    annotation_length / statement_length\n",
    "                )  # Quantità di testo annotato\n",
    "                print(\n",
    "                    f\"\\tDocument {i}: {to_percent(annotation_fraction)}%\",\n",
    "                )\n",
    "                docs[\n",
    "                    i\n",
    "                ] += annotation_fraction  # Aggiungi la quantità alla lista (servirà per calcolare la media)\n",
    "            workers += 1\n",
    "        except IndexError as error:  # Se il file json non contiene le informazioni giuste\n",
    "            print(\"Worker\", worker_file.name, \"is empty\")\n",
    "        print(\"\")\n",
    "\n",
    "for i in range(0, 3):\n",
    "    print(\n",
    "        f\"Average of Document {i}: {to_percent(docs[i] / workers)}%\"\n",
    "    )  # Calcola la media della quantità di testo annotato\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count annotations update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker AQQPMWFVE949YW\n",
      "Worker AQQPMWFVE949YW.json is empty\n",
      "\n",
      "Worker B2PU87OELNL97V\n",
      "\tDocument 0: annotation updated 1 time(s)\n",
      "\tDocument 1: annotation updated 3 time(s)\n",
      "\tDocument 2: annotation updated 1 time(s)\n",
      "\n",
      "Worker BM539KK2W9AIRX\n",
      "Worker BM539KK2W9AIRX.json is empty\n",
      "\n",
      "Worker HIOF5FD2FK3W9F\n",
      "\tDocument 0: annotation updated 3 time(s)\n",
      "\tDocument 1: annotation updated 14 time(s)\n",
      "\tDocument 2: annotation updated 1 time(s)\n",
      "\n",
      "Worker IMDORB1186UKOF\n",
      "\tDocument 0: annotation updated 1 time(s)\n",
      "\tDocument 1: annotation updated 1 time(s)\n",
      "\tDocument 2: annotation updated 1 time(s)\n",
      "\n",
      "Worker KFNXJP7XVSCPRX\n",
      "\tDocument 0: annotation updated 1 time(s)\n",
      "\tDocument 1: annotation updated 2 time(s)\n",
      "\tDocument 2: annotation updated 1 time(s)\n",
      "\n",
      "Worker NMXJ0KIWOLKHNV\n",
      "\tDocument 0: annotation updated 6 time(s)\n",
      "\tDocument 1: annotation updated 2 time(s)\n",
      "\tDocument 2: annotation updated 8 time(s)\n",
      "\n",
      "Worker OWAM2TV1SP4TU5\n",
      "\tDocument 0: annotation updated 1 time(s)\n",
      "\tDocument 1: annotation updated 1 time(s)\n",
      "\tDocument 2: annotation updated 1 time(s)\n",
      "\n",
      "Worker RBG8K61JCTLLQD\n",
      "\tDocument 0: annotation updated 1 time(s)\n",
      "\tDocument 1: annotation updated 1 time(s)\n",
      "\tDocument 2: annotation updated 1 time(s)\n",
      "\n",
      "Worker SKDKX9GZD4UH7L\n",
      "Worker SKDKX9GZD4UH7L.json is empty\n",
      "\n",
      "Worker TO5SRSVEVSUPUJ\n",
      "\tDocument 0: annotation updated 4 time(s)\n",
      "\tDocument 1: annotation updated 17 time(s)\n",
      "\tDocument 2: annotation updated 1 time(s)\n",
      "\n",
      "Worker TWNMYWGHKNZTW0\n",
      "\tDocument 0: annotation updated 1 time(s)\n",
      "\tDocument 1: annotation updated 1 time(s)\n",
      "\tDocument 2: annotation updated 1 time(s)\n",
      "\n",
      "Worker VZCC04FR6OASDY\n",
      "Worker VZCC04FR6OASDY.json is empty\n",
      "\n",
      "Worker XV8DEPZONNFP2D\n",
      "Worker XV8DEPZONNFP2D.json is empty\n",
      "\n",
      "Document 0 annotations updated 19.0 time(s)\n",
      "Document 1 annotations updated 42.0 time(s)\n",
      "Document 2 annotations updated 16.0 time(s)\n"
     ]
    }
   ],
   "source": [
    "docs = [0.0, 0.0, 0.0]\n",
    "\n",
    "for worker_file in data_dir.iterdir():\n",
    "    with open(worker_file, encoding=\"utf8\") as jsonfile:\n",
    "        worker_data = json.load(jsonfile)\n",
    "        print(\"Worker\", worker_file.stem)\n",
    "        try:\n",
    "            for i in range(0, 3):  # Per ogni document\n",
    "                annotations_update_data = len(\n",
    "                    worker_data[0][\"data_partial\"][\"documents_answers\"][i][\n",
    "                        \"serialization\"\n",
    "                    ][\"notes\"]\n",
    "                )\n",
    "                print(\n",
    "                    f\"\\tDocument {i}: annotation updated {annotations_update_data} time(s)\",\n",
    "                )\n",
    "                docs[\n",
    "                    i\n",
    "                ] += annotations_update_data  # Quante volte vengono aggiornate le annotazioni per ogni documento\n",
    "\n",
    "        except IndexError as error:\n",
    "            print(\"Worker\", worker_file.name, \"is empty\")\n",
    "        print(\"\")\n",
    "\n",
    "for i in range(0, 3):\n",
    "    print(f\"Document {i} annotations updated {docs[i]} time(s)\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get elapsed time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker AQQPMWFVE949YW\n",
      "Worker  AQQPMWFVE949YW.json  is empty\n",
      "\n",
      "Worker B2PU87OELNL97V\n",
      "\tDocument 0: 298.0 second(s) elapsed\n",
      "\tDocument 1: 229.2 second(s) elapsed\n",
      "\tDocument 2: 713.2 second(s) elapsed\n",
      "\n",
      "Worker BM539KK2W9AIRX\n",
      "Worker  BM539KK2W9AIRX.json  is empty\n",
      "\n",
      "Worker HIOF5FD2FK3W9F\n",
      "\tDocument 0: 120.3 second(s) elapsed\n",
      "\tDocument 1: 145.6 second(s) elapsed\n",
      "\tDocument 2: 53.0 second(s) elapsed\n",
      "\n",
      "Worker IMDORB1186UKOF\n",
      "\tDocument 0: 210.2 second(s) elapsed\n",
      "\tDocument 1: 234.5 second(s) elapsed\n",
      "\tDocument 2: 60.3 second(s) elapsed\n",
      "\n",
      "Worker KFNXJP7XVSCPRX\n",
      "\tDocument 0: 128.5 second(s) elapsed\n",
      "\tDocument 1: 90.0 second(s) elapsed\n",
      "\tDocument 2: 132.4 second(s) elapsed\n",
      "\n",
      "Worker NMXJ0KIWOLKHNV\n",
      "\tDocument 0: 110.0 second(s) elapsed\n",
      "\tDocument 1: 97.1 second(s) elapsed\n",
      "\tDocument 2: 107.8 second(s) elapsed\n",
      "\n",
      "Worker OWAM2TV1SP4TU5\n",
      "\tDocument 0: 95.0 second(s) elapsed\n",
      "\tDocument 1: 108.9 second(s) elapsed\n",
      "\tDocument 2: 68.3 second(s) elapsed\n",
      "\n",
      "Worker RBG8K61JCTLLQD\n",
      "\tDocument 0: 102.5 second(s) elapsed\n",
      "\tDocument 1: 45.1 second(s) elapsed\n",
      "\tDocument 2: 112.2 second(s) elapsed\n",
      "\n",
      "Worker SKDKX9GZD4UH7L\n",
      "Worker  SKDKX9GZD4UH7L.json  is empty\n",
      "\n",
      "Worker TO5SRSVEVSUPUJ\n",
      "\tDocument 0: 299.7 second(s) elapsed\n",
      "\tDocument 1: 327.3 second(s) elapsed\n",
      "\tDocument 2: 194.7 second(s) elapsed\n",
      "\n",
      "Worker TWNMYWGHKNZTW0\n",
      "\tDocument 0: 110.2 second(s) elapsed\n",
      "\tDocument 1: 69.7 second(s) elapsed\n",
      "\tDocument 2: 47.6 second(s) elapsed\n",
      "\n",
      "Worker VZCC04FR6OASDY\n",
      "Worker  VZCC04FR6OASDY.json  is empty\n",
      "\n",
      "Worker XV8DEPZONNFP2D\n",
      "Worker  XV8DEPZONNFP2D.json  is empty\n",
      "\n",
      "Document 0 average time elapsed: 163.8 second(s)\n",
      "Document 1 average time elapsed: 149.7 second(s)\n",
      "Document 2 average time elapsed: 165.5 second(s)\n"
     ]
    }
   ],
   "source": [
    "docs = [0.0, 0.0, 0.0]\n",
    "\n",
    "for worker_file in data_dir.iterdir():\n",
    "    with open(worker_file, encoding=\"utf8\") as jsonfile:\n",
    "        worker_data = json.load(jsonfile)\n",
    "        print(\"Worker\", worker_file.stem)\n",
    "        try:\n",
    "            for i in range(0, 3):\n",
    "                time_elapsed = worker_data[0][\"data_partial\"][\"documents_answers\"][i][\n",
    "                    \"serialization\"\n",
    "                ][\n",
    "                    \"timestamps_elapsed\"\n",
    "                ]  # Quanto tempo viene impiegato per valutare un documento\n",
    "                time_elapsed = round(time_elapsed, 1)  # Approssima a una cifra decimale\n",
    "                print(\n",
    "                    f\"\\tDocument {i}: {time_elapsed} second(s) elapsed\",\n",
    "                )\n",
    "                docs[i] += time_elapsed\n",
    "\n",
    "        except IndexError as error:\n",
    "            print(\"Worker \", worker_file.name, \" is empty\")\n",
    "        print(\"\")\n",
    "\n",
    "for i in range(0, 3):\n",
    "    print(\n",
    "        f\"Document {i} average time elapsed: {round(docs[i] / workers, 1)} second(s)\",\n",
    "    )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Truthfulness 1:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr><td></td><td>224557</td><td>171067</td><td>47357</td></tr><tr><td>TO5SRSVEVSUPUJ</td><td>4</td><td>3</td><td>3</td></tr><tr><td>OWAM2TV1SP4TU5</td><td>5</td><td>1</td><td>5</td></tr><tr><td>HIOF5FD2FK3W9F</td><td>5</td><td>0</td><td>5</td></tr><tr><td>IMDORB1186UKOF</td><td>5</td><td>1</td><td>5</td></tr><tr><td>KFNXJP7XVSCPRX</td><td>4</td><td>0</td><td>1</td></tr><tr><td>NMXJ0KIWOLKHNV</td><td>4</td><td>1</td><td>4</td></tr><tr><td>B2PU87OELNL97V</td><td>2</td><td>2</td><td>0</td></tr><tr><td>RBG8K61JCTLLQD</td><td>4</td><td>1</td><td>4</td></tr><tr><td>TWNMYWGHKNZTW0</td><td>3</td><td>0</td><td>5</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Truthfulness 2:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr><td></td><td>224557</td><td>171067</td><td>47357</td></tr><tr><td>TO5SRSVEVSUPUJ</td><td>3</td><td>2</td><td>5</td></tr><tr><td>OWAM2TV1SP4TU5</td><td>2</td><td>0</td><td>5</td></tr><tr><td>HIOF5FD2FK3W9F</td><td>5</td><td>0</td><td>5</td></tr><tr><td>IMDORB1186UKOF</td><td>5</td><td>1</td><td>5</td></tr><tr><td>KFNXJP7XVSCPRX</td><td>5</td><td>0</td><td>5</td></tr><tr><td>NMXJ0KIWOLKHNV</td><td>2</td><td>0</td><td>5</td></tr><tr><td>B2PU87OELNL97V</td><td>5</td><td>0</td><td>4</td></tr><tr><td>RBG8K61JCTLLQD</td><td>5</td><td>0</td><td>5</td></tr><tr><td>TWNMYWGHKNZTW0</td><td>5</td><td>0</td><td>5</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(answers_file, encoding=\"utf8\", newline=\"\") as f:\n",
    "    answers_rows = list(\n",
    "        csv.DictReader(f, delimiter=\",\", quotechar='\"')\n",
    "    )  # Importa il dataset in una lista (si guarda dall'indice 1 in poi)\n",
    "\n",
    "answers: dict[str, dict[int, dict[str, int]]] = defaultdict(\n",
    "    lambda: defaultdict(lambda: dict())\n",
    ")\n",
    "# Il csv è ordinato rispetto al tempo, perciò vengono salvate solo le ultime risposte\n",
    "for answer_cols in answers_rows:\n",
    "    answers[answer_cols[\"worker_id\"]][int(answer_cols[\"doc_id\"])][\n",
    "        \"truthfulness1\"\n",
    "    ] = float_atoi(answer_cols[\"doc_truthfulness-1_index\"])\n",
    "    answers[answer_cols[\"worker_id\"]][int(answer_cols[\"doc_id\"])][\n",
    "        \"truthfulness2\"\n",
    "    ] = float_atoi(answer_cols[\"doc_truthfulness-2_index\"])\n",
    "\n",
    "task_matrix1 = create_task_matrix(answers, \"truthfulness1\")\n",
    "task_matrix2 = create_task_matrix(answers, \"truthfulness2\")\n",
    "\n",
    "print(\"Truthfulness 1:\")\n",
    "print_tables(task_matrix1)\n",
    "print(\"Truthfulness 2:\")\n",
    "print_tables(task_matrix2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairwise Agreements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Truthfulness 1:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr><td></td><td>224557</td><td>171067</td><td>47357</td></tr><tr><td>TO5SRSVEVSUPUJ</td><td>4</td><td>3</td><td>3</td></tr><tr><td>OWAM2TV1SP4TU5</td><td>5</td><td>1</td><td>5</td></tr><tr><td>HIOF5FD2FK3W9F</td><td>5</td><td>0</td><td>5</td></tr><tr><td>IMDORB1186UKOF</td><td>5</td><td>1</td><td>5</td></tr><tr><td>KFNXJP7XVSCPRX</td><td>4</td><td>0</td><td>1</td></tr><tr><td>NMXJ0KIWOLKHNV</td><td>4</td><td>1</td><td>4</td></tr><tr><td>B2PU87OELNL97V</td><td>2</td><td>2</td><td>0</td></tr><tr><td>RBG8K61JCTLLQD</td><td>4</td><td>1</td><td>4</td></tr><tr><td>TWNMYWGHKNZTW0</td><td>3</td><td>0</td><td>5</td></tr><tr><td>pairwise agreement</td><td>0.25</td><td>0.25</td><td>0.19</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Pairwise Agreement: 0.23\n",
      "\n",
      "Truthfulness 2:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr><td></td><td>224557</td><td>171067</td><td>47357</td></tr><tr><td>TO5SRSVEVSUPUJ</td><td>3</td><td>2</td><td>5</td></tr><tr><td>OWAM2TV1SP4TU5</td><td>2</td><td>0</td><td>5</td></tr><tr><td>HIOF5FD2FK3W9F</td><td>5</td><td>0</td><td>5</td></tr><tr><td>IMDORB1186UKOF</td><td>5</td><td>1</td><td>5</td></tr><tr><td>KFNXJP7XVSCPRX</td><td>5</td><td>0</td><td>5</td></tr><tr><td>NMXJ0KIWOLKHNV</td><td>2</td><td>0</td><td>5</td></tr><tr><td>B2PU87OELNL97V</td><td>5</td><td>0</td><td>4</td></tr><tr><td>RBG8K61JCTLLQD</td><td>5</td><td>0</td><td>5</td></tr><tr><td>TWNMYWGHKNZTW0</td><td>5</td><td>0</td><td>5</td></tr><tr><td>pairwise agreement</td><td>0.44</td><td>0.58</td><td>0.78</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Pairwise Agreement: 0.6\n"
     ]
    }
   ],
   "source": [
    "pairs = list(itertools.combinations(answers.keys(), 2))\n",
    "\n",
    "pairwise_agreement1 = calc_pairwise_agreement(pairs, task_matrix1)\n",
    "pairwise_agreement2 = calc_pairwise_agreement(pairs, task_matrix2)\n",
    "\n",
    "print(\"Truthfulness 1:\")\n",
    "print_tables(task_matrix1, pairwise_agreement1)\n",
    "print(\"Average Pairwise Agreement:\", calc_average_agreement(pairwise_agreement1))\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(\"Truthfulness 2:\")\n",
    "print_tables(task_matrix2, pairwise_agreement2)\n",
    "print(\"Average Pairwise Agreement:\", calc_average_agreement(pairwise_agreement2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "social computing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cddab8898b1f086707db28a6a74918cdde48d6e218594c6ce384c312d4019bfd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
